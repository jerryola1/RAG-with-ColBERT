{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31067f6-303c-409a-b975-ab5828b3ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4dbfc0-b889-492e-86ac-ede45dad8dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8469d07-9ff7-462f-bbc3-2213862590a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9869a-b1b5-4385-9360-626a718107f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b188569-4999-4e52-a62e-81b194f5e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"hallucination_paper.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17ff21-c9d0-46f7-a9a9-89c48519d3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b1ac4f-547d-4fce-941a-e3a837b51738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4602"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ca0c09-8456-4e99-bc23-11cce795a8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4085fe9-7a26-4b21-b45d-1038ebb1948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_document = \"\"\n",
    "\n",
    "# for page in pages:\n",
    "#   full_document += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9f909-45da-4667-9c29-45b44fcf256c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22f8f2-10a1-435f-9d69-224bc12521b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5a0e2d-3027-44f7-9815-e1a9adcc4189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Survey on Hallucination in Large Language Models:\n",
      "Principles, Taxonomy, Challenges, and Open Questions\n",
      "Lei Huang1∗, Weijiang Yu2∗, Weitao Ma1, Weihong Zhong1\n",
      "Zhangyin Feng1, Haotian Wang1, Qianglong Chen2, Weihua Peng2\n",
      "Xiaocheng Feng1†, Bing Qin1, Ting Liu1\n",
      "1Harbin Institute of Technology, Harbin, China\n",
      "2Huawei Inc., Shenzhen, China\n",
      "{lhuang,wtma,whzhong,zyfeng,xcfeng†,qinb,tliu}@ir.hit.edu.cn\n",
      "{weijiangyu8,wanght1998,chenqianglong.ai,pengwh.hit}@gmail.com\n",
      "Abstract\n",
      "The emergence of large language models\n",
      "(LLMs) has marked a significant breakthrough\n",
      "in natural language processing (NLP), leading\n",
      "to remarkable advancements in text understand-\n",
      "ing and generation. Nevertheless, alongside\n",
      "these strides, LLMs exhibit a critical tendency\n",
      "to produce hallucinations, resulting in content\n",
      "that is inconsistent with real-world facts or user\n",
      "inputs. This phenomenon poses substantial\n",
      "challenges to their practical deployment and\n",
      "raises concerns over the reliability of LLMs in\n",
      "real-world scenarios, which attracts increasing\n",
      "attention to detect and mitigate these hallucina-\n",
      "tions. In this survey, we aim to provide a thor-\n",
      "ough and in-depth overview of recent advances\n",
      "in the field of LLM hallucinations. We begin\n",
      "with an innovative taxonomy of LLM halluci-\n",
      "nations, then delve into the factors contributing\n",
      "to hallucinations. Subsequently, we present a\n",
      "comprehensive overview of hallucination de-\n",
      "tection methods and benchmarks. Additionally,\n",
      "representative approaches designed to mitigate\n",
      "hallucinations are introduced accordingly. Fi-\n",
      "nally, we analyze the challenges that highlight\n",
      "the current limitations and formulate open ques-\n",
      "tions, aiming to delineate pathways for future\n",
      "research on hallucinations in LLMs.1\n",
      "1 Introduction\n",
      "Recently, the emergence of large language mod-\n",
      "els (LLMs) (OpenAI, 2022; Google, 2023; Tou-\n",
      "vron et al., 2023; Penedo et al., 2023; Zhao et al.,\n",
      "2023b) has ushered in a paradigm shift in nat-\n",
      "ural language processing (NLP), achieving un-\n",
      "precedented progress in language understanding\n",
      "(Hendrycks et al., 2021; Huang et al., 2023c), gen-\n",
      "eration (Zhang et al., 2023f; Zhu et al., 2023b) and\n",
      "∗Equal Contribution\n",
      "†Corresponding Author\n",
      "1Resources are available at: https://github.com/\n",
      "LuckyyySTA/Awesome-LLM-hallucinationreasoning (Wei et al., 2022; Kojima et al., 2022;\n",
      "Qiao et al., 2022; Yu et al., 2023a; Chu et al., 2023).\n",
      "Nevertheless, in tandem with the rapid advance-\n",
      "ment in LLMs, there’s a concerning trend where\n",
      "they exhibit an inclination to generate hallucina-\n",
      "tions (Bang et al., 2023; Guerreiro et al., 2023b),\n",
      "resulting in seemingly plausible yet factually un-\n",
      "supported content.\n",
      "The current definition of hallucinations aligns\n",
      "with prior research (Ji et al., 2023a), characterizing\n",
      "them as generated content that is nonsensical or\n",
      "unfaithful to the provided source content. These\n",
      "hallucinations are further categorized into intrin-\n",
      "sic hallucination andextrinsic hallucination types,\n",
      "depending on the contradiction with the source\n",
      "content. While this category is shared among vari-\n",
      "ous natural language generation (NLG) tasks, task-\n",
      "specific variations do exist. As LLMs are remark-\n",
      "ably versatile and excel across different NLG tasks\n",
      "(Bubeck et al., 2023; Bang et al., 2023), particu-\n",
      "larly in open-domain applications, their remarkable\n",
      "versatility amplifies the potential for hallucinations\n",
      "compared to task-specific models. In LLMs, the\n",
      "scope of hallucination encompasses a broader and\n",
      "more comprehensive concept, primarily centering\n",
      "on factual errors. In light of the evolution of the\n",
      "LLM era, there arises a need to adjust the existing\n",
      "hallucination taxonomy, enhancing its applicability\n",
      "and adaptability.\n",
      "In this survey, we have redefined the taxonomy\n",
      "of hallucination, offering a more tailored frame-\n",
      "work for LLM applications. We categorize hallu-\n",
      "cination into two main groups: factuality halluci-\n",
      "nation andfaithfulness hallucination .Factuality\n",
      "hallucination emphasizes the discrepancy between\n",
      "generated content and verifiable real-world facts,cination into two main groups: factuality halluci-\n",
      "nation andfaithfulness hallucination .Factuality\n",
      "hallucination emphasizes the discrepancy between\n",
      "generated content and verifiable real-world facts,\n",
      "typically manifesting as factual inconsistency or\n",
      "fabrication. For example, as in Fig. 1(a), when\n",
      "queried about the first person to walk on the moon,arXiv:2311.05232v1  [cs.CL]  9 Nov 2023Answer: The ﬁrst person to walk on the moon\n",
      "was Charles Lindbergh in 1951, during the Lunar\n",
      "Pioneer mission. His historic moonwalk was a\n",
      "testament to human spirit and was broadcasted\n",
      "live to millions of people around the globe.Who was the ﬁrst person to walk on the moon?\n",
      "Correct Answer: Neil Armstrong was the ﬁrst\n",
      "person to walk on the moon in 1969 during the\n",
      "Apollo 11 mission.\n",
      "60\n",
      "Answer: In October 2006, Israel declared war on\n",
      "Hamas after an unexpected attack, prompting\n",
      "ongoing violence, civilian crises, and regional\n",
      "conﬂict escalation.Please summarize the following news article:\n",
      "Context: In early October 2023, war broke out\n",
      "between Israel and Hamas, the militant Islamist\n",
      "group that has controlled Gaza since 2006.\n",
      "Hamas ﬁghters ﬁred rockets  ... civilians and\n",
      "taking dozens of hostages. \n",
      "(a) Factuality Hallucination (b) Faithfulness HallucinationFigure 1: An intuitive example of LLM hallucination.\n",
      "the model might assertively claim it was Charles\n",
      "Lindbergh in 1951. While the truth is that Neil\n",
      "Armstrong was the first individual to walk on the\n",
      "moon in 1969 during the Apollo 11 mission. On\n",
      "the other hand, faithfulness hallucination refers to\n",
      "the divergence of generated content from user in-\n",
      "structions or the context provided by the input, as\n",
      "well as self-consistency within generated content.\n",
      "As illustrated in Figure 1(b), when asked to sum-\n",
      "marize a news article, the model inaccurately gen-\n",
      "erated the actual event date of the conflict between\n",
      "Israel and Hamas from October 2023 to October\n",
      "2006. Regarding factuality, we further divide it\n",
      "based on the presence of verifiable sources into\n",
      "two subcategories: factual inconsistency and fac-\n",
      "tual fabrication. For faithfulness, we emphasize\n",
      "addressing inconsistency from the user’s perspec-\n",
      "tive, categorizing it into instruction inconsistency,\n",
      "context inconsistency, and logical inconsistency,\n",
      "thus aligning it better with the current usage of\n",
      "LLMs.\n",
      "As for the underlying causes of hallucinations,\n",
      "while studied in the context of NLG tasks, present\n",
      "unique challenges in cutting-edge LLMs that are\n",
      "worthy of an in-depth investigation. Our in-depth\n",
      "analysis specifically targets the unique origins of\n",
      "hallucinations in LLMs, spanning a spectrum of\n",
      "contributing factors from data, and training, to the\n",
      "inference stage. Within this framework, we pin-\n",
      "point potential data-related causes such as flawed\n",
      "sources and suboptimal utilization, inferior training\n",
      "strategies that may induce hallucinations during\n",
      "pre-training and alignment, and those stemming\n",
      "from the stochastic nature of decoding strategies\n",
      "and imperfect representations during the inference\n",
      "process. Furthermore, we comprehensively outline\n",
      "a variety of effective detection methods specifically\n",
      "devised for detecting hallucinations in LLMs, aswell as an exhaustive overview of benchmarks re-\n",
      "lated to LLM hallucinations, serving as appropriate\n",
      "testbeds to assess the extent of hallucinations gen-\n",
      "erated by LLMs and the efficacy of detection meth-\n",
      "ods. Moreover, we detail comprehensive strategies\n",
      "tailored to mitigate the identified causes of halluci-\n",
      "nations.\n",
      "Through this comprehensive survey, we aim to\n",
      "contribute to the advancement of the field of LLMs\n",
      "and provide valuable insights that deepen the under-\n",
      "standing of the opportunities and challenges associ-\n",
      "ated with hallucinations in LLMs. This exploration\n",
      "not only enhances our understanding of the limita-\n",
      "tions of current LLMs but also provides essential\n",
      "guidance for future research and the development\n",
      "of more robust and trustworthy LLMs.\n",
      "Comparing with Existing Surveys. As the push\n",
      "for reliable generative AI intensifies, LLM halluci-\n",
      "nation stands out as a major challenge, leading to\n",
      "numerous surveys on its recent advancements (Ji\n",
      "et al., 2023a; Rawte et al., 2023; Liu et al., 2023h;\n",
      "Zhang et al., 2023g; Wang et al., 2023c). While\n",
      "these works have probed into LLM hallucination\n",
      "from diverse angles and offered valuable insights,\n",
      "it is imperative to distinguish the unique aspectsZhang et al., 2023g; Wang et al., 2023c). While\n",
      "these works have probed into LLM hallucination\n",
      "from diverse angles and offered valuable insights,\n",
      "it is imperative to distinguish the unique aspects\n",
      "and comprehensive nature of our present survey.\n",
      "(Ji et al., 2023a) primarily sheds light on halluci-\n",
      "nations in pre-trained language models within the\n",
      "realm of NLG tasks, leaving LLMs outside their\n",
      "discussion purview. (Liu et al., 2023h) discusses\n",
      "the trustworthiness of LLMs from a broader per-\n",
      "spective, while (Wang et al., 2023c) delves deeply\n",
      "into LLM factuality. In contrast, our survey ze-\n",
      "roes in on a subset of challenges in LLM trustwor-\n",
      "thiness, covering aspects of factuality and further\n",
      "broadening the discourse to include faithfulness-\n",
      "related hallucinations. To the best of our knowl-\n",
      "edge, the work most aligned with our survey is(Zhang et al., 2023g), which outlines taxonomies of\n",
      "LLM hallucination phenomena, evaluation bench-\n",
      "marks, and mitigation strategies. Nevertheless, our\n",
      "survey distinguishes itself both in terms of its tax-\n",
      "onomy and organizational structure. We present\n",
      "a layered and granular classification of hallucina-\n",
      "tions. Structurally, we dissect the causes of LLM\n",
      "hallucination by tracing back to the capabilities of\n",
      "LLMs. More pertinently, our mitigation strategies\n",
      "are intricately linked with the underlying causes,\n",
      "ensuring a cohesive and targeted approach.\n",
      "Organization of this Survey. In this paper, we\n",
      "present a comprehensive survey of the latest de-\n",
      "velopments regarding hallucinations in LLMs. We\n",
      "commence by defining LLMs and constructing a\n",
      "taxonomy of hallucinations within this context (§2).\n",
      "Subsequently, we analyze the factors contributing\n",
      "to hallucinations in LLMs in depth (§3), followed\n",
      "by an examination of various methodologies and\n",
      "benchmarks employed for the reliable detection\n",
      "of hallucinations in LLMs (§4). We then detail a\n",
      "spectrum of approaches designed to mitigate hal-\n",
      "lucinations in LLMs (§5). Concluding, we delve\n",
      "into the challenges and open questions that frame\n",
      "the current limitations and future prospects of this\n",
      "field, offering insights and delineating potential\n",
      "pathways for forthcoming research (§6).\n",
      "2 Definitions\n",
      "For the sake of a comprehensive understanding of\n",
      "hallucinations in LLMs, we commence with a suc-\n",
      "cinct introduction to LLMs (§2.1), delineating the\n",
      "scope of this survey. Subsequently, we delve into\n",
      "the training process of LLMs (§2.2), as a thorough\n",
      "understanding of the underlying training mecha-\n",
      "nisms contributes significantly to elucidating the\n",
      "origins of hallucinations. Lastly, we expound upon\n",
      "the concept of hallucinations (§2.3) in LLMs, fur-\n",
      "ther categorizing it into two distinct types.\n",
      "2.1 Large Language Models\n",
      "Before delving into the causes of hallucination, we\n",
      "first introduce the concept of LLMs. Typically,\n",
      "LLMs refer to a series of general-purpose mod-\n",
      "els that leverage the Transformer-based language\n",
      "model architecture and undergo extensive train-\n",
      "ing on massive textual corpora with notable exam-\n",
      "ples including GPT-3 (Brown et al., 2020), PaLM\n",
      "(Chowdhery et al., 2023), Galactica (Taylor et al.,\n",
      "2022) LLaMA (Touvron et al., 2023) and GPT-4\n",
      "(OpenAI, 2023). By scaling the amount of data andmodel capacity, LLMs raise amazing emergent abil-\n",
      "ities, typically including In-Context Learning (ICL)\n",
      "(Brown et al., 2020), Chain-of-Thought prompting\n",
      "(Wei et al., 2022) and instruction following (Peng\n",
      "et al., 2023).\n",
      "2.2 Training Stages of Large Language\n",
      "Models\n",
      "The attributes and behaviors of LLMs are deeply\n",
      "intertwined with their training processes. LLMs\n",
      "undergo three primary training stages: pre-training,\n",
      "supervised fine-tuning (SFT), and reinforcement\n",
      "learning from human feedback (RLHF). Analyzing\n",
      "these stages provides insight into hallucination ori-\n",
      "gins in LLMs, as each stage equips the model with\n",
      "specific capabilities.\n",
      "Pre-training. Pre-training is generally consid-\n",
      "ered a crucial stage for LLM to acquire knowledge\n",
      "and skills (Zhou et al., 2023a). Language mod-\n",
      "els, during pre-training, aim to predict the next\n",
      "token in a sequence autoregressively. Through self-\n",
      "supervised training on extensive textual corpora,\n",
      "the model acquires knowledge of language syntax,\n",
      "world knowledge, and reasoning abilities, provid-\n",
      "ing a robust foundation for subsequent fine-tuning\n",
      "tasks. Besides, recent research (Sutskever, 2023;\n",
      "Delétang et al., 2023) suggests that predicting sub-\n",
      "sequent words is akin to losslessly compressing\n",
      "significant information. The essence of language\n",
      "models lies in predicting the probability distribu-\n",
      "tion for upcoming words. Accurate predictions\n",
      "indicate a profound grasp of knowledge, translat-\n",
      "ing to a nuanced understanding of the world.\n",
      "Supervised Fine-Tuning. While LLMs acquire\n",
      "substantial knowledge and capabilities during the\n",
      "pre-training stage, it’s crucial to recognize that pre-ing to a nuanced understanding of the world.\n",
      "Supervised Fine-Tuning. While LLMs acquire\n",
      "substantial knowledge and capabilities during the\n",
      "pre-training stage, it’s crucial to recognize that pre-\n",
      "training primarily optimizes for completion. Conse-\n",
      "quently, pre-trained LLMs fundamentally served as\n",
      "completion machines, which can lead to a misalign-\n",
      "ment between the next-word prediction objective\n",
      "of LLMs and the user’s objective of obtaining de-\n",
      "sired responses. To bridge this gap, SFT (Zhang\n",
      "et al., 2023d) has been introduced, which involves\n",
      "further training LLMs using a meticulously anno-\n",
      "tated set of (instruction, response) pairs, resulting\n",
      "in enhanced capabilities and improved controllabil-\n",
      "ity of LLMs. Furthermore, recent studies (Chung\n",
      "et al., 2022; Iyer et al., 2022) have confirmed the\n",
      "effectiveness of supervised fine-tuning to achieve\n",
      "exceptional performance on unseen tasks, showcas-\n",
      "ing their remarkable generalization abilities.Hallucinations in Large Language ModelsHallucination\n",
      "Causes (§3)Hallucination from DataFlawed Data SourceMisinformation and\n",
      "Biasese.g.Lin et al. (2022); Lee et al. (2022a); Bender et al. (2021)\n",
      "Knowledge Boundary e.g.Singhal et al. (2023); Katz et al. (2023); Onoe et al. (2022)\n",
      "Inferior Data UtilizationKnowledge Shortcut e.g.Li et al. (2022a); Kandpal et al. (2023); Kang and Choi (2023)\n",
      "Knowledge Recall\n",
      "Failurese.g.Mallen et al. (2023); Zheng et al. (2023); Liu et al. (2023e)\n",
      "Hallucination from\n",
      "TrainingHallucination from\n",
      "Pre-trainingArchitecture Flaw e.g.Li et al. (2023h); Liu et al. (2023a)\n",
      "Suboptimal Training\n",
      "Objectivee.g.Wang and Sennrich (2020); Zhang et al. (2023c)\n",
      "Hallucination from\n",
      "AlignmentCapability\n",
      "Misalignmente.g.Schulman (2023)\n",
      "Belief Misalignment e.g.Cotra (2021); Perez et al. (2023); Wei et al. (2023); Sharma et al. (2023)\n",
      "Hallucination from\n",
      "InferenceDefective Decoding\n",
      "StrategyInherent Sampling\n",
      "Randomnesse.g.Stahlberg and Byrne (2019); Holtzman et al. (2020)\n",
      "Imperfect Decoding\n",
      "RepresentationInsufficient Context\n",
      "Attentione.g.Miao et al. (2021); Chen et al. (2022b, 2023f); Liu et al. (2023i)\n",
      "Softmax Bottleneck e.g.Yang et al. (2018a); Chang and McCallum (2022)\n",
      "Hallucination\n",
      "Detection and\n",
      "Benchmarks(§4)Hallucination DetectionFactuality Hallucination\n",
      "Detectione.g.Chen et al. (2023c); Chern et al. (2023); Min et al. (2023)\n",
      "Faithfulness Hallucination\n",
      "Detectione.g.Maynez et al. (2020); Scialom et al. (2021); Fabbri et al. (2022)\n",
      "Hallucination BenchmarksHallucination Evaluation\n",
      "Benchmarkse.g.TruthfulQA (Lin et al., 2022),HalluQA (Cheng et al., 2023)\n",
      "Hallucination Detection\n",
      "Benchmarkse.g.HaluEval(Li et al., 2023c), FELM(Chen et al., 2023d)\n",
      "Hallucination\n",
      "Mitigation (§5)Mitigating Data-related\n",
      "HallucinationsMitigating Misinformation\n",
      "and BiasesFactuality Data\n",
      "Enhancemente.g.Gunasekar et al. (2023); Touvron et al. (2023)\n",
      "Debias e.g.Abbas et al. (2023); Viswanath and Zhang (2023); Ladhak et al. (2023)\n",
      "Mitigating Knowledge\n",
      "BoundaryModel Editing e.g. Mitchell et al. (2022b); Huang et al. (2023d); Dai et al. (2022b)\n",
      "Retrieval Augmentation e.g.Ram et al. (2023); Yu et al. (2023d); Gao et al. (2023a)\n",
      "Mitigating Knowledge\n",
      "ShortcutCo-occurrence Debias e.g.Kang and Choi (2023)\n",
      "Mitigating Knowledge\n",
      "Recall FailuresKnowledge Clue\n",
      "Enhancemente.g.Zheng et al. (2023); Wang et al. (2023g); Tang et al. (2023)\n",
      "Mitigating Training-related\n",
      "HallucinationsMitigating Pre-training-\n",
      "related HallucinationArchitecture\n",
      "Improvemente.g.Li et al. (2023h); Liu et al. (2023e,a)\n",
      "Pre-training Objective\n",
      "Improvemente.g.Lee et al. (2022b); Shi et al. (2023c); Wang et al. (2023b)\n",
      "Mitigating Alignment-\n",
      "related HallucinationSycophancy e.g.Wei et al. (2023); Sharma et al. (2023); Rimsky (2023b,a)\n",
      "Mitigating Inference-related\n",
      "HallucinationsFactuality Enhanced\n",
      "DecodingOn Standalone\n",
      "Decodinge.g.Lee et al. (2022b); Li et al. (2023d); Chuang et al. (2023)\n",
      "Post-editing Decoding e.g.Pan et al. (2023); Dhuliawala et al. (2023); Ji et al. (2023b)\n",
      "Faithfulness Enhanced\n",
      "DecodingContext Consistency e.g.Wan et al. (2023); Shi et al. (2023b); Chang et al. (2023a)\n",
      "Logical Consistency e.g.Wang et al. (2023f); O’Brien and Lewis (2023)\n",
      "Figure 2: The main content flow and categorization of this survey.Reinforcement Learning from Human Feed-\n",
      "back. While the SFT process successfully enables\n",
      "LLMs to follow user instructions, there is still room\n",
      "for them to better align with human preferences.\n",
      "Among various methods that utilize human feed-\n",
      "back, RLHF stands out as an institute solution for\n",
      "aligning with human preferences through reinforce-\n",
      "ment learning (Christiano et al., 2017; Stiennon\n",
      "et al., 2020; Ouyang et al., 2022). Typically, RLHF\n",
      "employs a preference model (Bradley and Terry,\n",
      "1952) trained to predict preference rankings given\n",
      "a prompt alongside a pair of human-labeled re-\n",
      "sponses. To align with human preferences, RLHF\n",
      "optimizes the LLM to generate outputs that max-\n",
      "imize the reward provided by the trained prefer-\n",
      "ence model, typically employing a reinforcement\n",
      "learning algorithm, such as Proximal Policy Op-\n",
      "timization (PPO)(Schulman et al., 2017). Such\n",
      "integration of human feedback into the training\n",
      "loop has proven effective in enhancing the align-\n",
      "ment of LLMs, guiding them toward producing\n",
      "high-quality and harmless responses.\n",
      "2.3 Hallucinations in Large Language Models\n",
      "The concept of hallucination traces its roots to the\n",
      "fields of pathology and psychology and is defined\n",
      "asthe perception of an entity or event that is ab-\n",
      "sent in reality (Macpherson and Platchias, 2013).\n",
      "Within the realm of NLP, hallucination is typically\n",
      "referred to as a phenomenon in which the gen-\n",
      "erated content appears nonsensical or unfaithful\n",
      "to the provided source content (Filippova, 2020;\n",
      "Maynez et al., 2020). This concept bears a loose\n",
      "resemblance to the phenomenon of hallucination\n",
      "observed in human psychology. Generally, halluci-\n",
      "nations in natural language generation tasks can be\n",
      "categorized into two primary types: intrinsic hallu-\n",
      "cination andextrinsic hallucination (Huang et al.,\n",
      "2021; Li et al., 2022b; Ji et al., 2023a). Specifically,\n",
      "intrinsic hallucinations pertain to the outputs of\n",
      "LLMs that conflict with the source content. Con-\n",
      "versely, extrinsic hallucinations refer to the LLM\n",
      "generations that cannot be verified from the source\n",
      "content.\n",
      "However, in the era of large language models,\n",
      "the versatile capabilities of these models have fa-\n",
      "cilitated their widespread use across diverse fields,\n",
      "highlighting limitations in existing task-specific\n",
      "categorization paradigms. Considering that LLMs\n",
      "place a significant emphasis on user-centric interac-\n",
      "tions and prioritize alignment with user directives,coupled with the fact that their hallucinations pre-\n",
      "dominantly surface at factual levels, we introduce\n",
      "a more granular taxonomy building upon the foun-\n",
      "dational work by Ji et al. (2023a). This refined tax-\n",
      "onomy seeks to encapsulate the distinct intricacies\n",
      "associated with LLM hallucinations. To provide a\n",
      "more intuitive illustration of our definition of LLM\n",
      "hallucination, we present examples for each type\n",
      "of hallucination in Table 1, accompanied by corre-\n",
      "sponding explanations. The details of our proposed\n",
      "categories are elaborated below:\n",
      "Factuality Hallucination. The emergence of\n",
      "LLMs marks a significant shift from traditional\n",
      "task-specific toolkits to AI assistants that have\n",
      "a heightened focus on open-domain interactions.\n",
      "This shift is primarily attributed to their vast para-\n",
      "metric factual knowledge. However, existing\n",
      "LLMs occasionally exhibit tendencies to produce\n",
      "outputs that are either inconsistent with real-world\n",
      "facts or potentially misleading, posing challenges\n",
      "to the trustworthiness of artificial intelligence. In\n",
      "this context, we categorize these factual errors as\n",
      "factuality hallucinations . Depending on whether\n",
      "the generated factual content can be verified against\n",
      "a reliable source, they can be further divided into\n",
      "two primary types:\n",
      "•Factual Inconsistency refers to situations\n",
      "where the LLM’s output contains facts that\n",
      "can be grounded in real-world information,\n",
      "but present contradictions. This type of\n",
      "hallucination occurs most frequently and\n",
      "arises from diverse sources, encompassing the\n",
      "LLM’s capture, storage, and expression of fac-but present contradictions. This type of\n",
      "hallucination occurs most frequently and\n",
      "arises from diverse sources, encompassing the\n",
      "LLM’s capture, storage, and expression of fac-\n",
      "tual knowledge. As shown in Table 1, when\n",
      "inquired about \"the first person to land on the\n",
      "Moon\" , the model erroneously generated \"Yuri\n",
      "Gagarin\" , which contradicts the real-world\n",
      "fact.\n",
      "•Factual Fabrication refers to instances where\n",
      "the LLM’s output contains facts that are unver-\n",
      "ifiable against established real-world knowl-\n",
      "edge. As demonstrated in Table 1, while \"the\n",
      "origins of unicorns\" traditionally lack empiri-\n",
      "cal grounding, the model fabricated a plausi-\n",
      "ble historical origin for unicorns.\n",
      "Faithfulness Hallucination. LLMs are inher-\n",
      "ently trained to align with user instructions. As\n",
      "the use of LLMs shifts towards more user-centric\n",
      "applications, ensuring their consistency with user-\n",
      "provided instructions and contextual informationHallucination Type Sub-Type User Input Model Output Explanation\n",
      "Factuality\n",
      "HallucinationFactual\n",
      "InconsistencyTell me about the first\n",
      "person to land on the\n",
      "Moon.Yuri Gagarin was the\n",
      "first person to land on\n",
      "the Moon.The LLM’s response is\n",
      "factual inconsistency as\n",
      "Neil Armstrong was\n",
      "the first person to land\n",
      "on the Moon , not Yuri\n",
      "Gagarin, who was the\n",
      "first human in space.\n",
      "Factual\n",
      "FabricationTell me about the histor-\n",
      "ical origins of unicorns.Unicorns were docu-\n",
      "mented to have roamed\n",
      "the plains of Atlantis\n",
      "around 10,000 BC,\n",
      "where they were consid-\n",
      "ered sacred creatures\n",
      "and were often associ-\n",
      "ated with royalty.The LLM’s response\n",
      "is a fabricated claim\n",
      "since there’s no ver-\n",
      "ified, real-world evi-\n",
      "dence supporting the\n",
      "existence of unicorns\n",
      "in Atlantis , especially\n",
      "in connection with roy-\n",
      "alty.\n",
      "Faithfulness\n",
      "HallucinationInstruction\n",
      "InconsistencyTranslate the following\n",
      "English question into\n",
      "Spanish: \"What is the\n",
      "capital of France?\"The capital of France is\n",
      "Paris.The user explicitly pro-\n",
      "vided an instruction to\n",
      "translate the English\n",
      "question into Spanish.\n",
      "Instead of perform-\n",
      "ing the translation as\n",
      "instructed, the LLM\n",
      "chose to answer the\n",
      "question.\n",
      "Context\n",
      "InconsistencySummarize the follow-\n",
      "ing paragraph about the\n",
      "Nile: The Nile is a ma-\n",
      "jor north-flowing river\n",
      "in northeastern Africa.\n",
      "It’s traditionally consid-\n",
      "ered the longest river in\n",
      "the world. Originating\n",
      "in the Great Lakes re-\n",
      "gion of central Africa,\n",
      "it flows through several\n",
      "countries before emp-\n",
      "tying into the Mediter-\n",
      "ranean Sea.The Nile originates\n",
      "from the mountain\n",
      "ranges of central Africa\n",
      "and flows into the\n",
      "Mediterranean.The LLM’s summary\n",
      "incorrectly states that\n",
      "the Nile originates from\n",
      "the \"mountain ranges of\n",
      "central Africa,\" while\n",
      "the paragraph clearly\n",
      "mentions that the Nile\n",
      "originates in the Great\n",
      "Lakes region of central\n",
      "Africa.\n",
      "Logical\n",
      "InconsistencySolve the equation 2x +\n",
      "3 = 11 step by step.Step 1: Subtract 3 from\n",
      "both sides to get: 2x =\n",
      "8. Step 2: Divide both\n",
      "sides by 2 to get: x =\n",
      "3. So the result of the\n",
      "equation is x=3.The first step correctly\n",
      "isolates the term with x\n",
      "to get 2x = 8. How-\n",
      "ever, the next step in-\n",
      "accurately divides 8 by\n",
      "2 to yield a result of x =\n",
      "3, which is inconsistent\n",
      "with the earlier reason-\n",
      "ing.\n",
      "Table 1: Examples of each category of LLM hallucinations. Content marked in Red represents the hallucinatory\n",
      "output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM\n",
      "hallucination.becomes increasingly vital. Furthermore, LLM’s\n",
      "faithfulness is also reflected in the logical consis-\n",
      "tency of its generated content. From this perspec-\n",
      "tive, we categorize three subtypes of faithfulness\n",
      "hallucinations:\n",
      "•Instruction inconsistency refers to the\n",
      "LLM’s outputs that deviate from a user’s di-\n",
      "rective. While some deviations might serve\n",
      "safety guidelines, the inconsistencies here\n",
      "signify unintentional misalignment with non-\n",
      "malicious user instructions. As described in\n",
      "Table 1, the user’s actual intention is transla-\n",
      "tion, However, the LLM erroneously deviated\n",
      "from the user’s instruction and performed a\n",
      "question-answering task instead.\n",
      "•Context inconsistency points to instances\n",
      "where the LLM’s output is unfaithful with the\n",
      "user’s provided contextual information. For\n",
      "example, as shown in Table 1, the user men-\n",
      "tioned the Nile’s source being in the Great\n",
      "Lakes region of central Africa, yet the LLM’s\n",
      "response contradicted the context.\n",
      "•Logical inconsistency underscores when\n",
      "LLM outputs exhibit internal logical contra-\n",
      "dictions, often observed in reasoning tasks.\n",
      "This manifests as inconsistency both among\n",
      "the reasoning steps themselves and between\n",
      "the steps and the final answer. For example,\n",
      "as shown in Table 1, while the reasoning step\n",
      "of dividing both sides of the equation by 2 is\n",
      "correct, the final answer of x=4 is inconsis-\n",
      "tent with the reasoning chain, leading to an\n",
      "incorrect result.\n",
      "3 Hallucination Causes\n",
      "Hallucinations have multifaceted origins, spanning\n",
      "the entire spectrum of LLMs’ capability acquisi-\n",
      "tion process. In this section, we delve into the root\n",
      "causes of hallucinations in LLMs, primarily catego-\n",
      "rized into three key aspects: Data (§3.1), Training\n",
      "(§3.2), and Inference (§3.3).\n",
      "3.1 Hallucination from Data\n",
      "Pre-training data stands as the bedrock for LLMs,\n",
      "enabling them to gain general capabilities and fac-\n",
      "tual knowledge (Zhou et al., 2023a). However,\n",
      "it can inadvertently become the source of LLM\n",
      "hallucinations. This mainly manifests in two as-\n",
      "pects: potential risks stemming from flawed datasources (§3.1.1), and the inferior utilization of fac-\n",
      "tual knowledge captured in the data (§3.1.2).\n",
      "3.1.1 Flawed Data Source\n",
      "While scaling up pre-training data substantially en-\n",
      "hances the competencies of LLMs (Kaplan et al.,\n",
      "2020; Hoffmann et al., 2022), challenges arise in\n",
      "maintaining consistent data quality, which can po-\n",
      "tentially introduce misinformation and biases (Ben-\n",
      "der et al., 2021; Weidinger et al., 2021). Moreover,\n",
      "the absence of specific domain knowledge and up-\n",
      "to-date facts in the data can lead the LLM to form\n",
      "knowledge boundaries, which pose limitations for\n",
      "LLMs in specific scenarios. Based on this, we pri-\n",
      "marily categorize the factors that could potentially\n",
      "lead to hallucinations into misinformation and bi-\n",
      "ases andknowledge boundary limitations . For a\n",
      "more comprehensive understanding, illustrative ex-\n",
      "amples of each type of data-induced hallucination\n",
      "are presented in Table 2.\n",
      "Misinformation and Biases. Given the in-\n",
      "creasing demand for large-scale corpora, heuristic\n",
      "data collection methods are employed to efficiently\n",
      "gather vast volumes of data. While providing ex-\n",
      "tensive data, they can inadvertently introduce erro-\n",
      "neous information, increasing the risk of imitative\n",
      "falsehoods . Additionally, social biases can inadver-\n",
      "tently be introduced into the LLMs’ learning pro-\n",
      "cess. These biases primarily include duplication\n",
      "bias and various social biases , potentially resulting\n",
      "in hallucinations.\n",
      "•Imitative Falsehoods. The primary objective\n",
      "of LLM pre-training is to mimic the training\n",
      "distribution. When LLMs are trained on fac-\n",
      "tual incorrect data, they may inadvertently am-\n",
      "plify these inaccuracies, potentially leading to\n",
      "factually incorrect hallucinations, termed as\n",
      "\"imitative falsehoods\" (Lin et al., 2022). For\n",
      "example, as shown in Table 2, the statement\n",
      "’Thomas Edison invented the light bulb’ is ac-\n",
      "tually a misconception that has been widely\n",
      "misbelieved over time. LLMs trained on suchexample, as shown in Table 2, the statement\n",
      "’Thomas Edison invented the light bulb’ is ac-\n",
      "tually a misconception that has been widely\n",
      "misbelieved over time. LLMs trained on such\n",
      "factual incorrect data can lead to misleading\n",
      "outputs.\n",
      "•Duplication Bias. Neural networks, espe-\n",
      "cially large language models, possess an in-\n",
      "trinsic tendency to memorize training data\n",
      "(Carlini et al., 2021). Studies (Carlini et al.,\n",
      "2022; Chowdhery et al., 2023) further show\n",
      "that this memorization tendency grows withType Premise Input Model Ouput Explanation\n",
      "Imitative False-\n",
      "hoodWithin the LLM’s train-\n",
      "ing data, a frequently\n",
      "referenced but incorrect\n",
      "fact appears multiple\n",
      "times: Thomas Edison\n",
      "invented the light bulb.Who is credited\n",
      "with the invention\n",
      "of the light bulb?Thomas Edison is\n",
      "credited with the in-\n",
      "vention of the light\n",
      "bulb.While Edison im-\n",
      "proved the light bulb’s\n",
      "design, he was not\n",
      "its sole inventor. The\n",
      "model’s answer re-\n",
      "flects a misconception\n",
      "prevalent in its training\n",
      "data, exemplifying an\n",
      "imitative falsehood.\n",
      "Duplication\n",
      "BiasWithin the LLM’s pre-\n",
      "training data, there is\n",
      "an overwhelming repe-\n",
      "tition of the statement\n",
      "that \"The most common\n",
      "red fruits are red ap-\n",
      "ples, watermelon, cher-\n",
      "ries, and strawberries.\"Please list some red\n",
      "fruits, excluding ap-\n",
      "ples.Red fruits are red\n",
      "apples, watermelon,\n",
      "cherries, and straw-\n",
      "berries.The model’s response\n",
      "is a hallucination\n",
      "inconsistent with the\n",
      "user’s direct inquiry\n",
      "about the red fruits,\n",
      "excluding apples. It\n",
      "instead reflects the\n",
      "model’s tendency to\n",
      "over-memorize the\n",
      "duplicated information\n",
      "within its training data.\n",
      "Social BiasesIn the model’s training\n",
      "data, certain names may\n",
      "frequently be associated\n",
      "with specific nationali-\n",
      "ties.Provide a summary\n",
      "of the following\n",
      "news: Dr. Kim\n",
      "recently published\n",
      "a breakthrough\n",
      "paper on quantum\n",
      "computing. The\n",
      "paper details the\n",
      "new algorithms\n",
      "and their potential\n",
      "applications in real-\n",
      "world scenarios. It\n",
      "has been hailed as\n",
      "revolutionary by\n",
      "experts in the field.Dr. Kim from South\n",
      "Korea recently pub-\n",
      "lished a revolution-\n",
      "ary paper on quan-\n",
      "tum computing, de-\n",
      "tailing new algo-\n",
      "rithms and their ap-\n",
      "plications.The model erroneously\n",
      "added \"South Korea\"\n",
      "based on the name\n",
      "\"Kim\" revealing its\n",
      "bias in linking certain\n",
      "names to specific na-\n",
      "tionalities, which can\n",
      "cause hallucinations in\n",
      "the summary.\n",
      "Table 2: Examples of Hallucinations from Misinformation and Biases. The table categorizes hallucinations arising\n",
      "from flawed data sources into imitative falsehoods, duplication bias, and social biases. Each category is accompanied\n",
      "by a premise outlining the data issue, user input, and the LLM’s hallucinatory output, and an explanation for the\n",
      "occurrence, aiding comprehension of these complex phenomena.\n",
      "model size. However, the inherent memo-\n",
      "rization capability becomes problematic in\n",
      "the context of duplicated information present\n",
      "within pre-training data (Lee et al., 2022a;\n",
      "Kandpal et al., 2023; Paullada et al., 2021).\n",
      "Such duplication can shift LLMs from gener-\n",
      "alization to memorization (Hernandez et al.,\n",
      "2022), ultimately giving rise to a duplication\n",
      "bias where LLMs over-prioritize the recall of\n",
      "duplicated data and lead to hallucinations that\n",
      "deviate from the desired content. In Table 2,\n",
      "when the user requests to \"list some red fruits,\n",
      "excluding apples,\" the presence of statements\n",
      "like \"red apples, watermelon, cherries, and\n",
      "strawberries\" frequently repeat in the training\n",
      "dataset leads the model to produce the over-\n",
      "memorized statement in its output.•Social Biases. Certain biases are intrinsi-\n",
      "cally tied to hallucinations, especially those\n",
      "related to gender (Paullada et al., 2021) and\n",
      "nationality (Narayanan Venkit et al., 2023;\n",
      "Ladhak et al., 2023). For instance, LLMs\n",
      "might associate the profession of nursing with\n",
      "females, even when gender isn’t explicitly\n",
      "mentioned in the user-provided context, exem-\n",
      "plifying context inconsistency hallucinations\n",
      "as discussed in Section (§2.3). Such biases\n",
      "can be inadvertently acquired from internet-\n",
      "based texts, which are rife with diverse and\n",
      "biased viewpoints, and subsequently be propa-\n",
      "gated into the generated content (Ladhak et al.,\n",
      "2023). Besides such biases, discrepancies in\n",
      "data distribution also pose a potential cause\n",
      "for hallucinations. In the context of the natu-ral language inference (NLI) task, McKenna\n",
      "et al. (2023) found that LLMs tend to falsely\n",
      "label by bias toward hypotheses affirmed in\n",
      "training data.\n",
      "Knowledge Boundary. While the vast pre-\n",
      "training corpora empower LLMs with extensive\n",
      "factual knowledge, they inherently possess bound-\n",
      "aries. This limitation primarily surfaces in two as-\n",
      "pects: the absence of up-to-date factual knowledge\n",
      "and specialized domain knowledge. An example is\n",
      "presented in Table 3.\n",
      "•Domain Knowledge Deficiency. LLMs have\n",
      "demonstrated remarkable performance across\n",
      "a wide range of downstream tasks in the\n",
      "generic domain. Nevertheless, given that\n",
      "these general-purpose LLMs are predomi-\n",
      "nantly trained on extensive publicly available\n",
      "datasets (Penedo et al., 2023; Raffel et al.,\n",
      "2020; Gao et al., 2021), their expertise in spe-\n",
      "cialized domains is inherently constrained by\n",
      "the absence of proprietary training data. As\n",
      "a result, when confronted with problems ne-\n",
      "cessitating domain-specific knowledge, such\n",
      "as medical (Li et al., 2023g; Singhal et al.,\n",
      "2023) and legal (Yu et al., 2022; Katz et al.,\n",
      "2023) questions, these models may exhibit\n",
      "pronounced hallucinations, often manifesting\n",
      "as factual fabrication.\n",
      "•Outdated Factual Knowledge. Beyond\n",
      "the shortfall in domain-specific knowledge,\n",
      "another intrinsic limitation concerning the\n",
      "knowledge boundaries within LLMs is their\n",
      "constrained capacity for up-to-date knowl-\n",
      "edge. The factual knowledge embedded\n",
      "within LLMs exhibits clear temporal bound-\n",
      "aries and can become outdated over time\n",
      "(Onoe et al., 2022; Kasai et al., 2022; Li et al.,\n",
      "2023a). Once these models are trained, their\n",
      "internal knowledge is never updated. This\n",
      "poses a challenge given the dynamic and ever-\n",
      "evolving nature of our world. When con-\n",
      "fronted with queries that transcend their tem-\n",
      "poral scope, LLMs often resort to fabricating\n",
      "facts or providing answers that might have\n",
      "been correct in the past but are now outdated.\n",
      "3.1.2 Inferior Data Utilization\n",
      "Pre-training data embodies a wealth of real-world\n",
      "factual knowledge, enabling LLMs to capture and\n",
      "subsequently encode vast of factual knowledgewithin their parameters (Petroni et al., 2019; Jiang\n",
      "et al., 2020; Roberts et al., 2020). However, despite\n",
      "this vast reservoir of knowledge, LLMs can still\n",
      "produce knowledge-induced hallucinations due to\n",
      "inferior utilization of parametric knowledge. In this\n",
      "context, we delve into two pivotal challenges: the\n",
      "spurious correlations in capturing factual knowl-\n",
      "edge and its struggles in knowledge recall. Ex-\n",
      "amples for each type of hallucination related to\n",
      "inferior data utilization are presented in Table 4 for\n",
      "further illustration.\n",
      "Knowledge Shortcut. While significant efforts\n",
      "have been undertaken in exploring their knowledge\n",
      "storage (Geva et al., 2021; Meng et al., 2022)and\n",
      "probing (Petroni et al., 2019; Zhong et al., 2021;\n",
      "Yu et al., 2023c), the exact mechanism by which\n",
      "LLMs capture the factual knowledge remains elu-\n",
      "sive. Recent studies (Li et al., 2022a; Kang and\n",
      "Choi, 2023; Kandpal et al., 2023) indicate that\n",
      "rather than genuinely understanding the intricacies\n",
      "of factual knowledge, LLMs often resort to short-\n",
      "cuts. They display a tendency to overly depend on\n",
      "positional close (Li et al., 2022a), co-occurrence\n",
      "statistics(Kang and Choi, 2023), and relevant doc-\n",
      "ument count (Kandpal et al., 2023) within the pre-\n",
      "training data, which can introduce a bias towards\n",
      "spurious correlations, potentially leading to hal-\n",
      "lucinations if the bias reflects factually incorrect\n",
      "information. For instance, as illustrated in Table 4,\n",
      "when queried about \"the capital of Canada\" , the\n",
      "model erroneously responds with \"Toronto\". This\n",
      "mistake might arise due to a higher co-occurrence\n",
      "frequency of Canada and Toronto in its training\n",
      "data, leading the model to incorrectly capture the\n",
      "factual knowledge about Canada’s capital.\n",
      "Knowledge Recall Failures. Hallucinations can\n",
      "arise when LLMs struggle to effectively leverage\n",
      "their extensive knowledge. We explore two primaryfactual knowledge about Canada’s capital.\n",
      "Knowledge Recall Failures. Hallucinations can\n",
      "arise when LLMs struggle to effectively leverage\n",
      "their extensive knowledge. We explore two primary\n",
      "challenges in knowledge recall: the inadequacy\n",
      "in recalling long-tail knowledge and difficulties in\n",
      "complex scenarios that require multi-hop reasoning\n",
      "and logical deduction.\n",
      "•Long-tail Knowledge Within the vast knowl-\n",
      "edge landscape that LLMs draw upon, a no-\n",
      "table challenge emerges in the form of long-\n",
      "tail knowledge utilization (Kandpal et al.,\n",
      "2023; Mallen et al., 2023). This long-tail\n",
      "knowledge, characterized by its relative rar-\n",
      "ity in pre-training data, poses inherent chal-\n",
      "lenges for LLMs, which primarily rely on\n",
      "co-occurrence patterns to memorize factualType Input Model Ouput Explanation\n",
      "Domain Knowledge De-\n",
      "ficiencyWhat causes phenylketonuria?Phenylketonuria is\n",
      "caused by a mutation in\n",
      "the hemoglobin gene,\n",
      "which results in an\n",
      "unusual interaction with\n",
      "hemoglobin, affecting\n",
      "its ability to transport\n",
      "oxygen in the blood-\n",
      "stream.Phenylketonuria (PKU)\n",
      "is actually caused by\n",
      "mutations in the gene\n",
      "that helps make an en-\n",
      "zyme called phenylala-\n",
      "nine hydroxylase . Due\n",
      "to the lack of special-\n",
      "ized medical knowledge\n",
      "in LLM’s training data,\n",
      "it may sometimes pro-\n",
      "vide incorrect informa-\n",
      "tion on medical topics.\n",
      "Outdated KnowledgeWhich city hosted the most re-\n",
      "cent Winter Olympics?The most recent\n",
      "city to host the Win-\n",
      "ter Olympics was\n",
      "Pyeongchang, South\n",
      "Korea, in 2018.The most recent city\n",
      "to host the Winter\n",
      "Olympics was Beijing,\n",
      "in 2022 . This is due to\n",
      "the fact that LLM stores\n",
      "outdated knowledge,\n",
      "and the answer to this\n",
      "question exhibits a\n",
      "time-shift phenomenon.\n",
      "Table 3: Example of Knowledge Boundary.\n",
      "knowledge. Consequently, when confronted\n",
      "with queries pertaining to such long-tail\n",
      "knowledge, LLMs are at a heightened risk\n",
      "of hallucination, attempting to generate fac-\n",
      "tually inaccurate responses. For instance, as\n",
      "shown in Table 4, when prompted to generate\n",
      "a biography for a long-tail entity previously\n",
      "encountered in Wikipedia training data, the\n",
      "LLM erroneously attributes the profession,\n",
      "mistakenly describing a politician as an ed-\n",
      "ucator.\n",
      "•Complex Scenario Beyond the challenges\n",
      "with long-tail knowledge, effective utilization\n",
      "of knowledge is inextricably linked with rea-\n",
      "soning capabilities. For instance, in multi-\n",
      "hop question-answering scenarios, even if the\n",
      "LLM possesses the necessary knowledge, it\n",
      "may struggle to produce accurate results if\n",
      "multiple associations exist between questions,\n",
      "due to its limitations in reasoning (Zheng\n",
      "et al., 2023). Furthermore, Berglund et al.\n",
      "(2023) unveiled a specific reasoning failure\n",
      "in LLMs termed the Reversal Curse . Specif-\n",
      "ically, while the model can correctly answer\n",
      "when the question is formulated as \"A is B\",\n",
      "it exhibits a failed logical deduction when\n",
      "asked the converse \"B is A\". This discrep-\n",
      "ancy in reasoning extends beyond simple de-\n",
      "ductions. In retrieval-augmented settings, Liu\n",
      "et al. (2023e) highlighted a related challenge.Despite having documents containing the cor-\n",
      "rect answers within the model’s context win-\n",
      "dow, the model still struggles to generate pre-\n",
      "cise responses due to its inadequacy in utiliz-\n",
      "ing the provided evidence effectively. As illus-\n",
      "trated in Table 4, although LLMs recognize\n",
      "Mount Everest as the world’s highest peak,\n",
      "they fail to determine which would become\n",
      "the highest mountain if Everest’s elevation\n",
      "were reduced by 500 meters, a task that re-\n",
      "quires complex reasoning ability.\n",
      "3.1.3 Summary\n",
      "Data-related hallucinations in LLMs are primar-\n",
      "ily rooted in flawed data sources and inferior data\n",
      "utilization. Misinformation and inherent biases\n",
      "within data sources not only propagate imitative\n",
      "falsehoods but also introduce biased outputs, lead-\n",
      "ing to various forms of hallucinations. Knowl-\n",
      "edge boundaries in LLMs become evident when\n",
      "handling domain-specific queries or encountering\n",
      "rapidly updating factual knowledge. Regarding\n",
      "data utilization, LLMs tend to capture spurious cor-\n",
      "relations and demonstrate difficulties in recalling\n",
      "knowledge, especially long-tail information, and\n",
      "in complex reasoning scenarios, further exacerbat-\n",
      "ing hallucinations. These challenges underscore\n",
      "the critical need for enhancing data quality and\n",
      "the models’ capabilities to learn and recall factual\n",
      "knowledge more effectively.Type Sub-Type User Input Model Output Explanation\n",
      "Knowledge\n",
      "ShortcutCo-occurrence\n",
      "BiasWhat is the capital of\n",
      "Canada?Toronto is the capital of\n",
      "Canada.The model leans heav-\n",
      "ily on the frequent co-\n",
      "occurrence of the terms\n",
      "Toronto andCanada in\n",
      "its training data, without\n",
      "truly capturing the fac-\n",
      "tual knowledge about\n",
      "the capital of Canada\n",
      "Knowledge\n",
      "Recall FailuresLong-tail\n",
      "KnowledgePlease generate a biog-\n",
      "raphy for George James\n",
      "Rankin.George James Rankin\n",
      "is a dedicated educator\n",
      "known for his contribu-\n",
      "tions to the field of ed-\n",
      "ucation and his passion\n",
      "for fostering learning.George James Rankin\n",
      "is actually a politi-\n",
      "cian, but the LLM may\n",
      "have difficulty effec-\n",
      "tively utilizing knowl-\n",
      "edge about this long-\n",
      "tail entity despite being\n",
      "trained on comprehen-\n",
      "sive Wikipedia data dur-\n",
      "ing pre-training.\n",
      "Complex\n",
      "ReasoningIf Mount Everest were\n",
      "to descend by 500\n",
      "meters, which mountain\n",
      "would become the\n",
      "world’s highest peak?If Mount Everest were\n",
      "to descend by 500 me-\n",
      "ters, it would still re-\n",
      "main the world’s high-\n",
      "est peak.The height of Mount\n",
      "Everest is 8844.43 me-\n",
      "ters, while K2’s height\n",
      "is 8611 meters. If\n",
      "Mount Everest were to\n",
      "descend by 500 me-\n",
      "ters, K2 would become\n",
      "the world’s highest peak.\n",
      "Facing complex multi-\n",
      "step reasoning questions\n",
      "like this, LLM may\n",
      "struggle to recall all the\n",
      "relevant knowledge as-\n",
      "sociated with it.\n",
      "Table 4: Examples of Inferior Data Utilization, showcasing the pitfalls of knowledge shortcuts and failures in\n",
      "knowledge recall. This includes instances where LLMs capture factual knowledge relying on co-occurrence statistics,\n",
      "as well as situations where it cannot recall relevant information from its parametric knowledge.\n",
      "3.2 Hallucination from Training\n",
      "The training process of LLMs mainly encompasses\n",
      "two primary stages: 1) the pre-training stage, where\n",
      "LLMs learn general-purpose representations and\n",
      "capture world knowledge, and 2) the alignment\n",
      "stage, where LLMs are adapted to better align with\n",
      "user instructions and preferences. While this pro-\n",
      "cess equips LLMs with remarkable capabilities,\n",
      "any shortfalls in these stages can inadvertently lead\n",
      "to hallucinations.\n",
      "3.2.1 Hallucination from Pre-training\n",
      "Pre-training serves as the foundational stage for\n",
      "LLMs, typically employing a transformer-based\n",
      "architecture to conduct causal language modeling\n",
      "on vast corpora. However, issues related to halluci-\n",
      "nation may arise from the inherent architectural de-\n",
      "sign and the particular training strategies employed.\n",
      "In this section, we delves into the challenges posedby the architecture flaw and impacts of exposure\n",
      "bias.\n",
      "Architecture Flaw. LLMs typically adopt\n",
      "a transformer-based architecture following the\n",
      "paradigm established by GPT (Radford et al., 2018,\n",
      "2019; Brown et al., 2020), where they acquire rep-\n",
      "resentations through a causal language modeling\n",
      "objective, a framework exemplified by models such\n",
      "as OPT(Zhang et al., 2022), Falcon (Penedo et al.,\n",
      "2023), and Llama-2 (Touvron et al., 2023). Despite\n",
      "its success, it is not without its pitfalls, particularly\n",
      "concerning Inadequate Unidirectional Representa-\n",
      "tionandAttention Glitches .\n",
      "•Inadequate Unidirectional Representation.\n",
      "Following the causal language modeling\n",
      "paradigm, LLMs predict the subsequent to-\n",
      "ken based solely on preceding tokens in a left-\n",
      "to-right manner. This unidirectional model-\n",
      "ing, while facilitating efficient training, alsohas its limitations. It exclusively utilizes con-\n",
      "text from a single direction, which hinders its\n",
      "ability to capture intricate contextual depen-\n",
      "dencies, potentially increasing risks for the\n",
      "emergence of hallucination (Li et al., 2023h).\n",
      "•Attention Glitches. Transformer-based ar-\n",
      "chitecture, equipped with the self-attention\n",
      "module, has shown remarkable capabilities\n",
      "in capturing long-range dependencies. How-\n",
      "ever, Recent research (Liu et al., 2023a) has\n",
      "shown that they can occasionally exhibit un-\n",
      "predictable reasoning errors in the context of\n",
      "algorithmic reasoning, spanning both long-\n",
      "range and short-range dependencies, regard-\n",
      "less of model scale. A potential cause is the\n",
      "limitations of soft attention (Hahn, 2020; Chi-\n",
      "ang and Cholak, 2022), where attention be-\n",
      "comes diluted across positions as sequence\n",
      "length increases.\n",
      "Exposure Bias. Beyond the architecture flaw,\n",
      "training strategies also play a crucial role. Notably,\n",
      "the phenomenon of exposure bias (Bengio et al.,\n",
      "2015; Ranzato et al., 2016) stands out, resulting\n",
      "from the disparity between training and inference in\n",
      "the auto-regressive generative model. During train-\n",
      "ing, these models typically employ a teacher-forced\n",
      "maximum likelihood estimation (MLE) training\n",
      "strategy where ground truth tokens are provided as\n",
      "input. However, during inference, the model relies\n",
      "on its own generated tokens for subsequent predic-\n",
      "tions. Such inconsistency can result in hallucina-\n",
      "tions (Wang and Sennrich, 2020), especially when\n",
      "an erroneous token generated by the model cas-\n",
      "cades errors throughout the subsequent sequence,\n",
      "akin to a snowball effect (Zhang et al., 2023c).\n",
      "3.2.2 Hallucination from Alignment\n",
      "Alignment, which typically involves two main pro-\n",
      "cesses, supervised fine-tuning and reinforcement\n",
      "learning from human feedback, serves as a crucial\n",
      "step toward unlocking the capabilities of LLMs\n",
      "and aligning them with human preferences. While\n",
      "alignment notably enhances the quality of LLM re-\n",
      "sponses, it also introduces the risk of hallucinations.\n",
      "In this section, we will categorize the alignment\n",
      "shortfalls related to hallucinations into two parts:\n",
      "Capability Misalignment andBelief Misalignment .\n",
      "Capability Misalignment. Considering that\n",
      "LLMs have inherent capability boundaries es-\n",
      "tablished during pre-training, SFT utilizes high-\n",
      "quality instructions along with their correspondingresponses to empower LLMs to follow user instruc-\n",
      "tions, unlocking their acquired abilities in this pro-\n",
      "cess. However, as the capabilities of LLMs expand,\n",
      "a significant challenge emerges: the potential mis-\n",
      "alignment between the LLMs’ intrinsic capabilities\n",
      "and those depicted in the annotation data. When\n",
      "the demands from alignment data exceed these pre-\n",
      "defined capability boundaries, LLMs are trained\n",
      "to produce content beyond their own knowledge\n",
      "boundaries, amplifying the risk of hallucinations\n",
      "(Schulman, 2023).\n",
      "Belief Misalignment. Several studies have\n",
      "demonstrated that LLM’s activations encapsulate\n",
      "an internal belief related to the truthfulness of its\n",
      "generated statements (Burns et al., 2022; Azaria\n",
      "and Mitchell, 2023). Nevertheless, misalignment\n",
      "can occasionally arise between these internal be-\n",
      "liefs and the generated outputs. Even when LLMs\n",
      "are refined with human feedback (Ouyang et al.,\n",
      "2022), they can sometimes produce outputs that\n",
      "diverge from their internal beliefs. Such behaviors,\n",
      "termed as sycophancy (Cotra, 2021), underscores\n",
      "the model’s inclination to appease human evalua-\n",
      "tors, often at the cost of truthfulness. Recent studies\n",
      "indicate that models trained via RLHF exhibit pro-\n",
      "nounced behaviors of pandering to user opinions.\n",
      "Such sycophantic behaviors are not restricted to\n",
      "ambiguous questions without definitive answers\n",
      "(Perez et al., 2023), like political stances, but can\n",
      "also arise when the model chooses a clearly incor-\n",
      "rect answer, despite being aware of its inaccuracy\n",
      "(Wei et al., 2023). Delving into this phenomenon,\n",
      "(Sharma et al., 2023) suggests that the root of syco-rect answer, despite being aware of its inaccuracy\n",
      "(Wei et al., 2023). Delving into this phenomenon,\n",
      "(Sharma et al., 2023) suggests that the root of syco-\n",
      "phancy may lie in the training process of RLHF\n",
      "models. By further exploring the role of human\n",
      "preferences in this behavior, the research indicates\n",
      "that the tendency for sycophancy is likely driven\n",
      "by both humans and preference models showing a\n",
      "bias towards sycophantic responses over truthful\n",
      "ones.\n",
      "3.2.3 Summary\n",
      "In training LLMs, both the foundational pre-\n",
      "training and the subsequent alignment present\n",
      "unique challenges that can induce hallucinations.\n",
      "During the pre-training stages, architecture flaws,\n",
      "notably inadequate unidirectional representation,\n",
      "and attention glitches, coupled with the well-known\n",
      "exposure bias, contribute to hallucinations. Mean-\n",
      "while, in the alignment phase, issues of capability\n",
      "misalignment and belief misalignment arise. The\n",
      "former risks pushing LLMs beyond their knowl-edge boundaries, while the latter reveals a disparity\n",
      "between the LLM’s beliefs and its outputs. These\n",
      "challenges underscore the importance of training\n",
      "LLMs to ensure their truthfulness. From founda-\n",
      "tional model designs and training strategies to align\n",
      "with human expectations, it remains a multifaceted\n",
      "endeavor.\n",
      "3.3 Hallucination from Inferece\n",
      "Decoding plays an important role in manifesting\n",
      "the capabilities of LLMs after pre-training and\n",
      "alignment. However, certain shortcomings within\n",
      "decoding strategies can lead to LLM hallucina-\n",
      "tions. In this section, we delve into potential causes\n",
      "rooted in the decoding process, emphasizing two\n",
      "critical factors: the inherent randomness of decod-\n",
      "ing strategies (§3.3.1) and imperfect decoding rep-\n",
      "resentation (§3.3.2).\n",
      "3.3.1 Inherent Sampling Randomness\n",
      "LLMs have demonstrated a remarkable aptitude for\n",
      "generating highly creative and diverse content, a\n",
      "proficiency that is critically dependent on the piv-\n",
      "otal role of randomness in their decoding strategies.\n",
      "Stochastic sampling (Fan et al., 2018; Holtzman\n",
      "et al., 2020) is currently the prevailing decoding\n",
      "strategy employed by these LLMs. The rationale\n",
      "for incorporating randomness into decoding strate-\n",
      "gies stems from the realization that high likelihood\n",
      "sequences often result in surprisingly low-quality\n",
      "text, which is called likelihood trap (Stahlberg and\n",
      "Byrne, 2019; Holtzman et al., 2020; Meister et al.,\n",
      "2020; Zhang et al., 2021). The diversity introduced\n",
      "by the randomness in decoding strategies comes\n",
      "at a cost, as it is positively correlated with an in-\n",
      "creased risk of hallucinations (Dziri et al., 2021a;\n",
      "Chuang et al., 2023). An elevation in the sam-\n",
      "pling temperature results in a more uniform token\n",
      "probability distribution, increasing the likelihood\n",
      "of sampling tokens with lower frequencies from the\n",
      "tail of the distribution. Consequently, this height-\n",
      "ened tendency to sample infrequently occurring\n",
      "tokens exacerbates the risk of hallucinations (Aksi-\n",
      "tov et al., 2023).\n",
      "3.3.2 Imperfect Decoding Representation\n",
      "During the decoding phase, LLMs use their top-\n",
      "layer representation to predict the next token. How-\n",
      "ever, the top-layer representation has its limitations,\n",
      "primarily manifested in two aspects: Insufficient\n",
      "Context Attention andSoftmax Bottleneck .Insufficient Context Attention. Prior stud-\n",
      "ies, particularly in domains like machine trans-\n",
      "lation (Miao et al., 2021) and summarization\n",
      "(Chen et al., 2022b), have highlighted the issue\n",
      "ofover-confidence in generative models employ-\n",
      "ing encoder-decoder architectures. Such over-\n",
      "confidence stems from an excessive focus on the\n",
      "partially generated content, often prioritizing flu-\n",
      "ency at the expense of faithfully adhering to the\n",
      "source context. While large language models, pri-\n",
      "marily adopting the causal language model archi-\n",
      "tecture, have gained widespread usage, the over-\n",
      "confidence phenomenon continues to persist. Dur-\n",
      "ing the generation process, the prediction of the\n",
      "next word is conditioned on both the language\n",
      "model context and the partially generated text.\n",
      "However, as demonstrated in prior studies (V oita\n",
      "et al., 2019; Beltagy et al., 2020; Liu et al., 2023e),\n",
      "language models often exhibit a localized focus\n",
      "within their attention mechanisms, giving priority\n",
      "to nearby words and resulting in a notable deficit\n",
      "in context attention (Shi et al., 2023b). Further-\n",
      "more, this concern is further amplified in LLMs\n",
      "that exhibit a proclivity for generating lengthy and\n",
      "comprehensive responses. In such cases, there is\n",
      "even a heightened susceptibility to the risk of in-\n",
      "struction forgetting (Chen et al., 2023f; Liu et al.,\n",
      "2023i). This insufficient attention can directly con-\n",
      "tribute to faithfulness hallucinations, wherein the\n",
      "model outputs content that deviates from the origi-\n",
      "nal context.\n",
      "Softmax Bottleneck. The majority of language\n",
      "models utilize a softmax layer that operates on\n",
      "the final layer’s representation within the language\n",
      "model, in conjunction with a word embedding, toSoftmax Bottleneck. The majority of language\n",
      "models utilize a softmax layer that operates on\n",
      "the final layer’s representation within the language\n",
      "model, in conjunction with a word embedding, to\n",
      "compute the ultimate probability associated with\n",
      "word prediction. Nevertheless, the efficacy of\n",
      "Softmax-based language models is impeded by a\n",
      "recognized limitation known as the Softmax bottle-\n",
      "neck (Yang et al., 2018a), wherein the employment\n",
      "of softmax in tandem with distributed word embed-\n",
      "dings is constrained the expressivity of the output\n",
      "probability distributions given the context which\n",
      "prevents LMs from outputting the desired distribu-\n",
      "tion. Additionally, Chang and McCallum (2022)\n",
      "discovered that when the desired distribution within\n",
      "the output word embedding space exhibits multiple\n",
      "modes, language models face challenges in accu-\n",
      "rately prioritizing words from all the modes as the\n",
      "top next words, which also introduces the risk of\n",
      "hallucination.3.3.3 Summary\n",
      "During the decoding phase, challenges arise from\n",
      "both the inherent decoding strategy and the repre-\n",
      "sentation utilized for predicting. The former, em-\n",
      "phasizing the randomness rooted in its decoding\n",
      "algorithm, can be a source of hallucinations as the\n",
      "randomness increases. While on the representation\n",
      "side, issues such as the over-reliance on nearby\n",
      "content and the softmax bottleneck can limit the\n",
      "model’s capability to express diverse output prob-\n",
      "abilities, leading to the risk of inaccurate token\n",
      "predictions. These complexities underscore the ne-\n",
      "cessity of maintaining factuality and faithfulness\n",
      "throughout the decoding process.\n",
      "4 Hallucination Detection and\n",
      "Benchmarks\n",
      "Hallucinations, as exhibited by LLMs, have gar-\n",
      "nered substantial attention due to their implications\n",
      "on model reliability and real-world deployment. As\n",
      "models become increasingly adept at generating\n",
      "human-like text, distinguishing between accurate\n",
      "and hallucinated content becomes a pivotal concern.\n",
      "Two primary facets encompass the broad spectrum\n",
      "of hallucination mitigation: detection mechanisms\n",
      "and evaluation benchmarks. This section serves as\n",
      "a deep dive into the state-of-the-art techniques for\n",
      "detecting hallucinations (§4.1) and the benchmarks\n",
      "(§4.2)that evaluate their prowess.\n",
      "4.1 Hallucination Detection\n",
      "Detecting hallucinations in LLMs is imperative\n",
      "for assuring the reliability and trustworthiness of\n",
      "the generated content. Traditional metrics, pre-\n",
      "dominantly hinged on word overlap, fall short in\n",
      "differentiating the nuanced discrepancies between\n",
      "plausible and hallucination content. Such a chal-\n",
      "lenge highlights the necessity for more sophisti-\n",
      "cated detection methods tailored to LLM hallucina-\n",
      "tions. Given the diverse nature of these hallucina-\n",
      "tions, detection approaches vary accordingly. Con-\n",
      "sequently, in this section, we provide a comprehen-\n",
      "sive overview of primary hallucination detection\n",
      "strategies, tailored to factuality and faithfulness\n",
      "hallucinations.\n",
      "4.1.1 Factuality Hallucination Detection\n",
      "Research by (Chen and Shu, 2023) underscored\n",
      "the challenge humans face in identifying ChatGPT-\n",
      "generated misinformation, leading to increasing\n",
      "studies aiming to design detection methods target-ing factuality hallucination. In this context, we\n",
      "propose an overview of established methods, typi-\n",
      "cally categorized into Retrieve External Facts and\n",
      "Uncertainty Estimation.\n",
      "Retrieve External Facts. To effectively pin-\n",
      "point factual inaccuracies in LLM outputs, one\n",
      "intuitive strategy involves comparing the model-\n",
      "generated content against reliable knowledge\n",
      "sources, as shown in Fig. 3. This methodology\n",
      "closely aligns with the workflow of fact-checking\n",
      "tasks, as delineated by (Guo et al., 2022). Nev-\n",
      "ertheless, traditional fact-checking methodologies\n",
      "(Augenstein et al., 2019; Hanselowski et al., 2019;\n",
      "Atanasova et al., 2020) often incorporate simpli-\n",
      "fied assumptions for practicality, leading to dis-\n",
      "crepancies when applied to complex real-world\n",
      "scenarios. Recognizing these constraints, Chen\n",
      "et al. (2023c) place greater emphasis on real-world\n",
      "scenarios, wherein evidence is procured from time-\n",
      "constrained, uncurated web sources. They have\n",
      "pioneered a fully automated pipeline that integrates\n",
      "multiple components: claim decomposition, raw\n",
      "document retrieval, fine-grained retrieval, claim-\n",
      "focused summarization, and veracity classification.\n",
      "Galitsky (2023) further addresses situations where\n",
      "potential conflict retrieval evidence by finding the\n",
      "least defeated authoritative source and avoiding\n",
      "the most defeated. Furthermore, Min et al. (2023)\n",
      "introduced FACTSCORE, a fine-grained factual\n",
      "metric specifically for long-form text generation.\n",
      "It decomposes the generation content into atomic\n",
      "facts and subsequently computes the percentage\n",
      "supported by reliable knowledge sources. Recently,\n",
      "Huo et al. (2023) enhanced the standard approach\n",
      "of retrieving supporting evidence for hallucination\n",
      "detection through query expansion. By combiningsupported by reliable knowledge sources. Recently,\n",
      "Huo et al. (2023) enhanced the standard approach\n",
      "of retrieving supporting evidence for hallucination\n",
      "detection through query expansion. By combining\n",
      "the original question with the LLM-generated an-\n",
      "swer during the retrieval process, they addressed\n",
      "topic drift concerns, ensuring that the retrieved pas-\n",
      "sages align with both the question and the LLM’s\n",
      "response. In a broader perspective, Chern et al.\n",
      "(2023) proposed a unified framework that enables\n",
      "LLMs to detect factual errors by leveraging a suite\n",
      "of external tools for evidence collection.\n",
      "Uncertainty Estimation. While many ap-\n",
      "proaches to hallucination detection rely on exter-\n",
      "nal knowledge sources for fact-checking, several\n",
      "methods have been devised to address this issue in\n",
      "zero-resource settings, thus eliminating the need for\n",
      "retrieval. The foundational premise behind these\n",
      "strategies is that the origin of LLM hallucinationsCheck\n",
      "RetrieveQuestion: What is the highest peak\n",
      "of the Himalayan mountain range?\n",
      "The highest peak of the Himalayan\n",
      "mountain range is Mount Everest\n",
      "The highest peak of the Himalayan\n",
      "mountain range is Mount Everest, also\n",
      "known as Qomolangma ...  located on\n",
      "the border between Nepal and China\n",
      "and was ﬁrst climbed in 1953. Figure 3: An example of detecting factuality hallucina-\n",
      "tion by retrieving external facts.\n",
      "is inherently tied to the model’s uncertainty. There-\n",
      "fore, by estimating the uncertainty of the factual\n",
      "content generated by the model, it becomes feasi-\n",
      "ble to detect hallucinations. The methodologies in\n",
      "uncertainty estimation can broadly be categorized\n",
      "into two approaches: based on internal states and\n",
      "LLM behavior , as shown in Fig. 4. The former\n",
      "operates under the assumption that one can access\n",
      "the model’s internal states, while the latter general-\n",
      "izes to more constrained environments, leveraging\n",
      "solely the model’s observable behaviors to infer its\n",
      "underlying uncertainty.\n",
      "•LLM Internal States. The internal states\n",
      "of LLMs can serve as informative indica-\n",
      "tors of their uncertainty, often manifested\n",
      "through metrics like token probability or en-\n",
      "tropy. Varshney et al. (2023) determine the\n",
      "model’s uncertainty towards key concepts\n",
      "quantified by considering the minimal token\n",
      "probability within those concepts. The under-\n",
      "lying rationale is that a low probability serves\n",
      "as a strong indicator of the model’s uncer-\n",
      "tainty, with less influence from higher proba-\n",
      "bility tokens present in the concept. Similarly,\n",
      "Luo et al. (2023a) employed a self-evaluation-\n",
      "based approach for uncertainty estimation by\n",
      "grounding in the rationale that a language\n",
      "model’s ability to adeptly reconstruct an orig-\n",
      "inal concept from its generated explanation\n",
      "is indicative of its proficiency with that con-\n",
      "cept. By initially prompting the model to gen-\n",
      "erate an explanation for a given concept and\n",
      "then employing constrained decoding to have\n",
      "the model recreate the original concept based\n",
      "on its generated explanation, the probabilityscore from the response sequence can serve\n",
      "as a familiarity score for the concept. Further-\n",
      "more, Yao et al. (2023a) interpreted halluci-\n",
      "nation through the lens of adversarial attacks.\n",
      "Utilizing gradient-based token replacement,\n",
      "they devised prompts to induce hallucinations.\n",
      "Notably, they observed that the first token gen-\n",
      "erated from a raw prompt typically exhibits\n",
      "low entropy, compared to those from adver-\n",
      "sarial attacks. Based on this observation, they\n",
      "proposed setting an entropy threshold to de-\n",
      "fine such hallucination attacks.\n",
      "•LLM Behavior. However, when systems are\n",
      "only accessible via API calls (OpenAI, 2022;\n",
      "Google, 2023; Microsoft, 2023), access to the\n",
      "output’s token-level probability distribution\n",
      "might be unavailable. Given this constraint,\n",
      "several studies have shifted their focus to prob-\n",
      "ing a model’s uncertainty, either through nat-\n",
      "ural language prompts (Xiong et al., 2023;\n",
      "Kadavath et al., 2022) or by examining its\n",
      "behavioral manifestations. For instance, by\n",
      "sampling multiple responses from an LLM for\n",
      "the same prompt, Manakul et al. (2023) detect\n",
      "hallucinations via evaluating the consistency\n",
      "among the factual statements. However, these\n",
      "methods predominantly rely on direct queries\n",
      "that explicitly solicit information or verifica-\n",
      "tion from the model. Agrawal et al. (2023),\n",
      "inspired by investigative interviews, advocate\n",
      "for the use of indirect queries. Unlike direct\n",
      "ones, these indirect counterparts often pose\n",
      "open-ended questions to elicit specific infor-\n",
      "mation. By employing these indirect queries,\n",
      "consistency across multiple model generations\n",
      "can be better evaluated. Beyond assessing un-\n",
      "certainty from the self-consistency of a sin-\n",
      "gle LLM’s multiple generations, one can em-\n",
      "brace a multi-agent perspective by incorpo-\n",
      "rating additional LLMs. Drawing inspiration\n",
      "from legal cross-examination practices, Co-\n",
      "hen et al. (2023) introduced the LMvLM ap-\n",
      "proach. This strategy leverages an ’examiner’rating additional LLMs. Drawing inspiration\n",
      "from legal cross-examination practices, Co-\n",
      "hen et al. (2023) introduced the LMvLM ap-\n",
      "proach. This strategy leverages an ’examiner’\n",
      "LM to question an ’examinee’ LM, aiming to\n",
      "unveil inconsistencies of claims during multi-\n",
      "turn interaction.\n",
      "4.1.2 Faithfulness Hallucination Detection\n",
      "Ensuring the faithfulness of LLMs to provide con-\n",
      "text or user directives is pivotal for their practical\n",
      "utility in a myriad of applications, from summariza-(1) Self-Consistency\n",
      "Mount Everest stands as the tallest\n",
      "peak in the world.\n",
      "As far as I know, the highest peak\n",
      "in the world is Mount Fuji in Japan.\n",
      "The highest peak is Mount Everest\n",
      "located in the Himalayas.Consistency\n",
      "The highest peak in the world\n",
      "is Mount Fuji.\n",
      "I stand corrected, you are right.I must correct you. Mount Fuji is the highest\n",
      "peak in Japan. The highest peak in the world\n",
      "is Mount Everest in the Himalayas range.\n",
      "(2) Multi-Debate\n",
      "(a) LLM Internal StatesQuestion: What is the highest peak in the world?\n",
      "Large Language ModelThe highest peak in the  world   is Mount Fuji.\n",
      "low uncertainty high uncertainty\n",
      "(b) LLM BehaviorFigure 4: Taxonomy of Uncertainty Estimation Methods in Factual Hallucination Detection, featuring a) LLM\n",
      "Internal States andb) LLM Behavior , with LLM Behavior encompassing two main categories: Self-Consistency\n",
      "and Multi-Debate.\n",
      "tion to interactive dialogue systems. Faithfulness\n",
      "hallucination detection primarily focuses on ensur-\n",
      "ing the alignment of the generated content with\n",
      "the given context, sidestepping the potential pit-\n",
      "falls of extraneous or contradictory output. In this\n",
      "subsection, we explore the methods to detect un-\n",
      "faithfulness in LLM generations and provide an\n",
      "overview of in Fig. 5.\n",
      "Fact-based Metrics. In the realm of assessing\n",
      "faithfulness, one of the most intuitive methods in-\n",
      "volves measuring the overlap of pivotal facts be-\n",
      "tween the generated content and the source content.\n",
      "Given the diverse manifestations of facts, metrics\n",
      "can be categorized based on entities, relation triples,\n",
      "and knowledge.\n",
      "•N-gram based. When treating the source\n",
      "content as the reference, traditional n-gram\n",
      "overlap-based evaluation metrics, such as\n",
      "ROUGE (Lin, 2004) and PARENT-T (Wang\n",
      "et al., 2020b), can also be applied to assess\n",
      "faithfulness. However, due to the natural diver-\n",
      "sity of language expression and their reliance\n",
      "on surface-level matching, these metrics show\n",
      "poor correlation with humans (Maynez et al.,\n",
      "2020).\n",
      "•Entity-based. Metrics based on entity over-\n",
      "lap are prevalently applied in summarization\n",
      "tasks, as any omission or inaccurate gener-\n",
      "ation of these key entities could lead to an\n",
      "unfaithful summary. Nan et al. (2021) intro-\n",
      "duced a metric to quantify the extent of entity\n",
      "hallucination, which calculates the precision\n",
      "of named-entities in the summary against the\n",
      "source entities.\n",
      "•Relation-based. Noting that even if entities\n",
      "match, the relations between them might beerroneous. Thus, Goodrich et al. (2019) focus\n",
      "on the overlap of relation tuples and introduce\n",
      "a metric that computes the overlap of relation\n",
      "tuples extracted using trained end-to-end fact\n",
      "extraction models.\n",
      "•Knowledge-based. Similarly, for knowledge-\n",
      "grounded dialogue tasks, facts often corre-\n",
      "spond to the knowledge presented in the di-\n",
      "alogue. Shuster et al. (2021) introduced the\n",
      "Knowledge F1 metric to assess how well the\n",
      "model’s generation aligns with the supplied\n",
      "knowledge.\n",
      "Classifier-based Metrics. Beyond computing\n",
      "fact overlap, another straightforward approach to\n",
      "assessing the faithfulness of the model involves uti-\n",
      "lizing classifiers trained on comprising both task-\n",
      "specific hallucinated and faithful content, as well\n",
      "as data from related tasks or synthetically gener-\n",
      "ated data. It can be broadly categorized into the\n",
      "following types:\n",
      "•Entailment-based. A prevailing concept in\n",
      "using Natural Language Inference (NLI) for\n",
      "assessing the faithfulness of generated text\n",
      "is anchored on the idea that genuinely faith-\n",
      "ful content should inherently be entailed by\n",
      "its source content. In line with this, numer-\n",
      "ous studies (Falke et al., 2019; Maynez et al.,\n",
      "2020) have trained classifiers on NLI datasets\n",
      "to identify factual inaccuracies, especially in\n",
      "the context of abstract summarization. How-\n",
      "ever, Mishra et al. (2021) highlighted that the\n",
      "mismatch in input granularity between con-\n",
      "ventional NLI datasets and inconsistency de-\n",
      "tection datasets limits their applicability for\n",
      "effectively detecting inconsistencies. Buildingmismatch in input granularity between con-\n",
      "ventional NLI datasets and inconsistency de-\n",
      "tection datasets limits their applicability for\n",
      "effectively detecting inconsistencies. Building\n",
      "on this, more advanced studies have proposedLLM GenerationUser Query\n",
      "Information\n",
      "Extraction\n",
      "Information\n",
      "ExtractionFacts\n",
      "Fact Overlap\n",
      "Question\n",
      "Answering\n",
      "Answer\n",
      "Overlap\n",
      "Question\n",
      "GenerationAnswers\n",
      "Answer\n",
      "SelectionAnswersQuestions\n",
      "binary\n",
      "judgement\n",
      "k-point\n",
      "Likert scaleLLM GenerationUser Query\n",
      "Entailment\n",
      " NLI Model\n",
      "LLM GenerationUser Query\n",
      "User Query\n",
      "LLM Generation(a) Fact-based Metric\n",
      "(b) Classiﬁer-based Metric\n",
      "(e) Prompting-based Metric(c) QA-based Metric(d) Uncertainty EstimationFacts\n",
      "User Query\n",
      "EOS\n",
      "Large Language Modelhigh uncertainty\n",
      "low uncertaintyFigure 5: The illustration of detection methods for faithfulness hallucinations: a) Fact-based Metrics , which\n",
      "assesses faithfulness by measuring the overlap of facts between the generated content and the source content; b)\n",
      "Classifier-based Metrics , utilizing trained classifiers to distinguish the level of entailment between the generated\n",
      "content and the source content; c) QA-based Metrics , employing question-answering systems to validate the\n",
      "consistency of information between the source content and the generated content; d) Uncertainty Estimation ,\n",
      "which assesses faithfulness by measuring the model’s confidence in its generated outputs; e) Prompting-based\n",
      "Metrics , wherein LLMs are induced to serve as evaluators, assessing the faithfulness of generated content through\n",
      "specific prompting strategies.\n",
      "methods such as fine-tuning on adversarial\n",
      "datasets (Barrantes et al., 2020), decomposing\n",
      "the entailment decisions at the dependency arc\n",
      "level (Goyal and Durrett, 2020), and segment-\n",
      "ing documents into sentence units then aggre-\n",
      "gating scores between sentence pairs (Laban\n",
      "et al., 2022). These collective efforts under-\n",
      "score the potential to enhance the accuracy of\n",
      "hallucination detection.\n",
      "•Weekly Supervised. While using data from\n",
      "related tasks to fine-tune the classifier has\n",
      "shown promise in evaluating faithfulness, it’s\n",
      "essential to recognize the inherent gap be-\n",
      "tween related tasks and the downstream task.\n",
      "The scarcity of annotated data further con-\n",
      "strains their applicability. In response to\n",
      "this challenge, Kryscinski et al. (2020) ana-\n",
      "lyzed errors made by cutting-edge summariza-\n",
      "tion models and introduced a method using\n",
      "rule-based transformations to create weakly-\n",
      "supervised data for fine-tuning the classifier.\n",
      "Concurrently, Zhou et al. (2021) devised an\n",
      "approach to automatically generate token-\n",
      "level hallucination data and perform token-level hallucination detection. Building upon\n",
      "the work of (Kryscinski et al., 2020), Dziri\n",
      "et al. (2021b) utilized perturbation methods to\n",
      "generate adversarial synthetic data aiming to\n",
      "enhance hallucination detection in knowledge-\n",
      "grounded dialogue tasks while Santhanam\n",
      "et al. (2021) focues on factual consistency for\n",
      "the conversation domain.\n",
      "Question-Answering based Metrics. In con-\n",
      "trast to classifier-based metrics, QA-based metrics\n",
      "have recently garnered attention for their enhanced\n",
      "ability to capture information overlap between the\n",
      "model’s generation and its source. These metrics\n",
      "operate by initially selecting target answers from\n",
      "the information units within the LLM’s output,\n",
      "and then questions are generated by the question-\n",
      "generation module. The questions are subsequently\n",
      "used to generate source answers based on the user\n",
      "context. Finally, the faithfulness of the LLM’s\n",
      "responses is calculated by comparing the match-\n",
      "ing scores between the source and target answers.\n",
      "Notable implementations include (Durmus et al.,\n",
      "2020; Wang et al., 2020a; Scialom et al., 2021;\n",
      "Honovich et al., 2021). Although these method-ologies share a common thematic approach, they\n",
      "exhibit variability in aspects like answer selection,\n",
      "question generation, and answer overlap, leading\n",
      "to diverse performance outcomes. Building on this\n",
      "foundational work, Fabbri et al. (2022) conducted\n",
      "an in-depth evaluation of the components within\n",
      "QA-based metrics, yielding further enhancements\n",
      "in faithfulness evaluation.\n",
      "Uncertainty Estimation. Drawing from the in-\n",
      "sights in Section (§4.1.1), hallucinations in con-\n",
      "ditional text generation are closely tied to high\n",
      "model uncertainty. Uncertainty estimation has been\n",
      "widely explored in Bayesian deep learning (Blun-\n",
      "dell et al., 2015; Gal and Ghahramani, 2016; Lak-\n",
      "shminarayanan et al., 2017). From a Bayesian per-\n",
      "spective, the total uncertainty of a prediction is\n",
      "characterized by the predictive entropy of the out-\n",
      "put distribution. Moreover, some works (Malinin\n",
      "and Gales, 2021) have sought to quantify model\n",
      "uncertainty using log probability. Based on these\n",
      "principles, we categorize the existing approaches\n",
      "for hallucination detection via uncertainty estima-\n",
      "tion into the following types:\n",
      "•Entropy based. Xiao and Wang (2021) ob-\n",
      "served a positive correlation between hallu-\n",
      "cination likelihood in data-to-text generation\n",
      "and predictive uncertainty, which is estimated\n",
      "by deep ensembles (Lakshminarayanan et al.,\n",
      "2017). Furthermore, Guerreiro et al. (2023a)\n",
      "leveraged the variance in hypotheses yielded\n",
      "by Monte Carlo Dropout (Gal and Ghahra-\n",
      "mani, 2016) as an uncertainty measure within\n",
      "Neural Machine Translation (NMT). More re-\n",
      "cently, van der Poel et al. (2022) employed\n",
      "conditional entropy (Xu et al., 2020) to assess\n",
      "model uncertainty in abstractive summariza-\n",
      "tion.\n",
      "•log-probability-based. Guerreiro et al.\n",
      "(2023a) use length-normalised sequence log-\n",
      "probability to measure model confidence.\n",
      "•Model based. Miao et al. (2023) concen-\n",
      "trates on error detection in complex reason-\n",
      "ing by employing SelfCheck, a step-by-step\n",
      "checker that evaluates each reasoning step\n",
      "within LLMs. The system aggregates confi-\n",
      "dence scores through a streamlined process of\n",
      "target extraction, information collection, step\n",
      "regeneration, and result comparison, thereby\n",
      "enhancing question-answering accuracy.Prompting-based Metrics. Recently, the re-\n",
      "markable instruction-following ability of LLMs\n",
      "has underscored their potential for automatic eval-\n",
      "uation (Chiang and Lee, 2023; Liu et al., 2023g;\n",
      "Wang et al., 2023d). Exploiting this capability, re-\n",
      "searchers have ventured into novel paradigms for\n",
      "assessing the faithfulness of model-generated con-\n",
      "tent (Luo et al., 2023b; Laban et al., 2023; Adlakha\n",
      "et al., 2023; Gao et al., 2023b; Jain et al., 2023). By\n",
      "providing LLMs with concrete evaluation guide-\n",
      "lines and feeding them both the model-generated\n",
      "and source content, they can effectively assess faith-\n",
      "fulness. The final evaluation output can either be a\n",
      "binary judgment on faithfulness (Luo et al., 2023b)\n",
      "or a k-point Likert scale indicating the degree of\n",
      "faithfulness (Gao et al., 2023b). For prompt selec-\n",
      "tion, evaluation prompt can either be direct prompt-\n",
      "ing, chain-of-thought prompting (Adlakha et al.,\n",
      "2023), using in-context-learning (Jain et al., 2023)\n",
      "or allowing the model to generate evaluation re-\n",
      "sults accompanying with explanations (Laban et al.,\n",
      "2023).\n",
      "4.2 Benchmarks\n",
      "In this section, we present a comprehensive\n",
      "overview of existing hallucination benchmarks,\n",
      "which can be categorized into two primary do-\n",
      "mains: Hallucination Evaluation Benchmarks\n",
      "(§4.2.1), which assess the extent of hallucinations\n",
      "generated by existing cutting-edge LLMs, and\n",
      "Hallucination Detection Benchmarks (§4.2.2), de-\n",
      "signed specifically to evaluate the performance of\n",
      "existing hallucination detection methods. Collec-\n",
      "tively, these benchmarks establish a unified frame-\n",
      "work, enabling a nuanced and thorough exploration\n",
      "of hallucinatory patterns in LLMs.\n",
      "4.2.1 Hallucination Evaluation Benchmarks\n",
      "Hallucination evaluation benchmarks are devised\n",
      "to assess LLMs’ proclivity to produce hallucina-of hallucinatory patterns in LLMs.\n",
      "4.2.1 Hallucination Evaluation Benchmarks\n",
      "Hallucination evaluation benchmarks are devised\n",
      "to assess LLMs’ proclivity to produce hallucina-\n",
      "tions, with a particular emphasis on identifying\n",
      "factual inaccuracies and measuring deviations from\n",
      "original contexts. Presently, the primary focus of\n",
      "these benchmarks is on evaluating the factuality\n",
      "of LLM-generated content. While most are struc-\n",
      "tured in a question-answering format, their primary\n",
      "focus remains on LLM factuality. Their unique\n",
      "characteristics stem from the selected knowledge\n",
      "domain, language, and response format they em-\n",
      "ploy. We present an overview of the most represen-\n",
      "tative benchmarks in detail below and concurrently\n",
      "provide an evaluation of common LLMs’ perfor-Attribute Task\n",
      "Benchmark Datasets Data Size LanguageFactuality Faithfulness Manual Task Type Input Label Metric\n",
      "Generative QA TruthfulQA\n",
      "(Lin et al., 2022)- 817 English ✔ ✗ ✔Multi-Choice QAQuestion AnswerLLM-Judge &\n",
      "Human\n",
      "Multi-Choice QA Acc REALTIMEQA\n",
      "(Kasai et al., 2022)- Dynamic English ✔ ✗ ✔Generative QAQuestion AnswerEM & F1\n",
      "SelfCheckGPT-Wikibio\n",
      "(Miao et al., 2023)- 1,908 English ✗ ✔ ✗ DetectionParagraph &\n",
      "ConceptPassage AUROC\n",
      "Task-specific 30,000 English ✗ ✔ ✗ Detection Query Response Acc HaluEval\n",
      "(Li et al., 2023c) General 5,000 English ✗ ✔ ✗ Detection Task Input Response Acc\n",
      "Med-HALT\n",
      "(Umapathi et al., 2023)- 4,916 Multilingual ✔ ✗ ✗ Multi-Choice QA Question ChoicePointwise Score\n",
      "& Acc\n",
      "Wiki-FACTOR 2,994 English ✔ ✗ ✗ Multi-Choice QA Question Answer likelihood FACTOR\n",
      "(Muhlgay et al., 2023) News-FACTOR 1,036 English ✔ ✗ ✗ Multi-Choice QA Question Answer likelihood\n",
      "SenHallu 200 English ✗ ✔ ✗ Detection Paper Summary P & R & F1 BAMBOO\n",
      "(Dong et al., 2023) AbsHallu 200 English ✗ ✔ ✗ Detection Paper Summary P & R & F1\n",
      "ChineseFactEval\n",
      "(Wang et al., 2023a)- 125 Chinese ✔ ✗ ✔ Generative QA Question - Score\n",
      "Misleading 175 Chinese ✔ ✗ ✔ Generative QA Question Answer LLM-Judge\n",
      "Misleading-hard 69 Chinese ✔ ✗ ✔ Generative QA Question Answer LLM-JudgeHaluQA\n",
      "(Cheng et al., 2023)\n",
      "Knowledge 206 Chinese ✔ ✗ ✔ Generative QA Question Answer LLM-Judge\n",
      "Never-changing 150 English ✔ ✗ ✔ Generative QA Question Answer Human\n",
      "Slow-changing 150 English ✔ ✗ ✔ Generative QA Question Answer Human\n",
      "Fast-changing 150 English ✔ ✗ ✔ Generative QA Question Answer HumanFreshQA\n",
      "(Vu et al., 2023)\n",
      "False-premise 150 English ✔ ✗ ✔ Generative QA Question Answer Human\n",
      "FELM\n",
      "(Chen et al., 2023d)- 3,948 English ✔ ✔ ✗ Detection Question ResponseBalanced\n",
      "Acc & F1\n",
      "PHD-LOW 100 English ✗ ✔ ✗ Detection Entity Response P & R & F1\n",
      "PHD-Meidum 100 English ✗ ✔ ✗ Detection Entity Response P & R & F1PHD\n",
      "(Yang et al., 2023)\n",
      "PHD-High 100 English ✗ ✔ ✗ Detection Entity Response P & R & F1\n",
      "ScreenEval\n",
      "(Lattimer et al., 2023)- 52 English ✗ ✔ ✗ Detection Document Summary AUROC\n",
      "COVID-QA N/A English ✗ ✔ ✗ Detection Question Answer AUROC\n",
      "DROP N/A English ✗ ✔ ✗ Detection Question Answer AUROC\n",
      "Open Assistant N/A English ✗ ✔ ✗ Detection Question Answer AUROCRealHall\n",
      "(Friel and Sanyal, 2023)\n",
      "TriviaQA N/A English ✗ ✔ ✗ Detection Question Answer AUROC\n",
      "LSum\n",
      "(Feng et al., 2023a)- 6,166 English ✗ ✔ ✗ Detection Document Summary Balanced Acc\n",
      "HotpotQA 250 English ✗ ✔ ✗ Detection Question Answer AUROC SAC3\n",
      "(Zhang et al., 2023a) NQ-Open 250 English ✗ ✔ ✗ Detection Question Answer AUROC\n",
      "Table 5: An overview of existing hallucination benchmarks. For Attribute, Factuality andFaithfulness represent\n",
      "whether the benchmark is used to evaluate LLM’s factuality or to detect faithfulness hallucination, and Manual\n",
      "represents whether the inputs in the data are handwritten.\n",
      "mances on these benchmarks in the (§A).\n",
      "TruthfulQA (Lin et al., 2022). Comprising 817\n",
      "questions that span 38 diverse categories, such as\n",
      "health, law, finance, and politics, TruthfulQA is a\n",
      "benchmark specifically designed to assess the truth-\n",
      "fulness of language models. Crafted using an adver-\n",
      "sarial methodology, it aims to elicit \"imitative false-\n",
      "hoods\"—misleading responses that models might\n",
      "generate due to their frequent presence in training\n",
      "data. The benchmark is divided into two parts, one\n",
      "of which contains manually curated questions that\n",
      "were further refined by filtering out those correctly\n",
      "answered by GPT-3, resulting in 437 filtered ques-\n",
      "tions. The other part includes 380 unfiltered non-\n",
      "adversarial questions. For evaluation, TruthfulQA\n",
      "offers two types of question-answering tasks: gen-\n",
      "eration and multiple-choice, with human evalua-\n",
      "tion employed to gauge the models’ truthfulnessand informativeness. Moreover, the benchmark in-\n",
      "troduces an automatic metric named GPT-judge,\n",
      "which is fine-tuned on a 6.7B GPT-3 model.\n",
      "REALTIMEQA (Kasai et al., 2022). Consid-\n",
      "ering that world knowledge is constantly evolving,troduces an automatic metric named GPT-judge,\n",
      "which is fine-tuned on a 6.7B GPT-3 model.\n",
      "REALTIMEQA (Kasai et al., 2022). Consid-\n",
      "ering that world knowledge is constantly evolving,\n",
      "it becomes crucial to validate the LLM’s factual-\n",
      "ity concerning the current world. This benchmark\n",
      "offers real-time open-domain multiple-choice ques-\n",
      "tions derived from newly-published news articles,\n",
      "spanning diverse topics such as politics, business,\n",
      "sports, and entertainment. Additionally, the bench-\n",
      "mark provides a platform for real-time evaluations,\n",
      "either through a multiple-choice format assessed\n",
      "by accuracy or a generation setting evaluated using\n",
      "exact matching and token-based F1 metrics.\n",
      "Med-HALT (Umapathi et al., 2023). Given\n",
      "the critical consequences of hallucinations in the\n",
      "medical domain on patient care, the benchmark em-phasizes challenges specific to LLMs in the med-\n",
      "ical domain. Med-HALT, incorporating multiple-\n",
      "choice questions from various countries, is tailored\n",
      "to assess LLMs’ reasoning and memorization in the\n",
      "medical context. The reasoning task, with 18,866\n",
      "samples, tests LLMs’ ability to distinguish incor-\n",
      "rect or irrelevant options and fake questions by\n",
      "using multiple-choice medical questions. Mean-\n",
      "while, the memory task, comprising 4,916 samples,\n",
      "evaluates LLMs’ ability to recall and generate accu-\n",
      "rate factual information by either generating links\n",
      "from a PubMed abstract/title or producing titles\n",
      "from given links and PMIDs. For evaluation, the\n",
      "performance of LLMs is measured either by their\n",
      "accuracy on test questions or by a Pointwise Score\n",
      "that considers both the positive scores for correct\n",
      "answers and a negative penalty for incorrect ones.\n",
      "FACTOR (Muhlgay et al., 2023). To quantita-\n",
      "tively assess LM factuality, Muhlgay et al. (2023)\n",
      "introduced a method for automatically creating\n",
      "benchmarks by perturbing factual statements from\n",
      "a designated corpus. resulting in two benchmarks:\n",
      "Wiki-FACTOR and News-FACTOR. Specifically,\n",
      "for a given prefix text, the original completion from\n",
      "the corpus serves as the factually correct answer. In-\n",
      "structGPT is then guided with prompts that contain\n",
      "specific error types to generate non-factual comple-\n",
      "tions. These generated responses are subsequently\n",
      "filtered for fluency and self-consistency, serving\n",
      "as the foundation for multi-choice tasks. For eval-\n",
      "uation, an LM’s factuality is gauged by whether\n",
      "the likelihood of the model producing the factually\n",
      "correct completion exceeds that of generating other\n",
      "non-factual completions.\n",
      "ChineseFactEval (Wang et al., 2023a). By\n",
      "gathering questions from diverse domains such as\n",
      "general knowledge, scientific research, medicine,\n",
      "law, finance, mathematics, and modern Chinese\n",
      "history, ChineseFactEval employed 125 questions\n",
      "to evaluate the factual capabilities of six contem-\n",
      "porary Chinese LLMs, alongside GPT-4. For eval-\n",
      "uation, questions are categorized based on the ac-\n",
      "curacy achieved by various LLMs, with different\n",
      "scores assigned to questions of varying difficulty.\n",
      "The responses from all LLMs are primarily anno-\n",
      "tated by humans, supplemented by FacTool (Chern\n",
      "et al., 2023). The final scores of the LLMs are then\n",
      "used to assess their factuality.\n",
      "HalluQA (Cheng et al., 2023). Drawing from\n",
      "the construction approach of TruthfulQA (Lin et al.,\n",
      "2022), HalluQA is crafted to specifically assesshallucinations in Chinese large language models,\n",
      "focusing on imitative falsehoods and factual er-\n",
      "rors. The benchmark comprises 450 handcrafted\n",
      "adversarial questions across 30 domains and is\n",
      "categorized into two parts. The misleading sec-\n",
      "tion captures questions that successfully deceive\n",
      "GLM-130B, while the knowledge section retains\n",
      "questions that both ChatGPT and Puyu consistently\n",
      "answer incorrectly. For evaluation, LLMs gener-\n",
      "ate responses to these questions, which are then\n",
      "compared with correct answers using GPT-4 to de-\n",
      "termine whether an answer contains hallucinations.\n",
      "FreshQA (Vu et al., 2023). Recognizing that\n",
      "hallucinations can partially arise from outdated\n",
      "knowledge within LLMs, the benchmark is intro-\n",
      "duced to evaluate the factuality of existing LLMs.\n",
      "Comprising 600 hand-crafted questions whose an-\n",
      "swers may change over time or whose premises\n",
      "are factually incorrect, this benchmark primarily\n",
      "evaluates the LLMs’ aptitude for fast-changing\n",
      "knowledge and their ability to identify questions\n",
      "with false premises. For evaluation, the benchmark\n",
      "provides a two-mode evaluation procedure: RE-\n",
      "LAXED, which solely evaluates the correctness of\n",
      "the primary answer, and STRICT, which further\n",
      "assesses the accuracy of every fact within the an-\n",
      "swer. In both modes, the factuality of the LLM\n",
      "is reflected by the accuracy of its responses, as\n",
      "determined through human annotations.\n",
      "4.2.2 Hallucination Detection Benchmarks\n",
      "For hallucination detection benchmarks, mostis reflected by the accuracy of its responses, as\n",
      "determined through human annotations.\n",
      "4.2.2 Hallucination Detection Benchmarks\n",
      "For hallucination detection benchmarks, most\n",
      "prior studies have primarily concentrated on task-\n",
      "specific hallucinations, such as abstractive summa-\n",
      "rization (Kryscinski et al., 2020; Wang et al., 2020a;\n",
      "Maynez et al., 2020; Fabbri et al., 2021; Goyal and\n",
      "Durrett, 2021; Pagnoni et al., 2021; Tang et al.,\n",
      "2022), data-to-text(Tian et al., 2019; Parikh et al.,\n",
      "2020), and machine translation (Zhou et al., 2021).\n",
      "However, the content generated in these studies of-\n",
      "ten originates from models with lesser capabilities,\n",
      "such as BART (Lewis et al., 2020a) and PEGASUS\n",
      "(Zhang et al., 2020). As a result, they may not\n",
      "accurately reflect the effectiveness of hallucination\n",
      "detection strategies. Therefore, such studies fall\n",
      "outside the scope of our current discussion.\n",
      "SelfCheckGPT-Wikibio (Miao et al., 2023).\n",
      "Miao et al. (2023) introduced a sentence-level hal-\n",
      "lucination detection dataset by generating synthetic\n",
      "Wikipedia articles using GPT-3, based on concepts\n",
      "from the WikiBio dataset. The factuality of these\n",
      "passages was then manually annotated at the sen-tence level, yielding a total of 1908 sentences for\n",
      "238 articles.\n",
      "HaluEval (Li et al., 2023c). To assess the capa-\n",
      "bility of LLMs in recognizing hallucination, HaluE-\n",
      "val was constructed using a combination of auto-\n",
      "mated generation and human annotation, yielding\n",
      "5,000 general user queries paired with ChatGPT\n",
      "responses and 30,000 task-specific samples. The\n",
      "automated generation employed a \"sampling-then-\n",
      "filtering\" approach. Drawing upon task-specific\n",
      "datasets from question answering, knowledge-\n",
      "grounded dialogue, and text summarization, the\n",
      "benchmark initially uses ChatGPT to sample multi-\n",
      "faceted hallucinated answers based on task-related\n",
      "hallucination patterns and then select the most plau-\n",
      "sible hallucinated samples by ChatGPT. For human\n",
      "annotation, Alpaca-sourced queries were processed\n",
      "by ChatGPT to sample multiple responses, which\n",
      "were then manually assessed for the presence of\n",
      "hallucinated content.\n",
      "BAMBOO (Dong et al., 2023). Expanding upon\n",
      "the methodologies introduced by Li et al. (2023c),\n",
      "this benchmark introduces two new datasets, Sen-\n",
      "Hallu and AbsHallu, aimed at detecting hallucina-\n",
      "tion in the context of long texts. These datasets\n",
      "are constructed by inducing ChatGPT to generate\n",
      "hallucinations given academic papers, resulting in\n",
      "200 samples, respectively.\n",
      "FELM (Chen et al., 2023d). Unlike previ-\n",
      "ous studies that predominantly focused on specific\n",
      "tasks such as summarization (Fabbri et al., 2021;\n",
      "Tang et al., 2022)or particular domains such as\n",
      "world knowledge (Miao et al., 2023), this bench-\n",
      "mark assesses factuality across five domains: world\n",
      "knowledge, science and technology, mathematics,\n",
      "writing and recommendation, and reasoning. While\n",
      "earlier research intentionally induced LLMs to hal-\n",
      "lucinate based on specific patterns (Li et al., 2023c),\n",
      "this benchmark employs ChatGPT to generate re-\n",
      "sponses in a zero-shot setting, yielding a total of\n",
      "817 samples (comprising 3948 segments). Each\n",
      "segment is annotated for factuality, error reasons,\n",
      "error type, and external references. Serving as a\n",
      "testbed for factuality detectors, the benchmark em-\n",
      "ploys the F1 score and balanced classification accu-\n",
      "racy to evaluate factual errors at both the segment\n",
      "and response levels.\n",
      "PHD (Yang et al., 2023). Rather than fo-\n",
      "cusing on sentence-level hallucination detection,\n",
      "the benchmark emphasizes passage-level detection.\n",
      "The construction of the benchmark begins with theextraction of entities from the Wikipedia dump,\n",
      "followed by generating passages using ChatGPT.\n",
      "Recognizing that factuality errors often arise when\n",
      "LLMs lack sufficient knowledge, the benchmark se-\n",
      "lects entities based on the number of related items\n",
      "returned by Google Search. This categorization\n",
      "results in three distinct groups: PHD-Low, PHD-\n",
      "Medium, and PHD-High. From each category, 100\n",
      "entities are sampled and then human-annotated at\n",
      "the passage level as factual ,non-factual , orun-\n",
      "verifiable . For the evaluation process, the bench-\n",
      "mark employs Precision, Recall, and F1 measures\n",
      "to assess the effectiveness of methods in detecting\n",
      "non-factual passages.\n",
      "ScreenEval (Lattimer et al., 2023). Building\n",
      "upon existing research predominantly focused on\n",
      "short documents, the ScreenEval benchmark ex-\n",
      "tends the scope to factual inconsistencies in long-\n",
      "form dialogues. Based on the SummScreen dataset\n",
      "(Chen et al., 2022a), which comprises TV scripts\n",
      "and human-crafted summaries, this benchmark in-\n",
      "troduces factual inconsistency annotations for sum-\n",
      "maries generated by Longformer and GPT-4 at sen-\n",
      "tence level, resulting in a dataset of 52 documents\n",
      "and 624 summary sentences. As for evaluation,\n",
      "hallucination detection methods are evaluated on\n",
      "this benchmark using the AUROC score.\n",
      "RealHall (Friel and Sanyal, 2023). The con-\n",
      "struction of this benchmark follows the principles\n",
      "that tasks within a hallucination detection bench-\n",
      "mark ought to present a substantive challenge to\n",
      "LLMs and bear relevance to real-world applications\n",
      "while ensuring a breadth of diversity. In alignmentmark ought to present a substantive challenge to\n",
      "LLMs and bear relevance to real-world applications\n",
      "while ensuring a breadth of diversity. In alignment\n",
      "with this, the benchmark concentrates on question-\n",
      "answering tasks, categorizing them into Closed and\n",
      "Open groups based on the availability of a reference\n",
      "text in the prompt. Each question within the bench-\n",
      "mark is initially approached using ChatGPT for\n",
      "generating responses, which are subsequently as-\n",
      "signed boolean ground-truth labels through a com-\n",
      "bined approach involving human annotation, GPT-\n",
      "4 evaluation, and automated rule-based assessment.\n",
      "The efficacy of hallucination detection methodolo-\n",
      "gies applied to this benchmark is quantified using\n",
      "the AUROC score.\n",
      "LSum (Feng et al., 2023a). The benchmark\n",
      "centers on factual consistency detection within the\n",
      "summarization tasks undertaken by LLMs. Built\n",
      "on XSum (Narayan et al., 2018), the benchmark in-\n",
      "volves generating summaries using various LLMs,\n",
      "from the GPTfamily, GLM-family, and LLaMA-family and annotating the factual consistency on\n",
      "the sentence level by employing ChatGPT and GPT-\n",
      "4, resulting in a total of 6,166 annotated summaries.\n",
      "SAC3(Zhang et al., 2023a). The benchmark\n",
      "comprises two datasets: HotpotQA-halu and NQ-\n",
      "open-halu. These datasets were constructed by\n",
      "sampling 250 examples from the training set of\n",
      "HotpotQA (Yang et al., 2018b) and NQ-open\n",
      "(Kwiatkowski et al., 2019), respectively. Hallu-\n",
      "cinated answers were then generated using gpt-3.5-\n",
      "turbo. Then, the answers were manually annotated,\n",
      "which involved comparing them with the ground\n",
      "truth and relevant knowledge sources.\n",
      "5 Hallucination Mitigating\n",
      "In this section, we present a comprehensive re-\n",
      "view of contemporary methods aimed at mitigat-\n",
      "ing hallucinations in LLMs. Drawing from in-\n",
      "sights discussed in Hallucination Causes (§3), we\n",
      "systematically categorize these methods based on\n",
      "the underlying causes of hallucinations. Specif-\n",
      "ically, we focus on approaches addressing Data-\n",
      "related Hallucinations (§5.1), Training-related Hal-\n",
      "lucinations (§5.2) and Inference-related Halluci-\n",
      "nations (§5.3), each offering tailored solutions to\n",
      "tackle specific challenges inherent to their respec-\n",
      "tive cause.\n",
      "5.1 Mitigating Data-related Hallucinations\n",
      "Data-related hallucinations generally emerge as a\n",
      "byproduct of biases, misinformation, and knowl-\n",
      "edge gaps, which are fundamentally rooted in the\n",
      "training data. In this context, we explore various\n",
      "strategies for mitigating such hallucinations, aim-\n",
      "ing to minimize the occurrence of misinformation\n",
      "and biases, while also providing knowledge aug-\n",
      "mentation and enhancing the effective utilization\n",
      "of knowledge by LLMs.\n",
      "5.1.1 Mitigating Misinformation and Biases\n",
      "To reduce the presence of misinformation and bi-\n",
      "ases, the most intuitive approach is to collect high-\n",
      "quality factual data to prevent the introduction of\n",
      "misinformation and conduct data cleansing to de-\n",
      "bias.\n",
      "Factuality Data Enhancement. Maintaining\n",
      "the factual correctness of the training data is cru-\n",
      "cial in mitigating issues like imitative falsehood\n",
      "(Lin et al., 2022). The most direct approach is the\n",
      "manual curation of the pre-training dataset. As\n",
      "early as the advent of GPT-2, Radford et al. (2019)underscored the significance of exclusively scrap-\n",
      "ing web pages that had undergone rigorous curation\n",
      "and filtration by human experts. However, as pre-\n",
      "training datasets continue to scale, manual curation\n",
      "becomes a challenge. Given that academic or spe-\n",
      "cialized domain data is typically factually accurate,\n",
      "gathering high-quality data emerges as a primary\n",
      "strategy. Notable examples include the Pile (Gao\n",
      "et al., 2021) and “textbook-like” data sources (Gu-\n",
      "nasekar et al., 2023; Li et al., 2023f). Additionally,\n",
      "up-sampling factual data during the pre-training\n",
      "phase has been proven effective in enhancing the\n",
      "factual correctness of LLMs (Touvron et al., 2023),\n",
      "thus alleviating hallucination.\n",
      "Debias. Biases within pre-training data can typ-\n",
      "ically be classified into two main categories: du-\n",
      "plication bias andsocietal biases , each requiring\n",
      "distinct debiasing approaches.\n",
      "•Duplication Bias. Deduplication serves as\n",
      "a crucial procedure in the pre-training phase.\n",
      "Existing practices typically fall into two cat-\n",
      "egories: exact duplicates and near-duplicates.\n",
      "For exact duplicates, the most straightforward\n",
      "method involves exact substring matching to\n",
      "identify identical strings. However, given the\n",
      "vastness of pre-training data, this process can\n",
      "be computationally intensive. In addition, a\n",
      "more efficient method utilizes the construc-\n",
      "tion of a suffix array (Manber and Myers,\n",
      "1993), enabling effective computation of nu-\n",
      "merous substring queries in linear time. Re-\n",
      "garding near-duplicates, the identification of-\n",
      "ten involves approximate full-text matching,\n",
      "typically utilizing hash-based techniques to\n",
      "identify document pairs with significant n-\n",
      "gram overlap. Furthermore, MinHash (Broder,\n",
      "1997) stands out as a prevalent algorithm for\n",
      "large-scale deduplication tasks (Gyawali et al.,identify document pairs with significant n-\n",
      "gram overlap. Furthermore, MinHash (Broder,\n",
      "1997) stands out as a prevalent algorithm for\n",
      "large-scale deduplication tasks (Gyawali et al.,\n",
      "2020). Additionally, SemDeDup (Abbas et al.,\n",
      "2023) makes use of embeddings from pre-\n",
      "trained models to identify semantic duplicates,\n",
      "which refers to data pairs with semantic simi-\n",
      "larities but not identical.\n",
      "•Societal Biases. Given the vastness and un-\n",
      "fathomable nature of pre-training data, di-\n",
      "rectly addressing the root cause of societal bi-\n",
      "ases is a formidable challenge (Ferrara, 2023).\n",
      "Consequently, current mainstream solutions\n",
      "lean heavily on curated training corpora. By\n",
      "carefully selecting diverse, balanced, and rep-resentative training data, we can mitigate bi-\n",
      "ases (Paullada et al., 2021; Narayanan Venkit\n",
      "et al., 2023; Ladhak et al., 2023) that may\n",
      "trigger hallucinations. Additionally, toolkits\n",
      "(Viswanath and Zhang, 2023) have been in-\n",
      "troduced to enable users to debiasing both\n",
      "existing and custom models.\n",
      "5.1.2 Mitigating Knowledge Boundary\n",
      "Constrained by the coverage and temporal bound-\n",
      "aries of training data, inevitably form knowledge\n",
      "boundaries, introducing notable challenges. To\n",
      "tackle these challenges, two popular approaches\n",
      "have gained significant attention. One is Knowl-\n",
      "edge editing (Sinitsin et al., 2020; Yao et al.,\n",
      "2023c), which aims at directly editing model pa-\n",
      "rameters to bridge the knowledge gap. The other\n",
      "leveraging non-parametric knowledge sources\n",
      "through Retrieval-Augmented Generation (RAG)\n",
      "(Lewis et al., 2020b; Guu et al., 2020; Shuster et al.,\n",
      "2021).\n",
      "Knowledge Editing. Knowledge editing De Cao\n",
      "et al. (2021); Sinitsin et al. (2020) has garnered\n",
      "rising attention from researchers, which aims to\n",
      "rectify model behavior by incorporating additional\n",
      "knowledge. Current knowledge editing techniques\n",
      "can fix factual errors and refresh outdated informa-\n",
      "tion to mitigate the knowledge gap, which can be\n",
      "categorized into two classes: changing the model’s\n",
      "behavior by modifying the model parameters or\n",
      "using an external model plug-in with the original\n",
      "model frozen (Yao et al., 2023c).\n",
      "•Modifying Model Parameters. These tech-\n",
      "niques directly inject knowledge into the orig-\n",
      "inal model, leading to a substantial alteration\n",
      "in the model’s output, which can be further\n",
      "split into locate-then-edit methods andmeta-\n",
      "learning methods .\n",
      "Locate-then-edit methods (Dai et al., 2022a;\n",
      "Meng et al., 2022) consist of two stages,\n",
      "which first locate the “buggy” part of the\n",
      "model parameters and then apply an update\n",
      "to them to alter the model’s behavior. For ex-\n",
      "ample, ROME (Meng et al., 2022) locates the\n",
      "edits-related layer by destroying and subse-\n",
      "quently restoring the activations and then up-\n",
      "dates the parameters of FFN in a directed man-\n",
      "ner to edit knowledge. MEMIT (Meng et al.,\n",
      "2023) employs the same knowledge locating\n",
      "methods as ROME, enabling the concurrent\n",
      "updating of multiple layers to facilitate the si-multaneous integration of thousands of editing\n",
      "knowledge. However, Yao et al. (2023c) finds\n",
      "that these methods lack non-trivial generaliza-\n",
      "tion capabilities and varying performance and\n",
      "applicability to different model architectures.\n",
      "The best-performing methods ROME (Meng\n",
      "et al., 2022) and MEMIT (Meng et al., 2023)\n",
      "empirically only work well on decoder-only\n",
      "LLMs.\n",
      "Meta-learning methods (De Cao et al., 2021;\n",
      "Mitchell et al., 2022a) train an external hyper-\n",
      "network to predict the weight update of the\n",
      "original model. Nevertheless, meta-learning\n",
      "methods often require additional training and\n",
      "memory cost, necessitating specialized design\n",
      "to reduce the size of hyper-networks in the\n",
      "age of LLMs ( e.g.low-rank decomposition\n",
      "(Mitchell et al., 2022a)). While these meth-\n",
      "ods can fine-grainedly adjust the behavior of\n",
      "the model, modifications to the parameters\n",
      "could have a potentially harmful impact on\n",
      "the inherent knowledge of the model.\n",
      "•Preserving Model Parameters. Instead of\n",
      "directly modifying model parameters, a line\n",
      "of studies apply an additional model plug-in\n",
      "into the original model to achieve the desired\n",
      "change in model behavior. SERAC (Mitchell\n",
      "et al., 2022b) employs a scope classifier to\n",
      "route the input associated with new knowl-\n",
      "edge stored in an external edit memory toward\n",
      "the counterfactual model, which can aid the\n",
      "base model in handling the updated informa-\n",
      "tion.\n",
      "In comparison to the whole model, there are\n",
      "various techniques that involve incorporating\n",
      "additional parameter layers ( e.g.adapter lay-\n",
      "ers (Hartvigsen et al., 2022)) as plug-ins into\n",
      "the original model. T-Patcher (Huang et al.,\n",
      "2023d) and NKB (Dai et al., 2022b) both\n",
      "add the patches into FFN layers which are ac-\n",
      "knowledged as the repository storing knowl-the original model. T-Patcher (Huang et al.,\n",
      "2023d) and NKB (Dai et al., 2022b) both\n",
      "add the patches into FFN layers which are ac-\n",
      "knowledged as the repository storing knowl-\n",
      "edge (Geva et al., 2021) to rectify the fac-\n",
      "tual mistakes. CALINET (Dong et al., 2022)\n",
      "proposes an assessment for identifying erro-\n",
      "neous knowledge in PLMs and similarly ad-\n",
      "justs the output of FFNs by introducing FFN-\n",
      "like memory slots, which is beneficial to al-\n",
      "leviate the knowledge gap. These methods\n",
      "require additional steps to train the parameter\n",
      "module, carefully designing training functionsand structures to promote the plug-in to play\n",
      "a role in updated knowledge while keeping\n",
      "unedited facts handled by the original mod-\n",
      "ule.\n",
      "Knowledge editing methods can effectively intro-\n",
      "duce knowledge to mitigate the model’s knowledge\n",
      "gap to some extent. Nevertheless, there is room\n",
      "for enhancement in the impact of knowledge edit-\n",
      "ing. (Zhong et al., 2023b) proposes MQUAKE to\n",
      "evaluate the generalization of injected knowledge\n",
      "and finds that the post-edited model can success-\n",
      "fully recall the edited facts but fails in complex\n",
      "multi-hop questions. There are also some studies\n",
      "(Wu et al., 2023; Wang et al., 2023e) indicating\n",
      "that existing editing methods exhibit limited cross-\n",
      "language generalization capabilities. Furthermore,\n",
      "Pinter and Elhadad (2023) suggests that knowl-\n",
      "edge editing techniques introduce potential risk to\n",
      "users when attempting to mitigate hallucinations\n",
      "of LLMs and advises utilizing methods incorpo-\n",
      "rating explicit knowledge ( e.g.retrieval-augmented\n",
      "methods).\n",
      "Retrieval Augmentation. An intuitive way\n",
      "to mitigate the knowledge gap is Retrieval-\n",
      "Augmented Generation (RAG)(Lewis et al., 2020b;\n",
      "Guu et al., 2020; Shuster et al., 2021), grounding\n",
      "the LLMs during generation by conditioning on rel-\n",
      "evant documents retrieved from an external knowl-\n",
      "edge source. Typically, RAG follows a retrieve-\n",
      "then-read pipeline, where relevant contextual docu-\n",
      "ments are firstly retrieved by a retriever (Karpukhin\n",
      "et al., 2020) from external sources, and then the\n",
      "desired output is generated by a generator condi-\n",
      "tioning on both input text and retrieved documents.\n",
      "We categorize the methods to mitigate hallucina-\n",
      "tion using retrieval augmentation into three types,\n",
      "including one-time retrieval ,iterative retrieval , and\n",
      "post-hoc retrieval .\n",
      "•One-time Retrieval. One-time retrieval aims\n",
      "to directly prepend the external knowledge ob-\n",
      "tained from a single retrieval to the LLMs’\n",
      "prompt. Ram et al. (2023) introduces In-\n",
      "context RALM, which entails a straightfor-\n",
      "ward yet effective strategy of prepending cho-\n",
      "sen documents to the input text of LLMs.\n",
      "Demonstrated empirical results indicate that\n",
      "the employment of In-context RALM consis-\n",
      "tently translates into enhanced performance\n",
      "across varying LLM sizes and a diverse array\n",
      "of corpora. Notably, the incorporation of aranking mechanism has been shown to further\n",
      "amplify performance gains.\n",
      "Beyond conventional knowledge repositories\n",
      "such as Wikipedia, ongoing research endeav-\n",
      "ors have explored alternative avenues, specif-\n",
      "ically the utilization of knowledge graphs\n",
      "(KGs). These KGs serve as a pivotal tool for\n",
      "prompting LLMs, facilitating their interaction\n",
      "with the most recent knowledge, and eliciting\n",
      "robust reasoning pathways (Wen et al., 2023;\n",
      "Qi et al., 2023; Baek et al., 2023). Varshney\n",
      "et al. (2023) introduce the Parametric Knowl-\n",
      "edge Guiding (PKG) framework, enhancing\n",
      "LLMs with domain-specific knowledge. PKG\n",
      "employs a trainable background knowledge\n",
      "module, aligning it with task knowledge and\n",
      "generating relevant contextual information.\n",
      "The effectiveness of PKG highlights the poten-\n",
      "tial for enhancing LLMs’ faithfulness by in-\n",
      "corporating retrieved background knowledge.\n",
      "•Iterative Retrieval. However, when con-\n",
      "fronted with intricate challenges like multi-\n",
      "step reasoning (Yang et al., 2018c) and long-\n",
      "form question answering (Fan et al., 2019;\n",
      "Stelmakh et al., 2022), traditional one-time\n",
      "retrieval may fall short.\n",
      "Addressing these demanding information\n",
      "needs, recent studies have proposed iterative\n",
      "retrieval, which allows for continuously gath-\n",
      "ering knowledge throughout the generation\n",
      "process. A burgeoning line of research (Khot\n",
      "et al., 2022; Yao et al., 2022; Press et al., 2022;\n",
      "He et al., 2023; Trivedi et al., 2023) endeavors\n",
      "to tackle such intricate tasks by decomposing\n",
      "them into more manageable sub-tasks. Rec-\n",
      "ognizing the substantial advancements chain-\n",
      "of-thought prompting has brought to LLMs\n",
      "in multi-step reasoning Wei et al. (2022), nu-\n",
      "merous studies (Yao et al., 2022; Trivedi et al.,ognizing the substantial advancements chain-\n",
      "of-thought prompting has brought to LLMs\n",
      "in multi-step reasoning Wei et al. (2022), nu-\n",
      "merous studies (Yao et al., 2022; Trivedi et al.,\n",
      "2023; He et al., 2023) try to incorporate exter-\n",
      "nal knowledge at each reasoning step and fur-\n",
      "ther guide retrieval process based on ongoing\n",
      "reasoning, reducing factual errors in reason-\n",
      "ing chains. Building upon chain-of-thought\n",
      "prompting, Press et al. (2022) introduced self-\n",
      "ask. Diverging from the conventional continu-\n",
      "ous, undelineated chain-of-thought prompting,\n",
      "self-ask delineates the question it intends to ad-\n",
      "dress at each step, subsequently incorporating\n",
      "a search action based on the follow-up ques-RetrieveQuery\n",
      "GenerateLarge Language\n",
      "Model\n",
      "AnswerQuery\n",
      "AnswerQuery\n",
      "GenerateLarge Language\n",
      "Model\n",
      "RertieveAnswer\n",
      " Revise\n",
      "Revisior\n",
      "(a) One-time Retrieval (b) Iterative Retrieval (c) Post-hoc Retrieval\n",
      "...\n",
      "LLM\n",
      "Output\n",
      "LLM\n",
      "OutputIteration \n",
      "Iteration \n",
      "...\n",
      "Revision\n",
      "Figure 6: The illustration of three distinct approaches for Retrieval-Augmented Generation: a) One-time Retrieval ,\n",
      "where relevant information is retrieved once before text generation; b) Iterative Retrieval , involving multiple\n",
      "retrieval iterations during text generation for dynamic information integration; and c) Post-hoc Retrieval , where the\n",
      "retrieval process happens after an answer is generated, aiming to refine and fact-check the generated content.\n",
      "tion. Instead of solely depending on chain-of-\n",
      "thought prompting for retrieval guidance, both\n",
      "Feng et al. (2023b) and Shao et al. (2023) em-\n",
      "ploy an iterative retrieval-generation collabo-\n",
      "rative framework, where a model’s response\n",
      "serves as an insightful context to procure more\n",
      "relevant knowledge, subsequently refining the\n",
      "response in the succeeding iteration.\n",
      "Beyond multi-step reasoning tasks, Jiang et al.\n",
      "(2023) shift their emphasis to long-form gen-\n",
      "eration. They proposed an active retrieval\n",
      "augmented generation framework, which it-\n",
      "eratively treats the upcoming prediction as\n",
      "a query to retrieve relevant documents. If\n",
      "the prediction contains tokens of low confi-\n",
      "dence, the sentence undergoes regeneration.\n",
      "In addition to using iterative retrieval to im-\n",
      "prove intermediate generations, Zhang et al.\n",
      "(2023e) present MixAlign, which iteratively\n",
      "refines user questions using model-based guid-\n",
      "ance and seeking clarifications from users,\n",
      "ultimately enhancing the alignment between\n",
      "questions and knowledge.\n",
      "•Post-hoc Retrieval. Beyond the traditional\n",
      "retrieve-then-read paradigm, a line of work\n",
      "has delved into post-hoc retrieval, refining\n",
      "LLM outputs through subsequent retrieval-\n",
      "based revisions.\n",
      "To enhance the trustworthiness and attribu-\n",
      "tion of LLMs, Gao et al. (2023a) adopt the\n",
      "research-then-revise workflow, which initially\n",
      "research relevant evidence and subsequentlyrevise the initial generation based on detected\n",
      "discrepancies with the evidence. Similarly,\n",
      "Zhao et al. (2023a) introduce the verify-and-\n",
      "editframework to enhance the factual accu-\n",
      "racy of reasoning chains by incorporating\n",
      "external knowledge. For reasoning chains\n",
      "that show lower-than-average consistency, the\n",
      "framework generates verifying questions and\n",
      "then refines the rationales based on retrieved\n",
      "knowledge, ensuring a more factual response.\n",
      "Yu et al. (2023d) enhanced the post-hoc re-\n",
      "trieval method through diverse answer gener-\n",
      "ation. Instead of generating just a single an-\n",
      "swer, they sample various potential answers,\n",
      "allowing for a more comprehensive retrieval\n",
      "feedback. Additionally, by employing an en-\n",
      "sembling technique that considers the likeli-\n",
      "hood of the answer before and after retrieval,\n",
      "they further mitigate the risk of misleading\n",
      "retrieval feedback.\n",
      "5.1.3 Mitigating Knowledge Shortcut\n",
      "Knowledge shortcuts manifest when LLMs lean\n",
      "on spurious correlations, such as the co-occurrence\n",
      "statistics of the pre-training corpora, to capture fac-\n",
      "tual knowledge. Kang and Choi (2023) suggested\n",
      "fine-tuning on a debiased dataset constructed by\n",
      "excluding biased samples. Although this leads to\n",
      "a notable decline in the recall of frequent facts as\n",
      "more samples are excluded, this method struggles\n",
      "to generalize when rare facts are unseen during\n",
      "finetuning.5.1.4 Mitigating Knowledge Recall Failures\n",
      "A prevalent source of hallucinations in LLMs is\n",
      "their inability to accurately retrieve and apply rel-\n",
      "evant information embedded in their parametric\n",
      "knowledge. This challenge is particularly acute in\n",
      "complex reasoning scenarios where the integrity\n",
      "of information is critical. By enhancing knowl-\n",
      "edge recall, we can better anchor the model’s out-\n",
      "puts to verifiable knowledge, thereby providing\n",
      "a more robust defense against generating hallu-\n",
      "cination content. Typically, the most direct ap-\n",
      "proach to recall knowledge is enabling LLMs to\n",
      "reason via Chain-of-Thought prompting. Zhong\n",
      "et al. (2023b) suggest that simply applying CoT\n",
      "can increase knowledge recall, which substantially\n",
      "boosts performance in editing facts under multi-\n",
      "hop settings. Instead of incorporating reasoning\n",
      "steps, Zheng et al. (2023) posit that directly supple-\n",
      "menting questions with relevant information can\n",
      "enhance the model’s ability to recall crucial knowl-\n",
      "edge. Wang et al. (2023g) advance this by em-\n",
      "ploying conceptualization, which distills original\n",
      "commonsense knowledge into high-level abstract\n",
      "knowledge, boosting knowledge recall.\n",
      "5.2 Mitigating Training-related Hallucination\n",
      "Training-related hallucinations typically arise from\n",
      "the intrinsic limitations of the architecture and train-\n",
      "ing strategies adopted by LLMs. In this context,\n",
      "we discuss various optimization methods ranging\n",
      "from training stages (§5.2.1) and alignment stages\n",
      "(§5.2.2), aiming to mitigate hallucinations within\n",
      "the training process.\n",
      "5.2.1 Mitigating Pretraining-related\n",
      "Hallucination\n",
      "To address pretraining-related hallucination, the\n",
      "majority of research emphasizes the exploration of\n",
      "novel model architectures and the improvement of\n",
      "pre-training objectives.\n",
      "Mitigating Flawed Model Architecture. One\n",
      "significant avenue of research in mitigating\n",
      "pretraining-related hallucination centers on the lim-\n",
      "itations inherent in model architectures, especially\n",
      "unidirectional representation andattention glitches .\n",
      "In light of this, numerous studies have delved into\n",
      "designing novel model architectures specifically\n",
      "tailored to address these flaws.\n",
      "•Mitigating Unidirectional Representation.\n",
      "Addressing the limitations inherent in uni-\n",
      "directional representation, Li et al. (2023h)introduced BATGPT that employs a bidirec-\n",
      "tional autoregressive approach. This design\n",
      "allows the model to predict the next token\n",
      "based on all previously seen tokens, consider-\n",
      "ing both past and future contexts, thus captur-\n",
      "ing dependencies in both directions. Building\n",
      "on this idea, Liu et al. (2023e) highlighted\n",
      "the potential of encoder-decoder models to\n",
      "make better use of their context windows, sug-\n",
      "gesting a promising direction for future LLMs\n",
      "architecture design.\n",
      "•Mitigating Attention Glitches. Recogniz-\n",
      "ing the limitations of soft attention within\n",
      "self-attention-based architecture, Liu et al.\n",
      "(2023a) proposed attention-sharpening regu-\n",
      "larizers. This plug-and-play approach sparsi-\n",
      "fies self-attention architectures using differen-\n",
      "tiable loss terms (Zhang et al., 2018) to pro-\n",
      "mote sparsity, leading to a significant reduc-\n",
      "tion in reasoning hallucinations.\n",
      "Mitigating Suboptimal Pre-training Objec-\n",
      "tive.\n",
      "In the pre-training phase of LLMs, the choice\n",
      "of objective plays a pivotal role in determining\n",
      "the model’s performance. However, conventional\n",
      "objectives can lead to fragmented representations\n",
      "and inconsistencies in model outputs. Recent ad-\n",
      "vancements have sought to address these challenges\n",
      "by refining pre-training strategies, ensuring richer\n",
      "context comprehension, and circumventing biases.\n",
      "This section sheds light on these pioneering ap-\n",
      "proaches, encompassing both novel training objec-\n",
      "tives and efforts to counteract exposure bias.\n",
      "•Training Objective. Addressing the inherent\n",
      "limitations in training LLMs, where unstruc-\n",
      "tured factual knowledge at a document level\n",
      "often gets chunked due to GPU memory con-\n",
      "straints and computational efficiency, leadinglimitations in training LLMs, where unstruc-\n",
      "tured factual knowledge at a document level\n",
      "often gets chunked due to GPU memory con-\n",
      "straints and computational efficiency, leading\n",
      "to fragmented information and incorrect entity\n",
      "associations, Lee et al. (2022b) introduced a\n",
      "factuality-enhanced training method. By ap-\n",
      "pending a TOPICPREFIX to each sentence in\n",
      "factual documents, the approach transforms\n",
      "them into standalone facts, significantly reduc-\n",
      "ing factual errors and enhancing the model’s\n",
      "comprehension of factual associations. Simi-\n",
      "larly, considering that randomly concatenating\n",
      "shorter documents during pre-training might\n",
      "introduce inconsistencies in model outputs,Shi et al. (2023c) propose In-Context Pretrain-\n",
      "ing, an innovative approach in which LLMs\n",
      "are trained on sequences of related documents.\n",
      "By altering the document order, this method\n",
      "aims to maximize similarity within the con-\n",
      "text windows. It explicitly encourages LLMs\n",
      "to reason across document boundaries, poten-\n",
      "tially bolstering the logical consistency be-\n",
      "tween generations.\n",
      "•Exposure Bias. Exposure bias-induced hallu-\n",
      "cinations are intricately tied to error accumu-\n",
      "lation, as noted by (Arora et al., 2022). While\n",
      "several approaches have proposed Chen et al.\n",
      "(2020); Welleck et al. (2020); Bertsch et al.\n",
      "(2023) to mitigate exposure bias, few studies\n",
      "are directly linked to the hallucinations. To\n",
      "bridge this gap, Wang et al. (2023b) introduce\n",
      "the incorporation of intermediate sequences\n",
      "as supervision signals within the permutation\n",
      "multi-task learning framework to mitigate spu-\n",
      "rious correlations in domain-shift scenarios in\n",
      "NMT. Additionally, by employing the Mini-\n",
      "mum Bayes Risk decoding (Shen et al., 2016),\n",
      "it can further reduce hallucinations related to\n",
      "exposure bias.\n",
      "5.2.2 Mitigating Misalignment Hallucination\n",
      "Hallucinations induced during alignment often\n",
      "stem from capability misalignment and belief mis-\n",
      "alignment. However, defining the knowledge\n",
      "boundary of LLMs proves challenging, making it\n",
      "difficult to bridge the gap between LLMs’ inherent\n",
      "capabilities and the knowledge presented in human-\n",
      "annotated data. While limited research addresses\n",
      "capability misalignment, the focus mainly shifts\n",
      "toward belief misalignment.\n",
      "Hallucinations stemming from belief misalign-\n",
      "ment often manifest as sycophancy, a tendency\n",
      "of LLMs to seek human approval in undesirable\n",
      "ways. This sycophantic behavior can be attributed\n",
      "to the fact that human preference judgments of-\n",
      "ten favor sycophantic responses over more truthful\n",
      "ones (Sharma et al., 2023), paving the way for re-\n",
      "ward hacking (Saunders et al., 2022). To address\n",
      "this, a straightforward strategy is to improve human\n",
      "preference judgments and, by extension, the prefer-\n",
      "ence model. Recent research (Bowman et al., 2022;\n",
      "Saunders et al., 2022) has investigated the use of\n",
      "LLMs to assist human labelers in identifying over-\n",
      "looked flaws. Additionally, Sharma et al. (2023)\n",
      "discovered that aggregating multiple human prefer-ences enhances feedback quality, thereby reducing\n",
      "sycophancy.\n",
      "Besides, modifications to LLMs’ internal activa-\n",
      "tions have also shown the potential to alter model\n",
      "behavior. This can be achieved through methods\n",
      "like fine-tuning (Wei et al., 2023) or activation steer-\n",
      "ing during inference (Dathathri et al., 2020; Sub-\n",
      "ramani et al., 2022; Gu et al., 2022b,c; Hernan-\n",
      "dez et al., 2023). Specifically, Wei et al. (2023)\n",
      "proposed a synthetic-data intervention, fine-tuning\n",
      "language models using synthetic data where the\n",
      "claim’s ground truth is independent of a user’s opin-\n",
      "ion, aiming to reduce sycophantic tendencies.\n",
      "Another avenue of research (Rimsky, 2023b,a)\n",
      "has been to mitigate sycophancy through activa-\n",
      "tion steering. This approach involves using pairs\n",
      "of sycophantic/non-sycophantic prompts to gener-\n",
      "ate the sycophancy steering vector, derived from\n",
      "averaging the differences in intermediate activa-\n",
      "tions. During inference, subtracting this vector can\n",
      "produce less sycophantic LLM outputs.\n",
      "5.3 Mitigating Inference-related\n",
      "Hallucination\n",
      "Decoding strategies in Large Language Models\n",
      "play a pivotal role in determining the factuality and\n",
      "faithfulness of the generated content. However, As\n",
      "analyzed in Section §3.3, imperfect decoding often\n",
      "results in outputs that might lack factuality or stray\n",
      "from the original context. In this subsection, we\n",
      "explore two advanced strategies aimed at refining\n",
      "the decoding strategy to enhance both the factuality\n",
      "and faithfulness of the LLMs’ outputs.\n",
      "5.3.1 Factuality Enhanced Decoding\n",
      "Factuality Enhanced Decoding focuses on ensuring\n",
      "the factuality of the information produced by LLMs.\n",
      "By emphasizing the accuracy of facts, this strategy5.3.1 Factuality Enhanced Decoding\n",
      "Factuality Enhanced Decoding focuses on ensuring\n",
      "the factuality of the information produced by LLMs.\n",
      "By emphasizing the accuracy of facts, this strategy\n",
      "aims to generate outputs that adhere strictly to real-\n",
      "world information and resist producing misleading\n",
      "or false statements.\n",
      "On Standalone Decoding. Considering the ran-\n",
      "domness in the sampling process can introduce non-\n",
      "factual content into open-ended text generation,\n",
      "Lee et al. (2022b) introduced the factual-nucleus\n",
      "sampling algorithm that dynamically adjusts the\n",
      "“nucleus” p throughout sentence generation. By dy-\n",
      "namically adjusting the nucleus probability based\n",
      "on decay factors and lower boundaries and reset-\n",
      "ting the nucleus probability at the beginning of\n",
      "every new sentence, the decoding strategy strikes\n",
      "a balance between generating factual content andpreserving output diversity.\n",
      "Moreover, some studies (Burns et al., 2022;\n",
      "Moschella et al., 2022) posit that the activation\n",
      "space of LLMs contains interpretable structures\n",
      "related to factuality. Building on this idea, Li\n",
      "et al. (2023d) introduce Inference-Time Interven-\n",
      "tion (ITI). This method first identifies a direc-\n",
      "tion in the activation space associated with factu-\n",
      "ally correct statements and then adjusts activations\n",
      "along the truth-correlated direction during infer-\n",
      "ence. By repeatedly applying such intervention,\n",
      "LLMs can be steered towards producing more fac-\n",
      "tual responses.\n",
      "Similarly, Chuang et al. (2023) delve into en-\n",
      "hancing the factuality of LLM’s decoding process\n",
      "from a perspective of factual knowledge storage.\n",
      "They exploit the hierarchical encoding of factual\n",
      "knowledge within transformer LLMs, noting that\n",
      "lower-level information is captured in earlier layers\n",
      "and semantic information in the later ones. Draw-\n",
      "ing inspiration from Li et al. (2022c), they intro-\n",
      "duce DoLa, a strategy that dynamically selects and\n",
      "contrasts logits from different layers to refine de-\n",
      "coding factuality. By placing emphasis on knowl-\n",
      "edge from higher layers and downplaying that from\n",
      "the lower layers, DoLa showcases its potential to\n",
      "make LLMs more factual, thus reducing hallucina-\n",
      "tions.\n",
      "Post-editing Decoding. Unlike methods that\n",
      "directly modify the probability distribution to pre-\n",
      "vent hallucinations during the initial decoding,\n",
      "post-editing decoding seeks to harness the self-\n",
      "correction capabilities of LLMs (Pan et al., 2023)\n",
      "to refine the originally generated content without\n",
      "relying on an external knowledge base. Dhuliawala\n",
      "et al. (2023) introduced the Chain-of-Verification\n",
      "(COVE), which operates under the assumption\n",
      "that, when appropriately prompted, LLMs can self-\n",
      "correct their mistakes and provide more accurate\n",
      "facts. Starting with an initial draft, it first formu-\n",
      "lates verification questions and then systematically\n",
      "answers those questions in order to finally produce\n",
      "an improved revised response. Similarly, Ji et al.\n",
      "(2023b) focus on the medical domain and intro-\n",
      "duces an iterative self-reflection process. This pro-\n",
      "cess leverages the inherent ability of LLMs to first\n",
      "generate factual knowledge and then refine the re-\n",
      "sponse until it aligns consistently with the provided\n",
      "background knowledge.5.3.2 Faithfulness Enhanced Decoding\n",
      "On the other hand, Faithfulness Enhanced Decod-\n",
      "ing prioritizes alignment with the user instructions\n",
      "or provided context and also emphasizes enhancing\n",
      "the consistency within the generated content. Thus,\n",
      "in this section, we summarize existing work into\n",
      "two categories, including Context Consistency and\n",
      "Logical Consistency .\n",
      "Context Consistency. Decoding strategies that\n",
      "prioritize context consistency are designed to en-\n",
      "hance the faithfulness of LLMs to both user in-\n",
      "structions and the provided context. Before the era\n",
      "of LLMs, prior studies extensively explored im-\n",
      "provements in context consistency, predominantly\n",
      "in the fields of abstractive summarization and data-\n",
      "to-text. Tian et al. (2019) proposed confident de-\n",
      "coding that incorporates a confidence score during\n",
      "the decoding process to measure the model’s at-\n",
      "tention level to the source. By emphasizing the\n",
      "source more when the confidence score is high,\n",
      "they mitigate hallucinations stemming from a lack\n",
      "of context attention. van der Poel et al. (2022)\n",
      "shifts the decoding objective to pointwise mutual\n",
      "information. This approach encourages the model\n",
      "to prioritize tokens relevant to the source document,\n",
      "especially when model uncertainty rises, aiming\n",
      "to prevent hallucinations. In contrast to previous\n",
      "strategies that emphasized enhanced attention to\n",
      "the source for bolstering context consistency, Wan\n",
      "et al. (2023) delved into whether better exploration\n",
      "of the search space could improve faithfulness. By\n",
      "using automatic faithfulness metrics to rank candi-\n",
      "dates generated by beam search and incorporating\n",
      "lookahead heuristics that assign a faithfulness scoreusing automatic faithfulness metrics to rank candi-\n",
      "dates generated by beam search and incorporating\n",
      "lookahead heuristics that assign a faithfulness score\n",
      "to the future generation, they achieved significant\n",
      "improvements in faithfulness compared to existing\n",
      "decoding strategies.\n",
      "However, in the era of LLMs, the issue of hal-\n",
      "lucinations due to insufficient attention to context\n",
      "remains. Shi et al. (2023b) propose context-aware\n",
      "decoding (CAD), which modifies the output dis-\n",
      "tribution by reducing reliance on prior knowledge,\n",
      "thereby promoting the model’s focus on the con-\n",
      "textual information, similar to that presented by\n",
      "(van der Poel et al., 2022). However, due to the\n",
      "inherent trade-off between diversity and attribution\n",
      "(Gu et al., 2022a), overemphasizing contextual in-\n",
      "formation can reduce diversity. In response, Chang\n",
      "et al. (2023a) introduced an innovative sampling\n",
      "algorithm to bolster attribution while preserving\n",
      "diversity. This method involves two parallel de-codings, one considering the source and the other\n",
      "not, and adjusts the temperature dynamically using\n",
      "the KL divergence between their token distribu-\n",
      "tions to reflect source attribution. Lei et al. (2023)\n",
      "explored a more generic post-edit framework to\n",
      "mitigate faithfulness hallucinations during infer-\n",
      "ence. This approach first detects hallucinations at\n",
      "both the sentence and entity levels and then uti-\n",
      "lizes this detection feedback to refine the gener-\n",
      "ated response. Furthermore, Choi et al. (2023) in-\n",
      "troduced knowledge-constrained decoding (KCD),\n",
      "which employs a token-level hallucination detec-\n",
      "tion to identify hallucinations and guides the gen-\n",
      "eration process by reweighing the token distribu-\n",
      "tion with a better estimate of the future knowledge-\n",
      "groundedness. Besides, considering that the soft-\n",
      "max bottleneck constrains the expression of diver-\n",
      "sity and faithful representations. A line of work\n",
      "explores methods to overcome the bottleneck, ei-\n",
      "ther by a mixture of Softmax, which uses multiple\n",
      "hidden states to compute softmax multiple times\n",
      "and merge the resulting distributions (Yang et al.,\n",
      "2019) or using pointer networks to enable LLMs\n",
      "to copy the context words (Chang et al., 2023b),\n",
      "further reducing hallucinations.\n",
      "Logical Consistency. Logical consistency\n",
      "in LLMs is essential to ensure consistent re-\n",
      "sponses and prevent hallucinations, particularly\n",
      "during multi-step reasoning. To enhance the self-\n",
      "consistency inherent in Chain-of-thought prompt-\n",
      "ing, Wang et al. (2023f) employs a knowledge\n",
      "distillation framework. They first generate a con-\n",
      "sistent rationale using contrastive decoding (Li\n",
      "et al., 2022c) and then fine-tune the student model\n",
      "with a counterfactual reasoning objective, which\n",
      "effectively eliminates reasoning shortcuts (Branco\n",
      "et al., 2021) that derive answers without consid-\n",
      "ering the rationale. Furthermore, by employing\n",
      "contrastive decoding directly, LLMs can reduce\n",
      "surface-level copying and prevent missed reason-\n",
      "ing steps (O’Brien and Lewis, 2023).\n",
      "6 Challenges and Open Questions\n",
      "In this section, we delve into the multifarious chal-\n",
      "lenges and open questions surrounding hallucina-\n",
      "tion in LLMs, aiming to guide future directions in\n",
      "this pivotal domain.\n",
      "6.1 Challenges in LLM Hallucination\n",
      "In the pursuit of reliable and truthful LLMs, ad-\n",
      "dressing hallucination is essential, given its inher-ent complexities. While significant strides have\n",
      "been made in mitigating LLM hallucinations, no-\n",
      "table challenges still remain. In this context,\n",
      "we delve into these challenges, highlighting their\n",
      "manifestation in domains such as long-form text\n",
      "generation (§6.1.1), retrieval augmented genera-\n",
      "tion (§6.1.2), and large vision-language models\n",
      "(§6.1.3).\n",
      "6.1.1 Hallucination in Long-form Text\n",
      "Generation\n",
      "Long-form text generation has gained widespread\n",
      "application in LLMs (Qin et al., 2023; Bhat et al.,\n",
      "2023; Chen et al., 2023b). However, as the length\n",
      "of the generated content increases, the propensity\n",
      "for hallucination also grows, leading to challenges\n",
      "in evaluating such hallucinations (Min et al., 2023).\n",
      "Firstly, existing LLM hallucination benchmarks\n",
      "(Lin et al., 2022; Cheng et al., 2023) are usually pre-\n",
      "sented in the form of factoid questions and answers,\n",
      "focusing more on factual hallucinations. There is\n",
      "a noticeable absence of manually annotated hallu-\n",
      "cination benchmarks in the domain of long-form\n",
      "text generation, which hinders researchers from\n",
      "studying specific types of hallucinations in this con-\n",
      "text. Secondly, evaluating hallucinations in long-\n",
      "form text generation is challenging. While there\n",
      "are some evaluation metrics available (Min et al.,\n",
      "2023), they have limitations and are not applicable\n",
      "when the facts are more nuanced, open-ended, and\n",
      "debatable, or when there are conflicts in knowl-\n",
      "edge sources. This poses obstacles for practical\n",
      "applications in real-world scenarios.\n",
      "6.1.2 Hallucination in Retrieval Augmented\n",
      "Generation\n",
      "Retrieval Augmented Generation (RAG) has\n",
      "emerged as a promising strategy to mitigate hal-applications in real-world scenarios.\n",
      "6.1.2 Hallucination in Retrieval Augmented\n",
      "Generation\n",
      "Retrieval Augmented Generation (RAG) has\n",
      "emerged as a promising strategy to mitigate hal-\n",
      "lucinations in LLMs. As concerns around LLM\n",
      "hallucinations have intensified, RAG has increas-\n",
      "ingly come under the spotlight, paving the way\n",
      "for a range of commercial applications, such as\n",
      "Perplexity2, YOU.com3and New Bing4. By re-\n",
      "trieving evidence from external knowledge bases,\n",
      "RAG enables LLMs to be equipped with up-to-date\n",
      "knowledge and generate responses conditioning on\n",
      "relevant evidence. However, despite its advantages,\n",
      "RAG also suffers from hallucinations. One notable\n",
      "issue is the potential for error accumulation within\n",
      "2https://www.perplexity.ai/\n",
      "3https://you.com/\n",
      "4https://www.bing.com/newthe RAG pipeline. Irrelevant evidence can be prop-\n",
      "agated into the generation phase, possibly tainting\n",
      "the output (Li et al., 2023a; Shi et al., 2023a; Cho\n",
      "et al., 2023; Xu et al., 2023). Another concern lies\n",
      "in the arena of generative retrievals, which occa-\n",
      "sionally suffer from citation inaccuracies (Rashkin\n",
      "et al.; Liu et al., 2023f; Yue et al., 2023; Gao et al.,\n",
      "2023a; Chen et al., 2023a). While citations aim to\n",
      "offer a traceable path to the information’s source for\n",
      "validation purposes, errors in this domain can lead\n",
      "users astray. Furthermore, existing RAG may suf-\n",
      "fer from a trade-off between diversity and factuality\n",
      "(Liu et al., 2023f) which poses a new challenge in\n",
      "terms of the need for diversity.\n",
      "6.1.3 Hallucination in Large Vision-Language\n",
      "Models\n",
      "Enabling the visual perception ability, along with\n",
      "exceptional language understanding and genera-\n",
      "tion capabilities, Large Vision-Language Mod-\n",
      "els (LVLMs) have exhibited remarkable vision-\n",
      "language capabilities(Zhu et al., 2023a; Liu et al.,\n",
      "2023d; Yu et al., 2021; Huang et al., 2023b;\n",
      "Maaz et al., 2023; Chen et al., 2023e; Yu et al.,\n",
      "2023b; Zellers et al., 2019). Unlike previous\n",
      "pre-trained multi-modal models that gain limited\n",
      "vision-language abilities from large-scale visual-\n",
      "language pre-training datasets (Wang et al., 2022;\n",
      "Li et al., 2023b; Luo et al., 2020; Zhong et al.,\n",
      "2023a), LVLMs exploit advanced large-language\n",
      "models to better interact with humans and the en-\n",
      "vironment. The consequent diverse applications of\n",
      "LVLMs also bring new challenges to maintaining\n",
      "the reliability of such systems, which therefore had\n",
      "to be further investigated and mitigated.\n",
      "Li et al. (2023e), Lovenia et al. (2023), take the\n",
      "first step towards evaluating the object hallucina-\n",
      "tions in the LVLMs. Evaluations and experiments\n",
      "reveal that current LVLMs are prone to generate\n",
      "inconsistent responses with respect to the associ-\n",
      "ated image, including non-existent objects, wrong\n",
      "object attributes, incorrect semantic relationships,\n",
      "etc. Furthermore, Liu et al. (2023c), Zong et al.\n",
      "(2023) and Liu et al. (2023b) show that LVLMs\n",
      "can be easily fooled and experience a severe per-\n",
      "formance drop due to their over-reliance on the\n",
      "strong language prior, as well as its inferior ability\n",
      "to defend against inappropriate user inputs. Current\n",
      "evaluations and discussions mainly focus on object\n",
      "hallucination. However, despite the witnessed per-\n",
      "ception errors, LVLMs can generate flawed logical\n",
      "reasoning results even when correctly recognizingall visual elements, which remains to be further\n",
      "investigated.\n",
      "Efforts have been made towards building a more\n",
      "robust large vision-language model. Gunjal et al.\n",
      "(2023), Lu et al. (2023), and Liu et al. (2023c) pro-\n",
      "pose to further finetuning the model to produce\n",
      "more truthful and helpful responses. Another line\n",
      "of work chooses to post-hoc rectify the generated\n",
      "inconsistent content, such as (Zhou et al., 2023b),\n",
      "and (Yin et al., 2023). Though proved to be ef-\n",
      "fective, those methods usually require additional\n",
      "data annotations, visual experts, or training phases,\n",
      "which prevent LVLMs from effectively scaling and\n",
      "generalizing to various fields. Thus, a more uni-\n",
      "versal approach is expected in the future to build\n",
      "a more reliable system. What’s more, when pre-\n",
      "sented with multiple images, LVLMs sometimes\n",
      "mix or miss parts of the visual context, as well as\n",
      "fail to understand temporal or logical connections\n",
      "between them, which might hinder their usage in\n",
      "many scenarios, yet properly identifying the reason\n",
      "for such disorders and tackling them still requires\n",
      "continued efforts.\n",
      "6.2 Open Questions in LLM Hallucination\n",
      "As research into LLM hallucination progresses,\n",
      "several questions demand ongoing discussion.\n",
      "These encompass the effectiveness of LLMs’ self-\n",
      "correction mechanisms in reducing hallucinations\n",
      "(§6.2.1), the understanding of knowledge bound-\n",
      "aries within LLMs (§6.2.2), and the balance be-\n",
      "tween their creativity and truthfulness (§6.2.3).correction mechanisms in reducing hallucinations\n",
      "(§6.2.1), the understanding of knowledge bound-\n",
      "aries within LLMs (§6.2.2), and the balance be-\n",
      "tween their creativity and truthfulness (§6.2.3).\n",
      "Delving into these open questions paves the way for\n",
      "a more profound understanding of the capabilities\n",
      "of LLMs and the intricacies of hallucinations.\n",
      "6.2.1 Can Self-Correct Mechanisms Help in\n",
      "Mitigating Reasoning Hallucinations?\n",
      "While LLMs have shown remarkable capabilities\n",
      "in tackling complex reasoning tasks through Chain-\n",
      "of-Thought prompting (Wei et al., 2022), they occa-\n",
      "sionally exhibit unfaithful reasoning characterized\n",
      "by inconsistencies within the reasoning steps or\n",
      "conclusions that do not logically follow the reason-\n",
      "ing chain (Golovneva et al., 2022; Ribeiro et al.,\n",
      "2023; Lyu et al., 2023). Research indicates that\n",
      "integrating external feedback into LLMs can sig-\n",
      "nificantly mitigate such hallucinations in reasoning.\n",
      "This feedback typically comes from external knowl-\n",
      "edge sources through retrieval processes (He et al.,\n",
      "2023; Gou et al., 2023), interactive debates with\n",
      "other LLMs (Du et al., 2023; Cohen et al., 2023),or guidance from external evaluation metrics (Lei\n",
      "et al., 2023; Khalifa et al., 2023).\n",
      "Nonetheless, a branch of research (Madaan et al.,\n",
      "2023; Yao et al., 2023b; Xie et al., 2023) explores\n",
      "the potential of self-correction mechanisms, where\n",
      "an LLM corrects its initial responses using its built-\n",
      "in capabilities, independent of external feedback.\n",
      "Although self-correction has shown promise for\n",
      "achieving faithful and accurate reasoning, espe-\n",
      "cially in iterative settings, certain studies (Stechly\n",
      "et al., 2023; Huang et al., 2023a; Valmeekam\n",
      "et al., 2023) question the effectiveness of the self-\n",
      "correction mechanism, pointing out that LLMs still\n",
      "struggle to self-correct their reasoning chains. Con-\n",
      "sequently, the effectiveness of self-correction mech-\n",
      "anisms in mitigating reasoning hallucinations re-\n",
      "mains an open question, which deserves further\n",
      "exploration.\n",
      "6.2.2 Can We Accurately Capture LLM\n",
      "Knowledge Boundaries?\n",
      "Despite the impressive capacity to capture factual\n",
      "knowledge from extensive data, LLMs still face\n",
      "challenges in recognizing their own knowledge\n",
      "boundaries. This shortfall leads to the occurrence\n",
      "of hallucinations, where LLMs confidently pro-\n",
      "duce falsehoods without an awareness of their own\n",
      "knowledge limits (Pacchiardi et al., 2023; Ren et al.,\n",
      "2023; Zhao et al., 2023c). Numerous studies delve\n",
      "into probing knowledge boundaries of LLMs, utiliz-\n",
      "ing strategies such as evaluating the probability of a\n",
      "correct response in a multiple-choice setting (Kada-\n",
      "vath et al., 2022), or quantifying the model’s output\n",
      "uncertainty by evaluating the similarity among sets\n",
      "of sentences with uncertain meanings.\n",
      "Furthermore, a line of work (Moschella et al.,\n",
      "2022; Burns et al., 2022; Li et al., 2023d; Azaria\n",
      "and Mitchell, 2023) has revealed that LLMs contain\n",
      "latent structures within their activation space that\n",
      "relate to beliefs about truthfulness. Recent research\n",
      "(Slobodkin et al., 2023) also found substantial ev-\n",
      "idence for LLMs’ ability to encode the unanswer-\n",
      "ability of questions, despite the fact that these mod-\n",
      "els exhibit overconfidence and produce hallucina-\n",
      "tions when presented with unanswerable questions.\n",
      "Nonetheless, Levinstein and Herrmann (2023) have\n",
      "employed empirical and conceptual tools to probe\n",
      "whether or not LLMs have beliefs. Their empirical\n",
      "results suggest that current lie-detector methods\n",
      "for LLMs are not yet fully reliable, and the prob-\n",
      "ing methods proposed by (Burns et al., 2022) and\n",
      "(Azaria and Mitchell, 2023) do not adequately gen-eralize. Consequently, whether we can effectively\n",
      "probe LLMs’ internal beliefs is ongoing, requiring\n",
      "further research.\n",
      "6.2.3 How Can We Strike a Balance between\n",
      "Creativity and Factuality?\n",
      "In the development of truthful and reliable LLMs,\n",
      "the challenge of balancing creativity and factual-\n",
      "ity stands out as a significant concern (Mukherjee\n",
      "and Chang, 2023; Lee, 2023). Ensuring factuality\n",
      "is critical for LLMs intended for real-world appli-\n",
      "cations; any inaccuracies can mislead users and\n",
      "pollute the online environment. The repercussions\n",
      "of such misinformation could be significant, po-\n",
      "tentially snowballing and cascading into the data\n",
      "used for subsequent LLM training. Conversely,\n",
      "hallucinations can sometimes offer valuable per-\n",
      "spectives, particularly in creative endeavors such\n",
      "as storytelling, brainstorming, and generating solu-\n",
      "tions that transcend conventional thinking.\n",
      "While current research on LLMs leans heavily\n",
      "towards reducing hallucinations, it often overlooks\n",
      "the important role of their creative capacities. As\n",
      "LLMs continue to evolve, the challenge of striking\n",
      "a balance between their creativity and factual ac-\n",
      "curacy remains unresolved. It is also interesting to\n",
      "explore the balance not only in multi-modal text\n",
      "generation tasks (Li et al., 2023b; Yu et al., 2021)\n",
      "but also in vision generation tasks (Zhang et al.,\n",
      "2023b; Rombach et al., 2022). This issue goes be-\n",
      "yond mere technicalities, necessitating a broader\n",
      "contemplation on the essence of artificial intelli-2023b; Rombach et al., 2022). This issue goes be-\n",
      "yond mere technicalities, necessitating a broader\n",
      "contemplation on the essence of artificial intelli-\n",
      "gence and its implications for human interaction\n",
      "and the exchange of knowledge.\n",
      "7 Conclusion\n",
      "In this comprehensive survey, we have undertaken\n",
      "an in-depth examination of hallucinations within\n",
      "large language models, delving into the intricacies\n",
      "of their underlying causes, pioneering detection\n",
      "methodologies as well as related benchmarks, and\n",
      "effective mitigation strategies. Although signifi-\n",
      "cant strides have been taken, the conundrum of\n",
      "hallucination in large language models remains a\n",
      "compelling and ongoing concern that demands con-\n",
      "tinuous investigation. Moreover, we envision this\n",
      "survey as a guiding beacon for researchers dedi-\n",
      "cated to advancing secure and trustworthy artificial\n",
      "intelligence. By navigating the complex landscape\n",
      "of hallucinations, we hope to empower these dedi-\n",
      "cated individuals with invaluable insights that drivethe evolution of AI technologies towards greater\n",
      "reliability and safety.\n",
      "References\n",
      "Amro Abbas, Kushal Tirumala, Dániel Simig, Surya\n",
      "Ganguli, and Ari S Morcos. 2023. Semdedup: Data-\n",
      "efficient learning at web-scale through semantic dedu-\n",
      "plication. ArXiv preprint , abs/2303.09540.\n",
      "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han\n",
      "Lu, Nicholas Meade, and Siva Reddy. 2023. Eval-\n",
      "uating correctness and faithfulness of instruction-\n",
      "following models for question answering. ArXiv\n",
      "preprint , abs/2307.16877.\n",
      "Ayush Agrawal, Lester Mackey, and Adam Tauman\n",
      "Kalai. 2023. Do language models know when\n",
      "they’re hallucinating references? ArXiv preprint ,\n",
      "abs/2305.18248.\n",
      "Renat Aksitov, Chung-Ching Chang, David Reitter, Sia-\n",
      "mak Shakeri, and Yun-Hsuan Sung. 2023. Charac-\n",
      "terizing attribution and fluency tradeoffs for retrieval-\n",
      "augmented large language models. ArXiv preprint ,\n",
      "abs/2302.05578.\n",
      "Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and\n",
      "Jackie Cheung. 2022. Why exposure bias matters:\n",
      "An imitation learning perspective of error accumu-\n",
      "lation in language generation. In Findings of the\n",
      "Association for Computational Linguistics: ACL\n",
      "2022 , pages 700–710, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Pepa Atanasova, Jakob Grue Simonsen, Christina Li-\n",
      "oma, and Isabelle Augenstein. 2020. Generating fact\n",
      "checking explanations. In Proceedings of the 58th\n",
      "Annual Meeting of the Association for Computational\n",
      "Linguistics , pages 7352–7364, Online. Association\n",
      "for Computational Linguistics.\n",
      "Isabelle Augenstein, Christina Lioma, Dongsheng\n",
      "Wang, Lucas Chaves Lima, Casper Hansen, Chris-\n",
      "tian Hansen, and Jakob Grue Simonsen. 2019. Mul-\n",
      "tiFC: A real-world multi-domain dataset for evidence-\n",
      "based fact checking of claims. In Proceedings of\n",
      "the 2019 Conference on Empirical Methods in Natu-\n",
      "ral Language Processing and the 9th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pages 4685–4697, Hong Kong,\n",
      "China. Association for Computational Linguistics.\n",
      "Amos Azaria and Tom M. Mitchell. 2023. The inter-\n",
      "nal state of an LLM knows when its lying. ArXiv\n",
      "preprint , abs/2304.13734.\n",
      "Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.\n",
      "Knowledge-augmented language model prompting\n",
      "for zero-shot knowledge graph question answering.\n",
      "ArXiv preprint , abs/2306.04136.\n",
      "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\n",
      "liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, ZiweiJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\n",
      "Xu, and Pascale Fung. 2023. A multitask, multilin-\n",
      "gual, multimodal evaluation of chatgpt on reason-\n",
      "ing, hallucination, and interactivity. ArXiv preprint ,\n",
      "abs/2302.04023.\n",
      "Mario Barrantes, Benedikt Herudek, and Richard\n",
      "Wang. 2020. Adversarial nli for factual correct-\n",
      "ness in text summarisation models. ArXiv preprint ,\n",
      "abs/2005.11739.\n",
      "Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\n",
      "Longformer: The long-document transformer. ArXiv\n",
      "preprint , abs/2004.05150.\n",
      "Emily M. Bender, Timnit Gebru, Angelina McMillan-\n",
      "Major, and Shmargaret Shmitchell. 2021. On the\n",
      "dangers of stochastic parrots: Can language models\n",
      "be too big? In FAccT ’21: 2021 ACM Conference on\n",
      "Fairness, Accountability, and Transparency, Virtual\n",
      "Event / Toronto, Canada, March 3-10, 2021 , pages\n",
      "610–623. ACM.\n",
      "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\n",
      "Shazeer. 2015. Scheduled sampling for sequence\n",
      "prediction with recurrent neural networks. In Ad-\n",
      "vances in Neural Information Processing Systems 28:\n",
      "Annual Conference on Neural Information Process-\n",
      "ing Systems 2015, December 7-12, 2015, Montreal,\n",
      "Quebec, Canada , pages 1171–1179.\n",
      "Lukas Berglund, Meg Tong, Max Kaufmann, Mikita\n",
      "Balesni, Asa Cooper Stickland, Tomasz Korbak, and\n",
      "Owain Evans. 2023. The reversal curse: Llms trained\n",
      "on\" a is b\" fail to learn\" b is a\". ArXiv preprint ,\n",
      "abs/2309.12288.\n",
      "Amanda Bertsch, Alex Xie, Graham Neubig, and\n",
      "Matthew R Gormley. 2023. It’s mbr all the way\n",
      "down: Modern generation techniques through the\n",
      "lens of minimum bayes risk. ArXiv preprint ,\n",
      "abs/2310.01387.Amanda Bertsch, Alex Xie, Graham Neubig, and\n",
      "Matthew R Gormley. 2023. It’s mbr all the way\n",
      "down: Modern generation techniques through the\n",
      "lens of minimum bayes risk. ArXiv preprint ,\n",
      "abs/2310.01387.\n",
      "Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo\n",
      "Zhou, and Semih Yavuz. 2023. Investigating an-\n",
      "swerability of llms for long-form question answering.\n",
      "ArXiv preprint , abs/2309.08210.\n",
      "Charles Blundell, Julien Cornebise, Koray\n",
      "Kavukcuoglu, and Daan Wierstra. 2015. Weight\n",
      "uncertainty in neural network. In Proceedings of the\n",
      "32nd International Conference on Machine Learning,\n",
      "ICML 2015, Lille, France, 6-11 July 2015 , volume 37\n",
      "ofJMLR Workshop and Conference Proceedings ,\n",
      "pages 1613–1622. JMLR.org.\n",
      "Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin\n",
      "Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite,\n",
      "Amanda Askell, Andy Jones, Anna Chen, et al. 2022.\n",
      "Measuring progress on scalable oversight for large\n",
      "language models. ArXiv preprint , abs/2211.03540.\n",
      "Ralph Allan Bradley and Milton E Terry. 1952. Rank\n",
      "analysis of incomplete block designs: I. the method\n",
      "of paired comparisons. Biometrika , 39(3/4):324–\n",
      "345.Ruben Branco, António Branco, João António Ro-\n",
      "drigues, and João Ricardo Silva. 2021. Shortcutted\n",
      "commonsense: Data spuriousness in deep learning\n",
      "of commonsense reasoning. In Proceedings of the\n",
      "2021 Conference on Empirical Methods in Natural\n",
      "Language Processing , pages 1504–1521, Online and\n",
      "Punta Cana, Dominican Republic. Association for\n",
      "Computational Linguistics.\n",
      "Andrei Z Broder. 1997. On the resemblance and con-\n",
      "tainment of documents. In Proceedings. Compres-\n",
      "sion and Complexity of SEQUENCES 1997 (Cat. No.\n",
      "97TB100171) , pages 21–29. IEEE.\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
      "Clemens Winter, Christopher Hesse, Mark Chen, Eric\n",
      "Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\n",
      "Jack Clark, Christopher Berner, Sam McCandlish,\n",
      "Alec Radford, Ilya Sutskever, and Dario Amodei.\n",
      "2020. Language models are few-shot learners. In Ad-\n",
      "vances in Neural Information Processing Systems 33:\n",
      "Annual Conference on Neural Information Process-\n",
      "ing Systems 2020, NeurIPS 2020, December 6-12,\n",
      "2020, virtual .\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\n",
      "Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter\n",
      "Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg,\n",
      "Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro,\n",
      "and Yi Zhang. 2023. Sparks of artificial general\n",
      "intelligence: Early experiments with GPT-4. ArXiv\n",
      "preprint , abs/2303.12712.\n",
      "Collin Burns, Haotian Ye, Dan Klein, and Jacob Stein-\n",
      "hardt. 2022. Discovering latent knowledge in lan-\n",
      "guage models without supervision. ArXiv preprint ,\n",
      "abs/2212.03827.\n",
      "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,\n",
      "Katherine Lee, Florian Tramer, and Chiyuan Zhang.\n",
      "2022. Quantifying memorization across neural lan-\n",
      "guage models. ArXiv preprint , abs/2202.07646.\n",
      "Nicholas Carlini, Florian Tramer, Eric Wallace,\n",
      "Matthew Jagielski, Ariel Herbert-V oss, Katherine\n",
      "Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\n",
      "Erlingsson, et al. 2021. Extracting training data from\n",
      "large language models. In 30th USENIX Security\n",
      "Symposium (USENIX Security 21) , pages 2633–2650.\n",
      "Chung-Ching Chang, David Reitter, Renat Aksitov, and\n",
      "Yun-Hsuan Sung. 2023a. Kl-divergence guided tem-\n",
      "perature sampling. ArXiv preprint , abs/2306.01286.\n",
      "Haw-Shiuan Chang and Andrew McCallum. 2022. Soft-\n",
      "max bottleneck makes language models unable to\n",
      "represent multi-mode word distributions. In Proceed-\n",
      "ings of the 60th Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: Long Pa-\n",
      "pers) , pages 8048–8073, Dublin, Ireland. Association\n",
      "for Computational Linguistics.Haw-Shiuan Chang, Zonghai Yao, Alolika Gon, Hong\n",
      "Yu, and Andrew McCallum. 2023b. Revisiting the\n",
      "architectures like pointer networks to efficiently im-\n",
      "prove the next word distribution, summarization fac-\n",
      "tuality, and beyond. In Findings of the Association\n",
      "for Computational Linguistics: ACL 2023, Toronto,\n",
      "Canada, July 9-14, 2023 , pages 12707–12730. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Anthony Chen, Panupong Pasupat, Sameer Singh, Hon-\n",
      "grae Lee, and Kelvin Guu. 2023a. PURR: efficiently\n",
      "editing language model hallucinations by denois-\n",
      "ing language model corruptions. ArXiv preprint ,\n",
      "abs/2305.14908.\n",
      "Canyu Chen and Kai Shu. 2023. Can llm-generated\n",
      "misinformation be detected? ArXiv preprint ,\n",
      "abs/2309.13788.\n",
      "Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and\n",
      "Eunsol Choi. 2023b. Understanding retrieval aug-\n",
      "mentation for long-form question answering. ArXiv\n",
      "preprint , abs/2310.12150.\n",
      "Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett,\n",
      "and Eunsol Choi. 2023c. Complex claim verification\n",
      "with evidence retrieved in the wild. ArXiv preprint ,\n",
      "abs/2305.11859.\n",
      "Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin\n",
      "Gimpel. 2022a. Summscreen: A dataset for abstrac-\n",
      "tive screenplay summarization. In Proceedings of the\n",
      "60th Annual Meeting of the Association for Compu-Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin\n",
      "Gimpel. 2022a. Summscreen: A dataset for abstrac-\n",
      "tive screenplay summarization. In Proceedings of the\n",
      "60th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers), ACL\n",
      "2022, Dublin, Ireland, May 22-27, 2022 , pages 8602–\n",
      "8615. Association for Computational Linguistics.\n",
      "Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern,\n",
      "Siyang Gao, Pengfei Liu, and Junxian He. 2023d.\n",
      "Felm: Benchmarking factuality evaluation of large\n",
      "language models. volume abs/2310.00741.\n",
      "Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang\n",
      "Zhang. 2022b. Towards improving faithfulness in\n",
      "abstractive summarization. In NeurIPS .\n",
      "Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji,\n",
      "and Ajay Divakaran. 2023e. Measuring and improv-\n",
      "ing chain-of-thought reasoning in vision-language\n",
      "models. ArXiv preprint , abs/2309.04461.\n",
      "Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen,\n",
      "Jinan Xu, and Jie Zhou. 2023f. Improving translation\n",
      "faithfulness of large language models via augmenting\n",
      "instructions. ArXiv preprint , abs/2308.12674.\n",
      "Yu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020.\n",
      "Reinforcement learning based graph-to-sequence\n",
      "model for natural question generation. In 8th Inter-\n",
      "national Conference on Learning Representations,\n",
      "ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n",
      "2020 . OpenReview.net.\n",
      "Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin\n",
      "Wang, Xiangyang Liu, Mozhi Zhang, Junliang He,\n",
      "Mianqiu Huang, Zhangyue Yin, Kai Chen, andXipeng Qiu. 2023. Evaluating hallucinations in chi-\n",
      "nese large language models.\n",
      "I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua\n",
      "Feng, Chunting Zhou, Junxian He, Graham Neubig,\n",
      "Pengfei Liu, et al. 2023. Factool: Factuality detec-\n",
      "tion in generative ai–a tool augmented framework\n",
      "for multi-task and multi-domain scenarios. ArXiv\n",
      "preprint , abs/2307.13528.\n",
      "Cheng-Han Chiang and Hung-yi Lee. 2023. Can large\n",
      "language models be an alternative to human evalua-\n",
      "tions? ArXiv preprint , abs/2305.01937.\n",
      "David Chiang and Peter Cholak. 2022. Overcoming a\n",
      "theoretical limitation of self-attention. In Proceed-\n",
      "ings of the 60th Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: Long Pa-\n",
      "pers) , pages 7654–7664, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Sukmin Cho, Soyeong Jeong, Jong C Park, et al. 2023.\n",
      "Improving zero-shot reader by reducing distractions\n",
      "from irrelevant documents in open-domain question\n",
      "answering. ArXiv preprint , abs/2310.17490.\n",
      "Sehyun Choi, Tianqing Fang, Zhaowei Wang, and\n",
      "Yangqiu Song. 2023. Kcts: Knowledge-constrained\n",
      "tree search decoding with token-level hallucination\n",
      "detection. ArXiv preprint , abs/2310.09044.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
      "Sebastian Gehrmann, Parker Schuh, Kensen Shi,\n",
      "Sasha Tsvyashchenko, Joshua Maynez, Abhishek\n",
      "Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\n",
      "odkumar Prabhakaran, Emily Reif, Nan Du, Ben\n",
      "Hutchinson, Reiner Pope, James Bradbury, Jacob\n",
      "Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\n",
      "Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\n",
      "Sunipa Dev, Henryk Michalewski, Xavier Garcia,\n",
      "Vedant Misra, Kevin Robinson, Liam Fedus, Denny\n",
      "Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\n",
      "Barret Zoph, Alexander Spiridonov, Ryan Sepassi,\n",
      "David Dohan, Shivani Agrawal, Mark Omernick, An-\n",
      "drew M. Dai, Thanumalayan Sankaranarayana Pil-\n",
      "lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\n",
      "Rewon Child, Oleksandr Polozov, Katherine Lee,\n",
      "Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\n",
      "Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\n",
      "Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\n",
      "and Noah Fiedel. 2023. Palm: Scaling language mod-\n",
      "eling with pathways. J. Mach. Learn. Res. , 24:240:1–\n",
      "240:113.\n",
      "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan\n",
      "Martic, Shane Legg, and Dario Amodei. 2017. Deep\n",
      "reinforcement learning from human preferences. In\n",
      "Advances in Neural Information Processing Systems\n",
      "30: Annual Conference on Neural Information Pro-\n",
      "cessing Systems 2017, December 4-9, 2017, Long\n",
      "Beach, CA, USA , pages 4299–4307.\n",
      "Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\n",
      "Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,Bing Qin, and Ting Liu. 2023. A survey of chain of\n",
      "thought reasoning: Advances, frontiers and future.\n",
      "ArXiv preprint , abs/2309.15402.\n",
      "Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\n",
      "Kim, James Glass, and Pengcheng He. 2023. Dola:\n",
      "Decoding by contrasting layers improves factual-\n",
      "ity in large language models. ArXiv preprint ,\n",
      "abs/2309.03883.\n",
      "Hyung Won Chung, Le Hou, Shayne Longpre, Bar-\n",
      "ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\n",
      "Wang, Mostafa Dehghani, Siddhartha Brahma, et al.\n",
      "2022. Scaling instruction-finetuned language models.\n",
      "ArXiv preprint , abs/2210.11416.\n",
      "Roi Cohen, May Hamri, Mor Geva, and Amir Glober-\n",
      "son. 2023. LM vs LM: detecting factual errors via\n",
      "cross examination. ArXiv preprint , abs/2305.13281.\n",
      "Ajeya Cotra. 2021. Why AI alignment could be hard\n",
      "with modern deep learning. Cold Takes.\n",
      "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\n",
      "Chang, and Furu Wei. 2022a. Knowledge neurons in\n",
      "pretrained transformers. In Proceedings of the 60th\n",
      "Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 8493–\n",
      "8502, Dublin, Ireland. Association for Computational\n",
      "Linguistics.\n",
      "Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu,\n",
      "Qiaoqiao She, and Zhifang Sui. 2022b. Neural8502, Dublin, Ireland. Association for Computational\n",
      "Linguistics.\n",
      "Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu,\n",
      "Qiaoqiao She, and Zhifang Sui. 2022b. Neural\n",
      "knowledge bank for pretrained transformers. ArXiv\n",
      "preprint , abs/2208.00399.\n",
      "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane\n",
      "Hung, Eric Frank, Piero Molino, Jason Yosinski, and\n",
      "Rosanne Liu. 2020. Plug and play language models:\n",
      "A simple approach to controlled text generation. In\n",
      "8th International Conference on Learning Represen-\n",
      "tations, ICLR 2020, Addis Ababa, Ethiopia, April\n",
      "26-30, 2020 . OpenReview.net.\n",
      "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\n",
      "ing factual knowledge in language models. In Pro-\n",
      "ceedings of the 2021 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing , pages 6491–\n",
      "6506, Online and Punta Cana, Dominican Republic.\n",
      "Association for Computational Linguistics.\n",
      "Grégoire Delétang, Anian Ruoss, Paul-Ambroise\n",
      "Duquenne, Elliot Catt, Tim Genewein, Christo-\n",
      "pher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,\n",
      "Matthew Aitchison, Laurent Orseau, et al. 2023. Lan-\n",
      "guage modeling is compression. ArXiv preprint ,\n",
      "abs/2309.10668.\n",
      "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\n",
      "Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Ja-\n",
      "son Weston. 2023. Chain-of-verification reduces hal-\n",
      "lucination in large language models. ArXiv preprint ,\n",
      "abs/2309.11495.Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\n",
      "Zhifang Sui, and Lei Li. 2022. Calibrating factual\n",
      "knowledge in pretrained language models. In Find-\n",
      "ings of the Association for Computational Linguistics:\n",
      "EMNLP 2022 , pages 5937–5947, Abu Dhabi, United\n",
      "Arab Emirates. Association for Computational Lin-\n",
      "guistics.\n",
      "Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao,\n",
      "and Ji-Rong Wen. 2023. Bamboo: A comprehen-\n",
      "sive benchmark for evaluating long text modeling\n",
      "capacities of large language models. ArXiv preprint ,\n",
      "abs/2309.13345.\n",
      "Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenen-\n",
      "baum, and Igor Mordatch. 2023. Improving factual-\n",
      "ity and reasoning in language models through multia-\n",
      "gent debate. ArXiv preprint , abs/2305.14325.\n",
      "Esin Durmus, He He, and Mona Diab. 2020. FEQA: A\n",
      "question answering evaluation framework for faith-\n",
      "fulness assessment in abstractive summarization. In\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 5055–\n",
      "5070, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Nouha Dziri, Andrea Madotto, Osmar Zaïane, and\n",
      "Avishek Joey Bose. 2021a. Neural path hunter: Re-\n",
      "ducing hallucination in dialogue systems via path\n",
      "grounding. In Proceedings of the 2021 Conference\n",
      "on Empirical Methods in Natural Language Process-\n",
      "ing, pages 2197–2214, Online and Punta Cana, Do-\n",
      "minican Republic. Association for Computational\n",
      "Linguistics.\n",
      "Nouha Dziri, Hannah Rashkin, Tal Linzen, and David\n",
      "Reitter. 2021b. Evaluating groundedness in dialogue\n",
      "systems: The begin benchmark. ArXiv preprint ,\n",
      "abs/2105.00071.\n",
      "Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\n",
      "Caiming Xiong. 2022. QAFactEval: Improved QA-\n",
      "based factual consistency evaluation for summariza-\n",
      "tion. In Proceedings of the 2022 Conference of the\n",
      "North American Chapter of the Association for Com-\n",
      "putational Linguistics: Human Language Technolo-\n",
      "gies, pages 2587–2601, Seattle, United States. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Alexander R. Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-\n",
      "Cann, Caiming Xiong, Richard Socher, and Dragomir\n",
      "Radev. 2021. SummEval: Re-evaluating summariza-\n",
      "tion evaluation. Transactions of the Association for\n",
      "Computational Linguistics , 9:391–409.\n",
      "Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\n",
      "Utama, Ido Dagan, and Iryna Gurevych. 2019. Rank-\n",
      "ing generated summaries by correctness: An interest-\n",
      "ing but challenging application for natural language\n",
      "inference. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguistics ,\n",
      "pages 2214–2220, Florence, Italy. Association for\n",
      "Computational Linguistics.Angela Fan, Yacine Jernite, Ethan Perez, David Grang-\n",
      "ier, Jason Weston, and Michael Auli. 2019. ELI5:\n",
      "Long form question answering. In Proceedings of\n",
      "the 57th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 3558–3567, Florence,\n",
      "Italy. Association for Computational Linguistics.\n",
      "Angela Fan, Mike Lewis, and Yann Dauphin. 2018.\n",
      "Hierarchical neural story generation. In Proceedings\n",
      "of the 56th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 889–898, Melbourne, Australia. Association\n",
      "for Computational Linguistics.\n",
      "Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun\n",
      "Yao, Yuchuan Wu, Fei Huang, Yongbin Li, and Qianli\n",
      "Ma. 2023a. Improving factual consistency of text\n",
      "summarization by adversarially decoupling compre-\n",
      "hension and embellishment abilities of llms. CoRR ,\n",
      "abs/2310.19347.\n",
      "Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin\n",
      "Yang, and Bing Qin. 2023b. Retrieval-generation\n",
      "synergy augmented large language models. ArXiv\n",
      "preprint , abs/2310.05149.\n",
      "Emilio Ferrara. 2023. Should chatgpt be biased? chal-\n",
      "lenges and risks of bias in large language models.\n",
      "ArXiv preprint , abs/2304.03738.\n",
      "Katja Filippova. 2020. Controlled hallucinations:\n",
      "Learning to generate faithfully from noisy data. In\n",
      "Findings of the Association for Computational Lin-\n",
      "guistics: EMNLP 2020 , pages 864–870, Online. As-Learning to generate faithfully from noisy data. In\n",
      "Findings of the Association for Computational Lin-\n",
      "guistics: EMNLP 2020 , pages 864–870, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Robert Friel and Atindriyo Sanyal. 2023. Chainpoll: A\n",
      "high efficacy method for llm hallucination detection.\n",
      "Yarin Gal and Zoubin Ghahramani. 2016. Dropout\n",
      "as a bayesian approximation: Representing model\n",
      "uncertainty in deep learning. In Proceedings of the\n",
      "33nd International Conference on Machine Learning,\n",
      "ICML 2016, New York City, NY, USA, June 19-24,\n",
      "2016 , volume 48 of JMLR Workshop and Conference\n",
      "Proceedings , pages 1050–1059. JMLR.org.\n",
      "Boris A Galitsky. 2023. Truth-o-meter: Collaborating\n",
      "with llm in fighting its hallucinations.\n",
      "Leo Gao, Stella Biderman, Sid Black, Laurence Gold-\n",
      "ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-\n",
      "race He, Anish Thite, Noa Nabeshima, et al. 2021.\n",
      "The pile: An 800gb dataset of diverse text for lan-\n",
      "guage modeling. ArXiv preprint , abs/2101.00027.\n",
      "Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\n",
      "Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\n",
      "cent Y . Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\n",
      "and Kelvin Guu. 2023a. RARR: researching and\n",
      "revising what language models say, using language\n",
      "models. In Proceedings of the 61st Annual Meeting\n",
      "of the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers), ACL 2023, Toronto, Canada,\n",
      "July 9-14, 2023 , pages 16477–16508. Association for\n",
      "Computational Linguistics.Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\n",
      "ing Yang, and Xiaojun Wan. 2023b. Human-like sum-\n",
      "marization evaluation with chatgpt. ArXiv preprint ,\n",
      "abs/2304.02554.\n",
      "Mor Geva, Roei Schuster, Jonathan Berant, and Omer\n",
      "Levy. 2021. Transformer feed-forward layers are key-\n",
      "value memories. In Proceedings of the 2021 Confer-\n",
      "ence on Empirical Methods in Natural Language Pro-\n",
      "cessing , pages 5484–5495, Online and Punta Cana,\n",
      "Dominican Republic. Association for Computational\n",
      "Linguistics.\n",
      "Olga Golovneva, Moya Chen, Spencer Poff, Martin\n",
      "Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,\n",
      "and Asli Celikyilmaz. 2022. Roscoe: A suite of\n",
      "metrics for scoring step-by-step reasoning. ArXiv\n",
      "preprint , abs/2212.07919.\n",
      "Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad\n",
      "Saleh. 2019. Assessing the factual accuracy of gener-\n",
      "ated text. In Proceedings of the 25th ACM SIGKDD\n",
      "International Conference on Knowledge Discovery\n",
      "& Data Mining, KDD 2019, Anchorage, AK, USA,\n",
      "August 4-8, 2019 , pages 166–175. ACM.\n",
      "Google. 2023. Bard.\n",
      "Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong\n",
      "Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.\n",
      "2023. Critic: Large language models can self-correct\n",
      "with tool-interactive critiquing. ArXiv preprint ,\n",
      "abs/2305.11738.\n",
      "Tanya Goyal and Greg Durrett. 2020. Evaluating factu-\n",
      "ality in generation with dependency-level entailment.\n",
      "InFindings of the Association for Computational Lin-\n",
      "guistics: EMNLP 2020 , pages 3592–3603, Online.\n",
      "Association for Computational Linguistics.\n",
      "Tanya Goyal and Greg Durrett. 2021. Annotating and\n",
      "modeling fine-grained factuality in summarization.\n",
      "InProceedings of the 2021 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies ,\n",
      "pages 1449–1462, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu,\n",
      "Heng Gong, and Bing Qin. 2022a. Improving control-\n",
      "lable text generation with position-aware weighted\n",
      "decoding. In Findings of the Association for Com-\n",
      "putational Linguistics: ACL 2022 , pages 3449–3467,\n",
      "Dublin, Ireland. Association for Computational Lin-\n",
      "guistics.\n",
      "Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan\n",
      "Zhang, Heng Gong, and Bing Qin. 2022b. Con-\n",
      "trollable text generation via probability density\n",
      "estimation in the latent space. ArXiv preprint ,\n",
      "abs/2212.08307.\n",
      "Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan\n",
      "Zhang, Heng Gong, and Bing Qin. 2022c. A distri-\n",
      "butional lens for multi-aspect controllable text gen-\n",
      "eration. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing ,\n",
      "pages 1023–1043, Abu Dhabi, United Arab Emirates.\n",
      "Association for Computational Linguistics.\n",
      "Nuno M. Guerreiro, Elena V oita, and André Martins.\n",
      "2023a. Looking for a needle in a haystack: A com-\n",
      "prehensive study of hallucinations in neural machine\n",
      "translation. In Proceedings of the 17th Conference\n",
      "of the European Chapter of the Association for Com-\n",
      "putational Linguistics , pages 1059–1075, Dubrovnik,\n",
      "Croatia. Association for Computational Linguistics.\n",
      "Nuno Miguel Guerreiro, Duarte M. Alves, Jonas\n",
      "Waldendorf, Barry Haddow, Alexandra Birch, Pierre\n",
      "Colombo, and André F. T. Martins. 2023b. Hallucina-\n",
      "tions in large multilingual translation models. ArXiv\n",
      "preprint , abs/2303.16104.\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\n",
      "César Teodoro Mendes, Allie Del Giorno, Sivakanth\n",
      "Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\n",
      "de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all\n",
      "you need. ArXiv preprint , abs/2306.11644.\n",
      "Anisha Gunjal, Jihan Yin, and Erhan Bas. 2023. De-\n",
      "tecting and preventing hallucinations in large vision\n",
      "language models. ArXiv preprint , abs/2308.06394.\n",
      "Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\n",
      "chos. 2022. A survey on automated fact-checking.\n",
      "Transactions of the Association for Computational\n",
      "Linguistics , 10:178–206.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\n",
      "and Ming-Wei Chang. 2020. Retrieval augmented\n",
      "language model pre-training. In Proceedings of theLinguistics , 10:178–206.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\n",
      "and Ming-Wei Chang. 2020. Retrieval augmented\n",
      "language model pre-training. In Proceedings of the\n",
      "37th International Conference on Machine Learning,\n",
      "ICML 2020, 13-18 July 2020, Virtual Event , volume\n",
      "119 of Proceedings of Machine Learning Research ,\n",
      "pages 3929–3938. PMLR.\n",
      "Bikash Gyawali, Lucas Anastasiou, and Petr Knoth.\n",
      "2020. Deduplication of scholarly documents using\n",
      "locality sensitive hashing and word embeddings. In\n",
      "Proceedings of the Twelfth Language Resources and\n",
      "Evaluation Conference , pages 901–910, Marseille,\n",
      "France. European Language Resources Association.\n",
      "Michael Hahn. 2020. Theoretical limitations of self-\n",
      "attention in neural sequence models. Transactions of\n",
      "the Association for Computational Linguistics , 8:156–\n",
      "171.\n",
      "Andreas Hanselowski, Christian Stab, Claudia Schulz,\n",
      "Zile Li, and Iryna Gurevych. 2019. A richly anno-\n",
      "tated corpus for different tasks in automated fact-\n",
      "checking. In Proceedings of the 23rd Conference on\n",
      "Computational Natural Language Learning (CoNLL) ,\n",
      "pages 493–503, Hong Kong, China. Association for\n",
      "Computational Linguistics.\n",
      "Thomas Hartvigsen, Swami Sankaranarayanan, Hamid\n",
      "Palangi, Yoon Kim, and Marzyeh Ghassemi. 2022.\n",
      "Aging with GRACE: lifelong model editing with\n",
      "discrete key-value adaptors. ArXiv preprint ,\n",
      "abs/2211.11031.Hangfeng He, Hongming Zhang, and Dan Roth. 2023.\n",
      "Rethinking with retrieval: Faithful large language\n",
      "model inference. ArXiv preprint , abs/2301.00303.\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andy\n",
      "Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-\n",
      "hardt. 2021. Measuring massive multitask language\n",
      "understanding. In 9th International Conference on\n",
      "Learning Representations, ICLR 2021, Virtual Event,\n",
      "Austria, May 3-7, 2021 . OpenReview.net.\n",
      "Danny Hernandez, Tom B. Brown, Tom Conerly, Nova\n",
      "DasSarma, Dawn Drain, Sheer El Showk, Nelson\n",
      "Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan\n",
      "Hume, Scott Johnston, Benjamin Mann, Chris Olah,\n",
      "Catherine Olsson, Dario Amodei, Nicholas Joseph,\n",
      "Jared Kaplan, and Sam McCandlish. 2022. Scaling\n",
      "laws and interpretability of learning from repeated\n",
      "data. ArXiv preprint , abs/2205.10487.\n",
      "Evan Hernandez, Belinda Z Li, and Jacob Andreas.\n",
      "2023. Inspecting and editing knowledge repre-\n",
      "sentations in language models. ArXiv preprint ,\n",
      "abs/2304.00740.\n",
      "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\n",
      "Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\n",
      "Diego de Las Casas, Lisa Anne Hendricks, Johannes\n",
      "Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\n",
      "Katie Millican, George van den Driessche, Bogdan\n",
      "Damoc, Aurelia Guy, Simon Osindero, Karen Si-\n",
      "monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and\n",
      "Laurent Sifre. 2022. Training compute-optimal large\n",
      "language models. ArXiv preprint , abs/2203.15556.\n",
      "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\n",
      "Yejin Choi. 2020. The curious case of neural text\n",
      "degeneration. In 8th International Conference on\n",
      "Learning Representations, ICLR 2020, Addis Ababa,\n",
      "Ethiopia, April 26-30, 2020 . OpenReview.net.\n",
      "Or Honovich, Leshem Choshen, Roee Aharoni, Ella\n",
      "Neeman, Idan Szpektor, and Omri Abend. 2021.\n",
      "q2: Evaluating factual consistency in knowledge-\n",
      "grounded dialogues via question generation and ques-\n",
      "tion answering. In Proceedings of the 2021 Confer-\n",
      "ence on Empirical Methods in Natural Language Pro-\n",
      "cessing , pages 7856–7870, Online and Punta Cana,\n",
      "Dominican Republic. Association for Computational\n",
      "Linguistics.\n",
      "Jie Huang, Xinyun Chen, Swaroop Mishra,\n",
      "Huaixiu Steven Zheng, Adams Wei Yu, Xiny-\n",
      "ing Song, and Denny Zhou. 2023a. Large language\n",
      "models cannot self-correct reasoning yet. ArXiv\n",
      "preprint , abs/2310.01798.\n",
      "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\n",
      "Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\n",
      "Cui, Owais Khan Mohammed, Qiang Liu, et al.\n",
      "2023b. Language is not all you need: Aligning\n",
      "perception with language models. ArXiv preprint ,\n",
      "abs/2302.14045.Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng,\n",
      "and Bing Qin. 2021. The factual inconsistency prob-\n",
      "lem in abstractive text summarization: A survey.\n",
      "ArXiv preprint , abs/2104.14839.\n",
      "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei\n",
      "Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\n",
      "Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023c.\n",
      "C-eval: A multi-level multi-discipline chinese eval-\n",
      "uation suite for foundation models. ArXiv preprint ,\n",
      "abs/2305.08322.\n",
      "Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,\n",
      "Wenge Rong, and Zhang Xiong. 2023d. Transformer-\n",
      "patcher: One mistake worth one neuron. In The\n",
      "Eleventh International Conference on Learning Rep-\n",
      "resentations, ICLR 2023, Kigali, Rwanda, May 1-5,\n",
      "2023 . OpenReview.net.\n",
      "Siqing Huo, Negar Arabzadeh, and Charles L. A. Clarke.\n",
      "2023. Retrieving supporting evidence for llms gener-\n",
      "ated answers. ArXiv preprint , abs/2306.13781.\n",
      "Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\n",
      "Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shus-\n",
      "ter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n",
      "2022. Opt-iml: Scaling language model instruc-\n",
      "tion meta learning through the lens of generalization.\n",
      "ArXiv preprint , abs/2212.12017.\n",
      "Sameer Jain, Vaishakh Keshava, Swarnashree Mysore\n",
      "Sathyendra, Patrick Fernandes, Pengfei Liu, Gra-\n",
      "ham Neubig, and Chunting Zhou. 2023. Multi-\n",
      "dimensional evaluation of text summarization with\n",
      "in-context learning. ArXiv preprint , abs/2306.01200.Sathyendra, Patrick Fernandes, Pengfei Liu, Gra-\n",
      "ham Neubig, and Chunting Zhou. 2023. Multi-\n",
      "dimensional evaluation of text summarization with\n",
      "in-context learning. ArXiv preprint , abs/2306.01200.\n",
      "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\n",
      "Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\n",
      "Madotto, and Pascale Fung. 2023a. Survey of halluci-\n",
      "nation in natural language generation. ACM Comput.\n",
      "Surv. , 55(12):248:1–248:38.\n",
      "Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko\n",
      "Ishii, and Pascale Fung. 2023b. Towards mitigat-\n",
      "ing hallucination in large language models via self-\n",
      "reflection. ArXiv preprint , abs/2310.06271.\n",
      "Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\n",
      "Haibo Ding, and Graham Neubig. 2020. X-FACTR:\n",
      "Multilingual factual knowledge retrieval from pre-\n",
      "trained language models. In Proceedings of the 2020\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing (EMNLP) , pages 5943–5959, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing\n",
      "Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\n",
      "Jamie Callan, and Graham Neubig. 2023. Active\n",
      "retrieval augmented generation. ArXiv preprint ,\n",
      "abs/2305.06983.\n",
      "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom\n",
      "Henighan, Dawn Drain, Ethan Perez, Nicholas\n",
      "Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\n",
      "Tran-Johnson, et al. 2022. Language models\n",
      "(mostly) know what they know. ArXiv preprint ,\n",
      "abs/2207.05221.Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric\n",
      "Wallace, and Colin Raffel. 2023. Large language\n",
      "models struggle to learn long-tail knowledge. In In-\n",
      "ternational Conference on Machine Learning, ICML\n",
      "2023, 23-29 July 2023, Honolulu, Hawaii, USA , vol-\n",
      "ume 202 of Proceedings of Machine Learning Re-\n",
      "search , pages 15696–15707. PMLR.\n",
      "Cheongwoong Kang and Jaesik Choi. 2023. Impact\n",
      "of co-occurrence on factual knowledge of large lan-\n",
      "guage models. ArXiv preprint , abs/2310.08256.\n",
      "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\n",
      "Brown, Benjamin Chess, Rewon Child, Scott Gray,\n",
      "Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\n",
      "Scaling laws for neural language models. ArXiv\n",
      "preprint , abs/2001.08361.\n",
      "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\n",
      "Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n",
      "Wen-tau Yih. 2020. Dense passage retrieval for open-\n",
      "domain question answering. In Proceedings of the\n",
      "2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP) , pages 6769–6781,\n",
      "Online. Association for Computational Linguistics.\n",
      "Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\n",
      "Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\n",
      "Radev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n",
      "2022. Realtime qa: What’s the answer right now?\n",
      "ArXiv preprint , abs/2207.13332.\n",
      "Daniel Martin Katz, Michael James Bommarito, Shang\n",
      "Gao, and Pablo Arredondo. 2023. Gpt-4 passes the\n",
      "bar exam. Available at SSRN 4389233 .\n",
      "Muhammad Khalifa, Lajanugen Logeswaran, Moon-\n",
      "tae Lee, Honglak Lee, and Lu Wang. 2023.\n",
      "Discriminator-guided multi-step reasoning with lan-\n",
      "guage models. ArXiv preprint , abs/2305.14934.\n",
      "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\n",
      "Fu, Kyle Richardson, Peter Clark, and Ashish Sab-\n",
      "harwal. 2022. Decomposed prompting: A modular\n",
      "approach for solving complex tasks. ArXiv preprint ,\n",
      "abs/2210.02406.\n",
      "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\n",
      "taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\n",
      "guage models are zero-shot reasoners. Advances in\n",
      "neural information processing systems , 35:22199–\n",
      "22213.\n",
      "Wojciech Kryscinski, Bryan McCann, Caiming Xiong,\n",
      "and Richard Socher. 2020. Evaluating the factual\n",
      "consistency of abstractive text summarization. In\n",
      "Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) ,\n",
      "pages 9332–9346, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
      "field, Michael Collins, Ankur Parikh, Chris Alberti,\n",
      "Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\n",
      "ton Lee, Kristina Toutanova, Llion Jones, Matthew\n",
      "Kelcey, Ming-Wei Chang, Andrew M. Dai, JakobUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\n",
      "ral questions: A benchmark for question answering\n",
      "research. Transactions of the Association for Compu-\n",
      "tational Linguistics , 7:452–466.\n",
      "Philippe Laban, Wojciech Kry ´sci´nski, Divyansh Agar-\n",
      "wal, Alexander R Fabbri, Caiming Xiong, Shafiq\n",
      "Joty, and Chien-Sheng Wu. 2023. Llms as factual\n",
      "reasoners: Insights from existing benchmarks and\n",
      "beyond. ArXiv preprint , abs/2305.14540.\n",
      "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and\n",
      "Marti A. Hearst. 2022. SummaC: Re-visiting NLI-\n",
      "based models for inconsistency detection in summa-\n",
      "rization. Transactions of the Association for Compu-\n",
      "tational Linguistics , 10:163–177.\n",
      "Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi\n",
      "Zhang, Dan Jurafsky, Kathleen McKeown, and Tat-\n",
      "sunori Hashimoto. 2023. When do pre-training bi-\n",
      "ases propagate to downstream tasks? a case study\n",
      "in text summarization. In Proceedings of the 17th\n",
      "Conference of the European Chapter of the Asso-\n",
      "ciation for Computational Linguistics , pages 3206–\n",
      "3219, Dubrovnik, Croatia. Association for Computa-\n",
      "tional Linguistics.\n",
      "Balaji Lakshminarayanan, Alexander Pritzel, and\n",
      "Charles Blundell. 2017. Simple and scalable pre-\n",
      "dictive uncertainty estimation using deep ensembles.\n",
      "InAdvances in Neural Information Processing Sys-\n",
      "tems 30: Annual Conference on Neural Information\n",
      "Processing Systems 2017, December 4-9, 2017, LongInAdvances in Neural Information Processing Sys-\n",
      "tems 30: Annual Conference on Neural Information\n",
      "Processing Systems 2017, December 4-9, 2017, Long\n",
      "Beach, CA, USA , pages 6402–6413.\n",
      "Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang,\n",
      "and Yi Yang. 2023. Fast and accurate factual incon-\n",
      "sistency detection over long documents.\n",
      "Katherine Lee, Daphne Ippolito, Andrew Nystrom,\n",
      "Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,\n",
      "and Nicholas Carlini. 2022a. Deduplicating training\n",
      "data makes language models better. In Proceedings\n",
      "of the 60th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 8424–8445, Dublin, Ireland. Association for\n",
      "Computational Linguistics.\n",
      "Minhyeok Lee. 2023. A mathematical investigation of\n",
      "hallucination and creativity in gpt models. Mathe-\n",
      "matics , 11(10):2320.\n",
      "Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary,\n",
      "Pascale N Fung, Mohammad Shoeybi, and Bryan\n",
      "Catanzaro. 2022b. Factuality enhanced language\n",
      "models for open-ended text generation. Advances in\n",
      "Neural Information Processing Systems , 35:34586–\n",
      "34599.\n",
      "Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily\n",
      "Ching, Eslam Kamal, et al. 2023. Chain of natu-\n",
      "ral language inference for reducing large language\n",
      "model ungrounded hallucinations. ArXiv preprint ,\n",
      "abs/2310.03951.BA Levinstein and Daniel A Herrmann. 2023. Still\n",
      "no lie detector for language models: Probing em-\n",
      "pirical and conceptual roadblocks. ArXiv preprint ,\n",
      "abs/2307.00175.\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\n",
      "Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\n",
      "Veselin Stoyanov, and Luke Zettlemoyer. 2020a.\n",
      "BART: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and com-\n",
      "prehension. In Proceedings of the 58th Annual Meet-\n",
      "ing of the Association for Computational Linguistics ,\n",
      "pages 7871–7880, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\n",
      "tus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\n",
      "Tim Rocktäschel, Sebastian Riedel, and Douwe\n",
      "Kiela. 2020b. Retrieval-augmented generation for\n",
      "knowledge-intensive NLP tasks. In Advances in Neu-\n",
      "ral Information Processing Systems 33: Annual Con-\n",
      "ference on Neural Information Processing Systems\n",
      "2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
      "Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin\n",
      "Wang, Michal Lukasik, Andreas Veit, Felix X. Yu,\n",
      "and Sanjiv Kumar. 2023a. Large language models\n",
      "with controllable working memory. In Findings of\n",
      "the Association for Computational Linguistics: ACL\n",
      "2023, Toronto, Canada, July 9-14, 2023 , pages 1774–\n",
      "1793. Association for Computational Linguistics.\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n",
      "2023b. Blip-2: Bootstrapping language-image pre-\n",
      "training with frozen image encoders and large lan-\n",
      "guage models. ArXiv preprint , abs/2301.12597.\n",
      "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\n",
      "Nie, and Ji-Rong Wen. 2023c. Halueval: A large-\n",
      "scale hallucination evaluation benchmark for large\n",
      "language models. CoRR , abs/2305.11747.\n",
      "Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\n",
      "Pfister, and Martin Wattenberg. 2023d. Inference-\n",
      "time intervention: Eliciting truthful answers from a\n",
      "language model. ArXiv preprint , abs/2306.03341.\n",
      "Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,\n",
      "Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang,\n",
      "and Qun Liu. 2022a. How pre-trained language mod-\n",
      "els capture factual knowledge? a causal-inspired\n",
      "analysis. In Findings of the Association for Com-\n",
      "putational Linguistics: ACL 2022 , pages 1720–1732,\n",
      "Dublin, Ireland. Association for Computational Lin-\n",
      "guistics.\n",
      "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan\n",
      "Xiao, and Hua Wu. 2022b. Faithfulness in natural\n",
      "language generation: A systematic survey of analysis,\n",
      "evaluation and optimization methods. ArXiv preprint ,\n",
      "abs/2203.05227.\n",
      "Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy\n",
      "Liang, Jason Eisner, Tatsunori Hashimoto, LukeZettlemoyer, and Mike Lewis. 2022c. Contrastive de-\n",
      "coding: Open-ended text generation as optimization.\n",
      "ArXiv preprint , abs/2210.15097.\n",
      "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,\n",
      "Wayne Xin Zhao, and Ji-Rong Wen. 2023e. Eval-\n",
      "uating object hallucination in large vision-language\n",
      "models.\n",
      "Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie\n",
      "Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\n",
      "2023f. Textbooks are all you need ii: phi-1.5 techni-\n",
      "cal report. ArXiv preprint , abs/2309.05463.\n",
      "Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and\n",
      "You Zhang. 2023g. Chatdoctor: A medical chat\n",
      "model fine-tuned on llama model using medical do-\n",
      "main knowledge. ArXiv preprint , abs/2303.14070.\n",
      "Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and\n",
      "Dongjie Yang. 2023h. Batgpt: A bidirectional au-\n",
      "toregessive talker from generative pre-trained trans-\n",
      "former. ArXiv preprint , abs/2307.00360.\n",
      "Chin-Yew Lin. 2004. ROUGE: A package for auto-\n",
      "matic evaluation of summaries. In Text Summariza-\n",
      "tion Branches Out , pages 74–81, Barcelona, Spain.\n",
      "Association for Computational Linguistics.\n",
      "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.\n",
      "TruthfulQA: Measuring how models mimic human\n",
      "falsehoods. In Proceedings of the 60th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 3214–3252, Dublin,falsehoods. In Proceedings of the 60th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 3214–3252, Dublin,\n",
      "Ireland. Association for Computational Linguistics.\n",
      "Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krish-\n",
      "namurthy, and Cyril Zhang. 2023a. Exposing atten-\n",
      "tion glitches with flip-flop language modeling. ArXiv\n",
      "preprint , abs/2306.00946.\n",
      "Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen,\n",
      "Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\n",
      "2023b. Hallusionbench: You see what you think? or\n",
      "you think what you see? an image-context reasoning\n",
      "benchmark challenging for gpt-4v(ision), llava-1.5,\n",
      "and other multi-modality models.\n",
      "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser\n",
      "Yacoob, and Lijuan Wang. 2023c. Mitigating hal-\n",
      "lucination in large multi-modal models via robust\n",
      "instruction tuning.\n",
      "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\n",
      "Lee. 2023d. Visual instruction tuning. ArXiv\n",
      "preprint , abs/2304.08485.\n",
      "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\n",
      "jape, Michele Bevilacqua, Fabio Petroni, and Percy\n",
      "Liang. 2023e. Lost in the middle: How lan-\n",
      "guage models use long contexts. ArXiv preprint ,\n",
      "abs/2307.03172.\n",
      "Nelson F. Liu, Tianyi Zhang, and Percy Liang. 2023f.\n",
      "Evaluating verifiability in generative search engines.\n",
      "ArXiv preprint , abs/2304.09848.Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\n",
      "Ruochen Xu, and Chenguang Zhu. 2023g. Gpte-\n",
      "val: Nlg evaluation using gpt-4 with better human\n",
      "alignment. ArXiv preprint , abs/2303.16634.\n",
      "Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying\n",
      "Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov,\n",
      "Muhammad Faaiz Taufiq, and Hang Li. 2023h. Trust-\n",
      "worthy llms: a survey and guideline for evaluating\n",
      "large language models’ alignment. ArXiv preprint ,\n",
      "abs/2308.05374.\n",
      "Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou.\n",
      "2023i. Instruction position matters in sequence gen-\n",
      "eration with large language models. ArXiv preprint ,\n",
      "abs/2308.12097.\n",
      "Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Zi-\n",
      "wei Ji, and Pascale Fung. 2023. Negative object\n",
      "presence evaluation (nope) to measure object halluci-\n",
      "nation in vision-language models.\n",
      "Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo,\n",
      "Yawen Zhang, Baochen Sun, Carl Yang, and Jie\n",
      "Yang. 2023. Evaluation and mitigation of agnosia in\n",
      "multimodal large language models. ArXiv preprint ,\n",
      "abs/2309.04041.\n",
      "Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\n",
      "Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming\n",
      "Zhou. 2020. Univl: A unified video and language\n",
      "pre-training model for multimodal understanding and\n",
      "generation. ArXiv preprint , abs/2002.06353.\n",
      "Junyu Luo, Cao Xiao, and Fenglong Ma. 2023a. Zero-\n",
      "resource hallucination prevention for large language\n",
      "models. ArXiv preprint , abs/2309.02654.\n",
      "Zheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n",
      "2023b. Chatgpt as a factual inconsistency evaluator\n",
      "for text summarization.\n",
      "Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\n",
      "Delip Rao, Eric Wong, Marianna Apidianaki, and\n",
      "Chris Callison-Burch. 2023. Faithful chain-of-\n",
      "thought reasoning. ArXiv preprint , abs/2301.13379.\n",
      "Muhammad Maaz, Hanoona Rasheed, Salman Khan,\n",
      "and Fahad Shahbaz Khan. 2023. Video-chatgpt:\n",
      "Towards detailed video understanding via large\n",
      "vision and language models. ArXiv preprint ,\n",
      "abs/2306.05424.\n",
      "Fiona Macpherson and Dimitris Platchias. 2013. Hallu-\n",
      "cination: Philosophy and psychology . MIT Press.\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\n",
      "Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\n",
      "Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\n",
      "et al. 2023. Self-refine: Iterative refinement with\n",
      "self-feedback. ArXiv preprint , abs/2303.17651.\n",
      "Andrey Malinin and Mark J. F. Gales. 2021. Uncertainty\n",
      "estimation in autoregressive structured prediction. In\n",
      "9th International Conference on Learning Represen-\n",
      "tations, ICLR 2021, Virtual Event, Austria, May 3-7,\n",
      "2021 . OpenReview.net.Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\n",
      "Daniel Khashabi, and Hannaneh Hajishirzi. 2023.\n",
      "When not to trust language models: Investigating\n",
      "effectiveness of parametric and non-parametric mem-\n",
      "ories. In Proceedings of the 61st Annual Meeting of\n",
      "the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers), ACL 2023, Toronto, Canada,\n",
      "July 9-14, 2023 , pages 9802–9822. Association for\n",
      "Computational Linguistics.\n",
      "Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n",
      "2023. Selfcheckgpt: Zero-resource black-box hal-\n",
      "lucination detection for generative large language\n",
      "models. ArXiv preprint , abs/2303.08896.\n",
      "Udi Manber and Gene Myers. 1993. Suffix arrays: a\n",
      "new method for on-line string searches. siam Journal\n",
      "on Computing , 22(5):935–948.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and\n",
      "Ryan McDonald. 2020. On faithfulness and factu-\n",
      "ality in abstractive summarization. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics , pages 1906–1919, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Nick McKenna, Tianyi Li, Liang Cheng, Moham-\n",
      "mad Javad Hosseini, Mark Johnson, and Mark Steed-\n",
      "man. 2023. Sources of hallucination by large lan-\n",
      "guage models on inference tasks. ArXiv preprint ,\n",
      "abs/2305.14552.\n",
      "Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If\n",
      "beam search is the answer, what was the question?\n",
      "InProceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) ,beam search is the answer, what was the question?\n",
      "InProceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) ,\n",
      "pages 2173–2185, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Kevin Meng, David Bau, Alex Andonian, and Yonatan\n",
      "Belinkov. 2022. Locating and editing factual associa-\n",
      "tions in GPT. In NeurIPS .\n",
      "Kevin Meng, Arnab Sen Sharma, Alex J. Andonian,\n",
      "Yonatan Belinkov, and David Bau. 2023. Mass-\n",
      "editing memory in a transformer. In The Eleventh\n",
      "International Conference on Learning Representa-\n",
      "tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .\n",
      "OpenReview.net.\n",
      "Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua\n",
      "Zhou, and Jie Zhou. 2021. Prevent the language\n",
      "model from being overconfident in neural machine\n",
      "translation. In Proceedings of the 59th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "and the 11th International Joint Conference on Natu-\n",
      "ral Language Processing (Volume 1: Long Papers) ,\n",
      "pages 3456–3468, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Ning Miao, Yee Whye Teh, and Tom Rainforth.\n",
      "2023. Selfcheck: Using llms to zero-shot check\n",
      "their own step-by-step reasoning. ArXiv preprint ,\n",
      "abs/2308.00436.\n",
      "Microsoft. 2023. New bing.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\n",
      "Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\n",
      "Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\n",
      "Factscore: Fine-grained atomic evaluation of fac-\n",
      "tual precision in long form text generation. ArXiv\n",
      "preprint , abs/2305.14251.\n",
      "Anshuman Mishra, Dhruvesh Patel, Aparna Vijayaku-\n",
      "mar, Xiang Lorraine Li, Pavan Kapanipathi, and Kar-\n",
      "tik Talamadupula. 2021. Looking beyond sentence-\n",
      "level natural language inference for question answer-\n",
      "ing and text summarization. In Proceedings of the\n",
      "2021 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 1322–1336, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\n",
      "Finn, and Christopher D. Manning. 2022a. Fast\n",
      "model editing at scale. In The Tenth International\n",
      "Conference on Learning Representations, ICLR 2022,\n",
      "Virtual Event, April 25-29, 2022 . OpenReview.net.\n",
      "Eric Mitchell, Charles Lin, Antoine Bosselut, Christo-\n",
      "pher D. Manning, and Chelsea Finn. 2022b. Memory-\n",
      "based model editing at scale. In International Con-\n",
      "ference on Machine Learning, ICML 2022, 17-23\n",
      "July 2022, Baltimore, Maryland, USA , volume 162 of\n",
      "Proceedings of Machine Learning Research , pages\n",
      "15817–15831. PMLR.\n",
      "Luca Moschella, Valentino Maiorca, Marco Fumero,\n",
      "Antonio Norelli, Francesco Locatello, and Emanuele\n",
      "Rodola. 2022. Relative representations enable zero-\n",
      "shot latent space communication. ArXiv preprint ,\n",
      "abs/2209.15430.\n",
      "Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\n",
      "Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin\n",
      "Leyton-Brown, Amnon Shashua, and Yoav Shoham.\n",
      "2023. Generating benchmarks for factuality eval-\n",
      "uation of language models. ArXiv preprint ,\n",
      "abs/2307.06908.\n",
      "Anirban Mukherjee and Hannah Chang. 2023. The cre-\n",
      "ative frontier of generative ai: Managing the novelty-\n",
      "usefulness tradeoff. ArXiv preprint , abs/2306.03601.\n",
      "Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero\n",
      "Nogueira dos Santos, Henghui Zhu, Dejiao Zhang,\n",
      "Kathleen McKeown, and Bing Xiang. 2021. Entity-\n",
      "level factual consistency of abstractive text summa-\n",
      "rization. In Proceedings of the 16th Conference of\n",
      "the European Chapter of the Association for Compu-\n",
      "tational Linguistics: Main Volume , pages 2727–2733,\n",
      "Online. Association for Computational Linguistics.\n",
      "Shashi Narayan, Shay B. Cohen, and Mirella Lapata.\n",
      "2018. Don’t give me the details, just the summary!\n",
      "topic-aware convolutional neural networks for ex-\n",
      "treme summarization. In Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing, Brussels, Belgium, October 31 -\n",
      "November 4, 2018 , pages 1797–1807. Association\n",
      "for Computational Linguistics.Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan-\n",
      "chanadikar, Ting-Hao Huang, and Shomir Wilson.\n",
      "2023. Nationality bias in text generation. In Proceed-\n",
      "ings of the 17th Conference of the European Chap-\n",
      "ter of the Association for Computational Linguistics ,\n",
      "pages 116–122, Dubrovnik, Croatia. Association for\n",
      "Computational Linguistics.\n",
      "Sean O’Brien and Mike Lewis. 2023. Contrastive de-\n",
      "coding improves reasoning in large language models.\n",
      "ArXiv preprint , abs/2309.09117.\n",
      "Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg\n",
      "Durrett. 2022. Entity cloze by date: What LMs know\n",
      "about unseen entities. In Findings of the Associa-\n",
      "tion for Computational Linguistics: NAACL 2022 ,\n",
      "pages 693–702, Seattle, United States. Association\n",
      "for Computational Linguistics.\n",
      "OpenAI. 2022. Introducing chatgpt.\n",
      "OpenAI. 2023. GPT-4 technical report. ArXiv preprint ,\n",
      "abs/2303.08774.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n",
      "Carroll L. Wainwright, Pamela Mishkin, Chong\n",
      "Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\n",
      "John Schulman, Jacob Hilton, Fraser Kelton, Luke\n",
      "Miller, Maddie Simens, Amanda Askell, Peter Welin-\n",
      "der, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n",
      "2022. Training language models to follow instruc-\n",
      "tions with human feedback. In NeurIPS .Miller, Maddie Simens, Amanda Askell, Peter Welin-\n",
      "der, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n",
      "2022. Training language models to follow instruc-\n",
      "tions with human feedback. In NeurIPS .\n",
      "Lorenzo Pacchiardi, Alex J Chan, Sören Mindermann,\n",
      "Ilan Moscovitz, Alexa Y Pan, Yarin Gal, Owain\n",
      "Evans, and Jan Brauner. 2023. How to catch an\n",
      "ai liar: Lie detection in black-box llms by asking\n",
      "unrelated questions. ArXiv preprint , abs/2309.15840.\n",
      "Artidoro Pagnoni, Vidhisha Balachandran, and Yulia\n",
      "Tsvetkov. 2021. Understanding factuality in abstrac-\n",
      "tive summarization with FRANK: A benchmark for\n",
      "factuality metrics. In Proceedings of the 2021 Con-\n",
      "ference of the North American Chapter of the Asso-\n",
      "ciation for Computational Linguistics: Human Lan-\n",
      "guage Technologies , pages 4812–4829, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Liangming Pan, Michael Saxon, Wenda Xu, Deepak\n",
      "Nathani, Xinyi Wang, and William Yang Wang. 2023.\n",
      "Automatically correcting large language models: Sur-\n",
      "veying the landscape of diverse self-correction strate-\n",
      "gies. ArXiv preprint , abs/2308.03188.\n",
      "Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann,\n",
      "Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\n",
      "Dipanjan Das. 2020. Totto: A controlled table-to-text\n",
      "generation dataset. In Proceedings of the 2020 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing, EMNLP 2020, Online, November 16-20,\n",
      "2020 , pages 1173–1186. Association for Computa-\n",
      "tional Linguistics.\n",
      "Amandalynne Paullada, Inioluwa Deborah Raji,\n",
      "Emily M. Bender, Emily Denton, and Alex Hanna.\n",
      "2021. Data and its (dis)contents: A survey of datasetdevelopment and use in machine learning research.\n",
      "Patterns , 2(11):100336.\n",
      "Guilherme Penedo, Quentin Malartic, Daniel Hesslow,\n",
      "Ruxandra Cojocaru, Alessandro Cappelli, Hamza\n",
      "Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\n",
      "and Julien Launay. 2023. The refinedweb dataset\n",
      "for falcon LLM: outperforming curated corpora with\n",
      "web data, and web data only. ArXiv preprint ,\n",
      "abs/2306.01116.\n",
      "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\n",
      "ley, and Jianfeng Gao. 2023. Instruction tuning with\n",
      "gpt-4. ArXiv preprint , abs/2304.03277.\n",
      "Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina\n",
      "Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,\n",
      "Catherine Olsson, Sandipan Kundu, Saurav Kada-\n",
      "vath, Andy Jones, Anna Chen, Benjamin Mann,\n",
      "Brian Israel, Bryan Seethor, Cameron McKinnon,\n",
      "Christopher Olah, Da Yan, Daniela Amodei, Dario\n",
      "Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson,\n",
      "Guro Khundadze, Jackson Kernion, James Landis,\n",
      "Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua\n",
      "Landau, Kamal Ndousse, Landon Goldberg, Liane\n",
      "Lovitt, Martin Lucas, Michael Sellitto, Miranda\n",
      "Zhang, Neerav Kingsland, Nelson Elhage, Nicholas\n",
      "Joseph, Noemí Mercado, Nova DasSarma, Oliver\n",
      "Rausch, Robin Larson, Sam McCandlish, Scott John-\n",
      "ston, Shauna Kravec, Sheer El Showk, Tamera Lan-\n",
      "ham, Timothy Telleen-Lawton, Tom Brown, Tom\n",
      "Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-\n",
      "Dodds, Jack Clark, Samuel R. Bowman, Amanda\n",
      "Askell, Roger Grosse, Danny Hernandez, Deep Gan-\n",
      "guli, Evan Hubinger, Nicholas Schiefer, and Jared\n",
      "Kaplan. 2023. Discovering language model behav-\n",
      "iors with model-written evaluations. In Findings\n",
      "of the Association for Computational Linguistics:\n",
      "ACL 2023, Toronto, Canada, July 9-14, 2023 , pages\n",
      "13387–13434. Association for Computational Lin-\n",
      "guistics.\n",
      "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,\n",
      "Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and\n",
      "Alexander Miller. 2019. Language models as knowl-\n",
      "edge bases? In Proceedings of the 2019 Confer-\n",
      "ence on Empirical Methods in Natural Language Pro-\n",
      "cessing and the 9th International Joint Conference\n",
      "on Natural Language Processing (EMNLP-IJCNLP) ,\n",
      "pages 2463–2473, Hong Kong, China. Association\n",
      "for Computational Linguistics.\n",
      "Yuval Pinter and Michael Elhadad. 2023. Emptying the\n",
      "ocean with a spoon: Should we edit models?\n",
      "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n",
      "Noah A Smith, and Mike Lewis. 2022. Measuring\n",
      "and narrowing the compositionality gap in language\n",
      "models. ArXiv preprint , abs/2210.03350.\n",
      "Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and\n",
      "Yongfeng Huang. 2023. Foodgpt: A large lan-\n",
      "guage model in food testing domain with incremental\n",
      "pre-training and knowledge graph prompt. ArXiv\n",
      "preprint , abs/2308.10173.Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\n",
      "Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\n",
      "and Huajun Chen. 2022. Reasoning with lan-\n",
      "guage model prompting: A survey. ArXiv preprint ,\n",
      "abs/2212.09597.\n",
      "Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\n",
      "Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\n",
      "Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\n",
      "Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-\n",
      "teractive web search for chinese long-form question\n",
      "answering.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\n",
      "Sutskever, et al. 2018. Improving language under-\n",
      "standing by generative pre-training.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, Ilya Sutskever, et al. 2019. Language\n",
      "models are unsupervised multitask learners. OpenAI\n",
      "blog, 1(8):9.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J. Liu. 2020. Exploring the limits\n",
      "of transfer learning with a unified text-to-text trans-\n",
      "former. J. Mach. Learn. Res. , 21:140:1–140:67.\n",
      "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and Yoav\n",
      "Shoham. 2023. In-context retrieval-augmented lan-\n",
      "guage models. ArXiv preprint , abs/2302.00083.\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\n",
      "and Wojciech Zaremba. 2016. Sequence level train-Shoham. 2023. In-context retrieval-augmented lan-\n",
      "guage models. ArXiv preprint , abs/2302.00083.\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\n",
      "and Wojciech Zaremba. 2016. Sequence level train-\n",
      "ing with recurrent neural networks. In 4th Inter-\n",
      "national Conference on Learning Representations,\n",
      "ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\n",
      "Conference Track Proceedings .\n",
      "Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\n",
      "Lora Aroyo, Michael Collins, Dipanjan Das, Slav\n",
      "Petrov, Gaurav Singh Tomar, Iulia Turc, and David\n",
      "Reitter. Measuring attribution in natural language\n",
      "generation models. Computational Linguistics ,\n",
      "pages 1–66.\n",
      "Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023.\n",
      "A survey of hallucination in large foundation models.\n",
      "ArXiv preprint , abs/2309.05922.\n",
      "Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin\n",
      "Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,\n",
      "and Haifeng Wang. 2023. Investigating the fac-\n",
      "tual knowledge boundary of large language mod-\n",
      "els with retrieval augmentation. ArXiv preprint ,\n",
      "abs/2307.11019.\n",
      "Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu,\n",
      "Rui Dong, Deguang Kong, Juliette Burger, Anjelica\n",
      "Ramos, William Wang, Zhiheng Huang, et al. 2023.\n",
      "Street: A multi-task structured reasoning and expla-\n",
      "nation benchmark. ArXiv preprint , abs/2302.06729.\n",
      "Nina Rimsky. 2023a. Modulating sycophancy in an rlhf\n",
      "model via activation steering.Nina Rimsky. 2023b. Reducing sycophancy and im-\n",
      "proving honesty via activation steering.\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n",
      "How much knowledge can you pack into the param-\n",
      "eters of a language model? In Proceedings of the\n",
      "2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP) , pages 5418–5426,\n",
      "Online. Association for Computational Linguistics.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n",
      "Patrick Esser, and Björn Ommer. 2022. High-\n",
      "resolution image synthesis with latent diffusion mod-\n",
      "els. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 10684–10695.\n",
      "Sashank Santhanam, Behnam Hedayatnia, Spandana\n",
      "Gella, Aishwarya Padmakumar, Seokhwan Kim,\n",
      "Yang Liu, and Dilek Hakkani-Tur. 2021. Rome was\n",
      "built in 1776: A case study on factual correctness\n",
      "in knowledge-grounded response generation. ArXiv\n",
      "preprint , abs/2110.05456.\n",
      "William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,\n",
      "Long Ouyang, Jonathan Ward, and Jan Leike. 2022.\n",
      "Self-critiquing models for assisting human evaluators.\n",
      "ArXiv preprint , abs/2206.05802.\n",
      "John Schulman. 2023. Reinforcement learning from\n",
      "human feedback: Progress and challenges.\n",
      "John Schulman, Filip Wolski, Prafulla Dhariwal,\n",
      "Alec Radford, and Oleg Klimov. 2017. Proximal\n",
      "policy optimization algorithms. ArXiv preprint ,\n",
      "abs/1707.06347.\n",
      "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\n",
      "Benjamin Piwowarski, Jacopo Staiano, Alex Wang,\n",
      "and Patrick Gallinari. 2021. QuestEval: Summariza-\n",
      "tion asks for fact-based evaluation. In Proceedings of\n",
      "the 2021 Conference on Empirical Methods in Natu-\n",
      "ral Language Processing , pages 6594–6604, Online\n",
      "and Punta Cana, Dominican Republic. Association\n",
      "for Computational Linguistics.\n",
      "Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie\n",
      "Huang, Nan Duan, and Weizhu Chen. 2023. En-\n",
      "hancing retrieval-augmented large language models\n",
      "with iterative retrieval-generation synergy. ArXiv\n",
      "preprint , abs/2305.15294.\n",
      "Mrinank Sharma, Meg Tong, Tomasz Korbak, David\n",
      "Duvenaud, Amanda Askell, Samuel R. Bowman,\n",
      "Newton Cheng, Esin Durmus, Zac Hatfield-Dodds,\n",
      "Scott R. Johnston, Shauna Kravec, Timothy Maxwell,\n",
      "Sam McCandlish, Kamal Ndousse, Oliver Rausch,\n",
      "Nicholas Schiefer, Da Yan, Miranda Zhang, and\n",
      "Ethan Perez. 2023. Towards understanding syco-\n",
      "phancy in language models. ArXiv preprint ,\n",
      "abs/2310.13548.\n",
      "Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua\n",
      "Wu, Maosong Sun, and Yang Liu. 2016. Minimumrisk training for neural machine translation. In Pro-\n",
      "ceedings of the 54th Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (Volume 1: Long\n",
      "Papers) , pages 1683–1692, Berlin, Germany. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan\n",
      "Scales, David Dohan, Ed H. Chi, Nathanael Schärli,\n",
      "and Denny Zhou. 2023a. Large language models can\n",
      "be easily distracted by irrelevant context. In Interna-\n",
      "tional Conference on Machine Learning, ICML 2023,\n",
      "23-29 July 2023, Honolulu, Hawaii, USA , volume\n",
      "202 of Proceedings of Machine Learning Research ,\n",
      "pages 31210–31227. PMLR.\n",
      "Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia\n",
      "Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau\n",
      "Yih. 2023b. Trusting your evidence: Hallucinate\n",
      "less with context-aware decoding. ArXiv preprint ,\n",
      "abs/2305.14739.\n",
      "Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou,\n",
      "Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke\n",
      "Zettlemoyer, Scott Yih, and Mike Lewis. 2023c. In-\n",
      "context pretraining: Language modeling beyond doc-\n",
      "ument boundaries. ArXiv preprint , abs/2310.10638.\n",
      "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\n",
      "and Jason Weston. 2021. Retrieval augmentation\n",
      "reduces hallucination in conversation. In Findings\n",
      "of the Association for Computational Linguistics:\n",
      "EMNLP 2021 , pages 3784–3803, Punta Cana, Do-\n",
      "minican Republic. Association for Computational\n",
      "Linguistics.\n",
      "Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\n",
      "Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\n",
      "Heather Cole-Lewis, Darlene Neal, Mike Schaeker-Linguistics.\n",
      "Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\n",
      "Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\n",
      "Heather Cole-Lewis, Darlene Neal, Mike Schaeker-\n",
      "mann, Amy Wang, Mohamed Amin, Sami Lachgar,\n",
      "Philip Andrew Mansfield, Sushant Prakash, Bradley\n",
      "Green, Ewa Dominowska, Blaise Agüera y Arcas,\n",
      "Nenad Tomasev, Yun Liu, Renee Wong, Christo-\n",
      "pher Semturs, S. Sara Mahdavi, Joelle K. Barral,\n",
      "Dale R. Webster, Gregory S. Corrado, Yossi Matias,\n",
      "Shekoofeh Azizi, Alan Karthikesalingam, and Vivek\n",
      "Natarajan. 2023. Towards expert-level medical ques-\n",
      "tion answering with large language models. ArXiv\n",
      "preprint , abs/2305.09617.\n",
      "Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,\n",
      "Sergei Popov, and Artem Babenko. 2020. Editable\n",
      "neural networks. In 8th International Conference on\n",
      "Learning Representations, ICLR 2020, Addis Ababa,\n",
      "Ethiopia, April 26-30, 2020 . OpenReview.net.\n",
      "Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido\n",
      "Dagan, and Shauli Ravfogel. 2023. The curious case\n",
      "of hallucinatory unanswerablity: Finding truths in\n",
      "the hidden states of over-confident large language\n",
      "models. ArXiv preprint , abs/2310.11877.\n",
      "Felix Stahlberg and Bill Byrne. 2019. On NMT search\n",
      "errors and model errors: Cat got your tongue? In\n",
      "Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the9th International Joint Conference on Natural Lan-\n",
      "guage Processing (EMNLP-IJCNLP) , pages 3356–\n",
      "3362, Hong Kong, China. Association for Computa-\n",
      "tional Linguistics.\n",
      "Kaya Stechly, Matthew Marquez, and Subbarao Kamb-\n",
      "hampati. 2023. Gpt-4 doesn’t know it’s wrong: An\n",
      "analysis of iterative prompting for reasoning prob-\n",
      "lems. ArXiv preprint , abs/2310.12397.\n",
      "Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\n",
      "Wei Chang. 2022. ASQA: Factoid questions meet\n",
      "long-form answers. In Proceedings of the 2022 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing , pages 8273–8288, Abu Dhabi, United\n",
      "Arab Emirates. Association for Computational Lin-\n",
      "guistics.\n",
      "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M.\n",
      "Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,\n",
      "Dario Amodei, and Paul F. Christiano. 2020. Learn-\n",
      "ing to summarize with human feedback. In Advances\n",
      "in Neural Information Processing Systems 33: An-\n",
      "nual Conference on Neural Information Processing\n",
      "Systems 2020, NeurIPS 2020, December 6-12, 2020,\n",
      "virtual .\n",
      "Nishant Subramani, Nivedita Suresh, and Matthew Pe-\n",
      "ters. 2022. Extracting latent steering vectors from\n",
      "pretrained language models. In Findings of the Asso-\n",
      "ciation for Computational Linguistics: ACL 2022 ,\n",
      "pages 566–581, Dublin, Ireland. Association for\n",
      "Computational Linguistics.\n",
      "Ilya Sutskever. 2023. An observation on generalization.\n",
      "Liyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe\n",
      "Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kry ´s-\n",
      "ci´nski, Justin F Rousseau, and Greg Durrett. 2022.\n",
      "Understanding factual errors in summarization: Er-\n",
      "rors, summarizers, datasets, error detectors. ArXiv\n",
      "preprint , abs/2205.12854.\n",
      "Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang,\n",
      "Yang Liu, Tianshui Chen, and Liang Lin. 2023. To-\n",
      "wards causalgpt: A multi-agent approach for faithful\n",
      "knowledge reasoning via promoting causal consis-\n",
      "tency in llms. CoRR , abs/2308.11914.\n",
      "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas\n",
      "Scialom, Anthony Hartshorn, Elvis Saravia, Andrew\n",
      "Poulton, Viktor Kerkez, and Robert Stojnic. 2022.\n",
      "Galactica: A large language model for science. ArXiv\n",
      "preprint , abs/2211.09085.\n",
      "Ran Tian, Shashi Narayan, Thibault Sellam, and\n",
      "Ankur P Parikh. 2019. Sticking to the facts: Con-\n",
      "fident decoding for faithful data-to-text generation.\n",
      "ArXiv preprint , abs/1910.08684.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\n",
      "bert, Amjad Almahairi, Yasmine Babaei, Nikolay\n",
      "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\n",
      "Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\n",
      "Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,\n",
      "Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-\n",
      "thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\n",
      "Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\n",
      "Isabel Kloumann, Artem Korenev, Punit Singh Koura,\n",
      "Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\n",
      "ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\n",
      "tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\n",
      "bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\n",
      "stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\n",
      "Ruan Silva, Eric Michael Smith, Ranjan Subrama-\n",
      "nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\n",
      "lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\n",
      "Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\n",
      "Melanie Kambadur, Sharan Narang, Aurélien Ro-\n",
      "driguez, Robert Stojnic, Sergey Edunov, and Thomas\n",
      "Scialom. 2023. Llama 2: Open foundation and fine-\n",
      "tuned chat models. ArXiv preprint , abs/2307.09288.\n",
      "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\n",
      "and Ashish Sabharwal. 2023. Interleaving retrieval\n",
      "with chain-of-thought reasoning for knowledge-\n",
      "intensive multi-step questions. In Proceedings of\n",
      "the 61st Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 1: Long Papers),\n",
      "ACL 2023, Toronto, Canada, July 9-14, 2023 , pages\n",
      "10014–10037. Association for Computational Lin-\n",
      "guistics.\n",
      "Logesh Kumar Umapathi, Ankit Pal, and Malaikannan\n",
      "Sankarasubbu. 2023. Med-halt: Medical domain\n",
      "hallucination test for large language models. ArXivguistics.\n",
      "Logesh Kumar Umapathi, Ankit Pal, and Malaikannan\n",
      "Sankarasubbu. 2023. Med-halt: Medical domain\n",
      "hallucination test for large language models. ArXiv\n",
      "preprint , abs/2307.15343.\n",
      "Karthik Valmeekam, Matthew Marquez, and Subbarao\n",
      "Kambhampati. 2023. Can large language models\n",
      "really improve by self-critiquing their own plans?\n",
      "ArXiv preprint , abs/2310.08118.\n",
      "Liam van der Poel, Ryan Cotterell, and Clara Meis-\n",
      "ter. 2022. Mutual information alleviates hallucina-\n",
      "tions in abstractive summarization. In Proceedings\n",
      "of the 2022 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing , pages 5956–5965, Abu\n",
      "Dhabi, United Arab Emirates. Association for Com-\n",
      "putational Linguistics.\n",
      "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jian-\n",
      "shu Chen, and Dong Yu. 2023. A stitch in time saves\n",
      "nine: Detecting and mitigating hallucinations of\n",
      "llms by validating low-confidence generation. ArXiv\n",
      "preprint , abs/2307.03987.\n",
      "Hrishikesh Viswanath and Tianyi Zhang. 2023. Fairpy:\n",
      "A toolkit for evaluation of social biases and their\n",
      "mitigation in large language models. ArXiv preprint ,\n",
      "abs/2302.05508.\n",
      "Elena V oita, David Talbot, Fedor Moiseev, Rico Sen-\n",
      "nrich, and Ivan Titov. 2019. Analyzing multi-head\n",
      "self-attention: Specialized heads do the heavy lift-\n",
      "ing, the rest can be pruned. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Computa-\n",
      "tional Linguistics , pages 5797–5808, Florence, Italy.\n",
      "Association for Computational Linguistics.Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry\n",
      "Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny\n",
      "Zhou, Quoc Le, and Thang Luong. 2023. Freshllms:\n",
      "Refreshing large language models with search engine\n",
      "augmentation.\n",
      "David Wan, Mengwen Liu, Kathleen McKeown,\n",
      "Markus Dreyer, and Mohit Bansal. 2023.\n",
      "Faithfulness-aware decoding strategies for ab-\n",
      "stractive summarization. In Proceedings of the\n",
      "17th Conference of the European Chapter of the\n",
      "Association for Computational Linguistics , pages\n",
      "2864–2880, Dubrovnik, Croatia. Association for\n",
      "Computational Linguistics.\n",
      "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020a.\n",
      "Asking and answering questions to evaluate the fac-\n",
      "tual consistency of summaries. In Proceedings of the\n",
      "58th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 5008–5020, Online. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Binjie Wang, Ethan Chern, and Pengfei Liu. 2023a.\n",
      "Chinesefacteval: A factuality benchmark for chinese\n",
      "llms.\n",
      "Chaojun Wang, Yang Liu, and Wai Lam. 2023b. Pro-\n",
      "gressive translation: Improving domain robustness\n",
      "of neural machine translation with intermediate se-\n",
      "quences. ArXiv preprint , abs/2305.09154.\n",
      "Chaojun Wang and Rico Sennrich. 2020. On exposure\n",
      "bias, hallucination and domain shift in neural ma-\n",
      "chine translation. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 3544–3552, Online. Association for\n",
      "Computational Linguistics.\n",
      "Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru\n",
      "Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao,\n",
      "Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang,\n",
      "Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang,\n",
      "and Yue Zhang. 2023c. Survey on factuality in large\n",
      "language models: Knowledge, retrieval and domain-\n",
      "specificity. ArXiv preprint , abs/2310.07521.\n",
      "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\n",
      "Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n",
      "2023d. Is chatgpt a good nlg evaluator? a preliminary\n",
      "study. ArXiv preprint , abs/2303.04048.\n",
      "Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao,\n",
      "and Jiarong Xu. 2023e. Cross-lingual knowledge\n",
      "editing in large language models.\n",
      "Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan\n",
      "Gao, Bing Yin, and Xiang Ren. 2023f. Scott:\n",
      "Self-consistent chain-of-thought distillation. ArXiv\n",
      "preprint , abs/2305.01879.\n",
      "Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan\n",
      "Xu, Xin Liu, Yangqiu Song, and Antoine Bosse-\n",
      "lut. 2023g. CAR: conceptualization-augmented rea-\n",
      "soner for zero-shot commonsense question answer-\n",
      "ing. CoRR , abs/2305.14869.Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu,\n",
      "and Changyou Chen. 2020b. Towards faithful neu-\n",
      "ral table-to-text generation with content-matching\n",
      "constraints. In Proceedings of the 58th Annual Meet-\n",
      "ing of the Association for Computational Linguistics ,\n",
      "pages 1072–1086, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\n",
      "lia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\n",
      "visual language model pretraining with weak super-\n",
      "vision. In The Tenth International Conference on\n",
      "Learning Representations, ICLR 2022, Virtual Event,\n",
      "April 25-29, 2022 . OpenReview.net.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\n",
      "et al. 2022. Chain-of-thought prompting elicits rea-\n",
      "soning in large language models. Advances in Neural\n",
      "Information Processing Systems , 35:24824–24837.\n",
      "Jerry W. Wei, Da Huang, Yifeng Lu, Denny Zhou,\n",
      "and Quoc V . Le. 2023. Simple synthetic data re-\n",
      "duces sycophancy in large language models. ArXiv\n",
      "preprint , abs/2308.03958.\n",
      "Laura Weidinger, John Mellor, Maribeth Rauh, Conor\n",
      "Griffin, Jonathan Uesato, Po-Sen Huang, Myra\n",
      "Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\n",
      "Zac Kenton, Sasha Brown, Will Hawkins, Tom\n",
      "Stepleton, Courtney Biles, Abeba Birhane, Julia\n",
      "Haas, Laura Rimell, Lisa Anne Hendricks, William\n",
      "Isaac, Sean Legassick, Geoffrey Irving, and Iason\n",
      "Gabriel. 2021. Ethical and social risks of harm from\n",
      "language models. ArXiv preprint , abs/2112.04359.Isaac, Sean Legassick, Geoffrey Irving, and Iason\n",
      "Gabriel. 2021. Ethical and social risks of harm from\n",
      "language models. ArXiv preprint , abs/2112.04359.\n",
      "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\n",
      "nan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\n",
      "ral text generation with unlikelihood training. In\n",
      "8th International Conference on Learning Represen-\n",
      "tations, ICLR 2020, Addis Ababa, Ethiopia, April\n",
      "26-30, 2020 . OpenReview.net.\n",
      "Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023.\n",
      "Mindmap: Knowledge graph prompting sparks graph\n",
      "of thoughts in large language models. ArXiv preprint ,\n",
      "abs/2308.09729.\n",
      "Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and\n",
      "Mingming Sun. 2023. Eva-kellm: A new bench-\n",
      "mark for evaluating knowledge editing of llms. ArXiv\n",
      "preprint , abs/2308.09954.\n",
      "Yijun Xiao and William Yang Wang. 2021. On hal-\n",
      "lucination and predictive uncertainty in conditional\n",
      "language generation. In Proceedings of the 16th Con-\n",
      "ference of the European Chapter of the Association\n",
      "for Computational Linguistics: Main Volume , pages\n",
      "2734–2744, Online. Association for Computational\n",
      "Linguistics.\n",
      "Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-\n",
      "Yen Kan, Junxian He, and Qizhe Xie. 2023. De-\n",
      "composition enhances reasoning via self-evaluation\n",
      "guided decoding. ArXiv preprint , abs/2305.00633.Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie\n",
      "Fu, Junxian He, and Bryan Hooi. 2023. Can llms\n",
      "express their uncertainty? an empirical evaluation\n",
      "of confidence elicitation in llms. ArXiv preprint ,\n",
      "abs/2306.13063.\n",
      "Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-\n",
      "comp: Improving retrieval-augmented lms with com-\n",
      "pression and selective augmentation. ArXiv preprint ,\n",
      "abs/2310.04408.\n",
      "Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Un-\n",
      "derstanding neural abstractive summarization models\n",
      "via uncertainty. In Proceedings of the 2020 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP) , pages 6275–6281, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023.\n",
      "A new benchmark and reverse validation method for\n",
      "passage-level hallucination detection. ArXiv preprint ,\n",
      "abs/2310.06498.\n",
      "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\n",
      "William W. Cohen. 2018a. Breaking the softmax\n",
      "bottleneck: A high-rank RNN language model. In\n",
      "6th International Conference on Learning Represen-\n",
      "tations, ICLR 2018, Vancouver, BC, Canada, April\n",
      "30 - May 3, 2018, Conference Track Proceedings .\n",
      "OpenReview.net.\n",
      "Zhilin Yang, Thang Luong, Ruslan Salakhutdinov, and\n",
      "Quoc V . Le. 2019. Mixtape: Breaking the softmax\n",
      "bottleneck efficiently. In Advances in Neural In-\n",
      "formation Processing Systems 32: Annual Confer-\n",
      "ence on Neural Information Processing Systems 2019,\n",
      "NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\n",
      "Canada , pages 15922–15930.\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\n",
      "William Cohen, Ruslan Salakhutdinov, and Christo-\n",
      "pher D. Manning. 2018b. HotpotQA: A dataset for\n",
      "diverse, explainable multi-hop question answering.\n",
      "InProceedings of the 2018 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "2369–2380, Brussels, Belgium. Association for Com-\n",
      "putational Linguistics.\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\n",
      "William Cohen, Ruslan Salakhutdinov, and Christo-\n",
      "pher D. Manning. 2018c. HotpotQA: A dataset for\n",
      "diverse, explainable multi-hop question answering.\n",
      "InProceedings of the 2018 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "2369–2380, Brussels, Belgium. Association for Com-\n",
      "putational Linguistics.\n",
      "Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan\n",
      "Ning, and Li Yuan. 2023a. LLM lies: Hallucinations\n",
      "are not bugs, but features as adversarial examples.\n",
      "ArXiv preprint , abs/2310.01469.\n",
      "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\n",
      "Thomas L Griffiths, Yuan Cao, and Karthik\n",
      "Narasimhan. 2023b. Tree of thoughts: Deliberateproblem solving with large language models. ArXiv\n",
      "preprint , abs/2305.10601.\n",
      "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\n",
      "Shafran, Karthik Narasimhan, and Yuan Cao. 2022.\n",
      "React: Synergizing reasoning and acting in language\n",
      "models. ArXiv preprint , abs/2210.03629.\n",
      "Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,\n",
      "Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu\n",
      "Zhang. 2023c. Editing large language models: Prob-\n",
      "lems, methods, and opportunities. ArXiv preprint ,\n",
      "abs/2305.13172.\n",
      "Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao\n",
      "Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,\n",
      "and Enhong Chen. 2023. Woodpecker: Hallucination\n",
      "correction for multimodal large language models.\n",
      "Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Le-\n",
      "gal prompting: Teaching a language model to think\n",
      "like a lawyer. ArXiv preprint , abs/2212.01326.\n",
      "Fei Yu, Hongbo Zhang, and Benyou Wang. 2023a. Na-\n",
      "ture language reasoning, a survey. ArXiv preprint ,\n",
      "abs/2303.14725.\n",
      "Weijiang Yu, Jian Liang, Lei Ji, Lu Li, Yuejian Fang,\n",
      "Nong Xiao, and Nan Duan. 2021. Hybrid reasoning\n",
      "network for video-based commonsense captioning.\n",
      "InProceedings of the 29th ACM international con-\n",
      "ference on multimedia , pages 5213–5221.\n",
      "Weijiang Yu, Haofan Wang, Guohao Li, Nong Xiao, and\n",
      "Bernard Ghanem. 2023b. Knowledge-aware global\n",
      "reasoning for situation recognition. IEEE Transac-\n",
      "tions on Pattern Analysis and Machine Intelligence .Bernard Ghanem. 2023b. Knowledge-aware global\n",
      "reasoning for situation recognition. IEEE Transac-\n",
      "tions on Pattern Analysis and Machine Intelligence .\n",
      "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\n",
      "Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\n",
      "Michael Zeng, and Meng Jiang. 2023c. Generate\n",
      "rather than retrieve: Large language models are\n",
      "strong context generators. In The Eleventh Inter-\n",
      "national Conference on Learning Representations,\n",
      "ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . Open-\n",
      "Review.net.\n",
      "Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\n",
      "Jiang, and Ashish Sabharwal. 2023d. Improving lan-\n",
      "guage models via plug-and-play retrieval feedback.\n",
      "ArXiv preprint , abs/2305.14002.\n",
      "Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\n",
      "and Huan Sun. 2023. Automatic evaluation of at-\n",
      "tribution by large language models. ArXiv preprint ,\n",
      "abs/2305.06311.\n",
      "Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\n",
      "Choi. 2019. From recognition to cognition: Visual\n",
      "commonsense reasoning. In IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, CVPR\n",
      "2019, Long Beach, CA, USA, June 16-20, 2019 , pages\n",
      "6720–6731. Computer Vision Foundation / IEEE.Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and\n",
      "Arvind Neelakantan. 2021. Trading off diversity and\n",
      "quality in natural language generation. In Proceed-\n",
      "ings of the Workshop on Human Evaluation of NLP\n",
      "Systems (HumEval) , pages 25–33, Online. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Jiajun Zhang, Yang Zhao, Haoran Li, and Chengqing\n",
      "Zong. 2018. Attention with sparsity regularization\n",
      "for neural machine translation and summarization.\n",
      "IEEE/ACM Transactions on Audio, Speech, and Lan-\n",
      "guage Processing , 27(3):507–518.\n",
      "Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A.\n",
      "Malin, and Sricharan Kumar. 2023a. Sac3: Reliable\n",
      "hallucination detection in black-box language models\n",
      "via semantic-aware cross-check consistency.\n",
      "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\n",
      "ter J. Liu. 2020. PEGASUS: pre-training with ex-\n",
      "tracted gap-sentences for abstractive summarization.\n",
      "InProceedings of the 37th International Conference\n",
      "on Machine Learning, ICML 2020, 13-18 July 2020,\n",
      "Virtual Event , volume 119 of Proceedings of Machine\n",
      "Learning Research , pages 11328–11339. PMLR.\n",
      "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.\n",
      "2023b. Adding conditional control to text-to-image\n",
      "diffusion models. In Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision , pages\n",
      "3836–3847.\n",
      "Muru Zhang, Ofir Press, William Merrill, Alisa\n",
      "Liu, and Noah A Smith. 2023c. How language\n",
      "model hallucinations can snowball. ArXiv preprint ,\n",
      "abs/2305.13534.\n",
      "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\n",
      "Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-\n",
      "wei Zhang, Fei Wu, et al. 2023d. Instruction tuning\n",
      "for large language models: A survey. ArXiv preprint ,\n",
      "abs/2308.10792.\n",
      "Shuo Zhang, Liangming Pan, Junzhou Zhao, and\n",
      "William Yang Wang. 2023e. Mitigating lan-\n",
      "guage model hallucination with interactive\n",
      "question-knowledge alignment. ArXiv preprint ,\n",
      "abs/2305.13669.\n",
      "Susan Zhang, Stephen Roller, Naman Goyal, Mikel\n",
      "Artetxe, Moya Chen, Shuohui Chen, Christopher\n",
      "Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\n",
      "Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\n",
      "ter, Daniel Simig, Punit Singh Koura, Anjali Sridhar,\n",
      "Tianlu Wang, and Luke Zettlemoyer. 2022. OPT:\n",
      "open pre-trained transformer language models. ArXiv\n",
      "preprint , abs/2205.01068.\n",
      "Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy\n",
      "Liang, Kathleen McKeown, and Tatsunori B\n",
      "Hashimoto. 2023f. Benchmarking large language\n",
      "models for news summarization. ArXiv preprint ,\n",
      "abs/2301.13848.Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\n",
      "Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\n",
      "Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei\n",
      "Bi, Freda Shi, and Shuming Shi. 2023g. Siren’s song\n",
      "in the AI ocean: A survey on hallucination in large\n",
      "language models. ArXiv preprint , abs/2309.01219.\n",
      "Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\n",
      "Qin, and Lidong Bing. 2023a. Verify-and-edit: A\n",
      "knowledge-enhanced chain-of-thought framework.\n",
      "InProceedings of the 61st Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume\n",
      "1: Long Papers), ACL 2023, Toronto, Canada, July\n",
      "9-14, 2023 , pages 5823–5840. Association for Com-\n",
      "putational Linguistics.\n",
      "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\n",
      "Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\n",
      "Zhang, Junjie Zhang, Zican Dong, et al. 2023b. A\n",
      "survey of large language models. ArXiv preprint ,\n",
      "abs/2303.18223.\n",
      "Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang\n",
      "Xing, Chong Meng, Shuaiqiang Wang, Zhicong\n",
      "Cheng, Zhaochun Ren, and Dawei Yin. 2023c.\n",
      "Knowing what llms do not know: A simple yet\n",
      "effective self-detection method. ArXiv preprint ,\n",
      "abs/2310.17918.\n",
      "Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n",
      "2023. Why does chatgpt fall short in answering ques-\n",
      "tions faithfully? ArXiv preprint , abs/2304.10513.\n",
      "Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo,\n",
      "Heng Gong, Xiaocheng Feng, and Bing Qin. 2023a.\n",
      "Stoa-vlp: Spatial-temporal modeling of object and\n",
      "action for video-language pre-training. In Proceed-\n",
      "ings of the Thirty-Seventh AAAI Conference on Ar-\n",
      "tificial Intelligence and Thirty-Fifth Conference onaction for video-language pre-training. In Proceed-\n",
      "ings of the Thirty-Seventh AAAI Conference on Ar-\n",
      "tificial Intelligence and Thirty-Fifth Conference on\n",
      "Innovative Applications of Artificial Intelligence and\n",
      "Thirteenth Symposium on Educational Advances in\n",
      "Artificial Intelligence , volume 37, pages 3715–3723.\n",
      "Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\n",
      "Factual probing is [MASK]: Learning vs. learning\n",
      "to recall. In Proceedings of the 2021 Conference\n",
      "of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language\n",
      "Technologies , pages 5017–5033, Online. Association\n",
      "for Computational Linguistics.\n",
      "Zexuan Zhong, Zhengxuan Wu, Christopher D Man-\n",
      "ning, Christopher Potts, and Danqi Chen. 2023b.\n",
      "Mquake: Assessing knowledge editing in language\n",
      "models via multi-hop questions. ArXiv preprint ,\n",
      "abs/2305.14795.\n",
      "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\n",
      "Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\n",
      "Lili Yu, et al. 2023a. Lima: Less is more for align-\n",
      "ment. ArXiv preprint , abs/2305.11206.\n",
      "Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\n",
      "Francisco Guzmán, Luke Zettlemoyer, and Marjan\n",
      "Ghazvininejad. 2021. Detecting hallucinated contentin conditional neural sequence generation. In Find-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics: ACL-IJCNLP 2021 , pages 1393–1404, Online.\n",
      "Association for Computational Linguistics.\n",
      "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun\n",
      "Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and\n",
      "Huaxiu Yao. 2023b. Analyzing and mitigating object\n",
      "hallucination in large vision-language models.\n",
      "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\n",
      "Mohamed Elhoseiny. 2023a. Minigpt-4: Enhancing\n",
      "vision-language understanding with advanced large\n",
      "language models. ArXiv preprint , abs/2304.10592.\n",
      "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,\n",
      "Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian\n",
      "Huang. 2023b. Multilingual machine translation with\n",
      "large language models: Empirical results and analy-\n",
      "sis.ArXiv preprint , abs/2304.04675.\n",
      "Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika\n",
      "Chavhan, and Timothy Hospedales. 2023. Fool\n",
      "your (vision and) language model with embar-\n",
      "rassingly simple permutations. ArXiv preprint ,\n",
      "abs/2310.01651.\n",
      "A AppendixTruthfulQA FacTool ChineseFactEval HalluQA\n",
      "Method LanguageTrue + Info True Info Claim-level Acc Response-level Acc Manual GPT-4 vote\n",
      "GPT-4 en - - - 75.6 43.33 183.5/301 53.11%\n",
      "ChatGPT en 78.46% 79.92% 98.53% 68.63 36.67 - 39.33%\n",
      "Claude-v1 en - - - 63.95 26.67 - -\n",
      "Bard en - - - 61.15 33.33 - -\n",
      "Doubao zh - - - - - 139/301 -\n",
      "Yiyan zh - - - - - 122.5/301 69.33%\n",
      "Sensetime zh - - - - - 103.5/301 -\n",
      "ChatGLM-pro zh - - - - - 89.5/301 61.33%\n",
      "ABAB zh - - - - - 77/301 56.00%\n",
      "MPT-7B en 29.13% 36.72% 92.04% - - - -\n",
      "MPT-30B en 35.25% 40.27% 94.74% - - - -\n",
      "Falcon-7B en 25.95% 29.01% 96.08% - - - -\n",
      "Falcon-40B en 40.39% 44.8% 95.23% - - - -\n",
      "Llama-1-7B en 27.42% 32.31% 94.86% - - - -\n",
      "Llama-1-13B en 41.74% 45.78% 95.72% - - - -\n",
      "Llama-1-33B en 44.19% 48.71% 95.23% - - - -\n",
      "Llama-1-65B en 48.71% 51.29% 96.82% - - - -\n",
      "Llama-2-7B en 33.29% 39.53% 93.02% - - - -\n",
      "Llama-2-13B en 41.86% 45.65% 96.08% - - - -\n",
      "Llama-2-34B en 43.45% 46.14% 96.70% - - - -\n",
      "Llama-2-70B en 50.18% 53.37% 96.21% - - - -\n",
      "MPT-instruct-7B en 29.99% 35.13% 94.37% - - - -\n",
      "Falcon-instruct-7B en 28.03% 41.00% 85.68% - - - -\n",
      "Llama-2-7B-Chat en 57.04% 60.59% 96.45% - - - -\n",
      "Llama-2-13B-Chat en 62.18% 65.73% 96.45% - - - -\n",
      "Llama-2-34B-Chat en 67.20% 70.01% 97.06% - - - -\n",
      "Llama-2-70B-Chat en 64.14% 67.07% 97.06% - - - -\n",
      "Baichuan2-53B zh - - - - - - 68.22%\n",
      "SparkDesk zh - - - - - - 60.00%\n",
      "QWen-14B-Chat zh - - - - - - 46.89%\n",
      "Baichuan2-13B-Chat zh - - - - - - 42.44%\n",
      "Baichuan2-7B-Chat zh - - - - - - 40.67%\n",
      "Xverse-13B-Chat zh - - - - - - 39.11%\n",
      "Xverse-7B-Chat zh - - - - - - 36.89%\n",
      "QWen-14B zh - - - - - - 36.22%\n",
      "ChatGLM2-6B zh - - - - - - 34.89%\n",
      "Baichuan2-13B zh - - - - - - 33.78%\n",
      "Qwen-7B-Chat zh - - - - - - 31.78%\n",
      "Baichuan-13B-Chat zh - - - - - - 31.33%\n",
      "ChatGLM-6B zh - - - - - - 30.44%\n",
      "QWen-7B zh - - - - - - 29.78%\n",
      "Xverse-13B zh - - - - - - 27.33%\n",
      "Baichuan-13B zh - - - - - - 25.33%\n",
      "Baichuan2-7B zh - - - - - - 25.33%\n",
      "Baichuan-7B zh - - - - - - 22.22%\n",
      "Xverse-7B zh - - - - - - 20.22%\n",
      "Table 6: An overview of the performance of existing LLMs on hallucination evaluation benchmarks. For results in\n",
      "TruthfulQA, we directly used the results in Llama-2 Technical Report; as for ChineseFactEval and FacTool, we\n",
      "used the results in its GitHub repository and report. Regarding HalluQA, we directly used the results in its paper.\n"
     ]
    }
   ],
   "source": [
    "print(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e17c2b45-7f99-4b8b-954a-ea089ecffa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(full_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd12e78a-94fb-4843-be70-1ec50b6976cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Apr 06, 21:52:27] #> Creating directory .ragatouille/colbert/indexes/hallucination_paper \n",
      "\n",
      "\n",
      "[Apr 06, 21:52:30] [0] \t\t #> Encoding 139 passages..\n",
      "[Apr 06, 21:52:33] [0] \t\t avg_doclen_est = 366.3165588378906 \t len(local_sample) = 139\n",
      "[Apr 06, 21:52:33] [0] \t\t Creating 2,048 partitions.\n",
      "[Apr 06, 21:52:33] [0] \t\t *Estimated* 50,918 embeddings.\n",
      "[Apr 06, 21:52:33] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/hallucination_paper/plan.json ..\n",
      "used 20 iterations (0.7326s) to cluster 48373 items into 2048 clusters\n",
      "[Apr 06, 21:52:33] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 06, 21:53:23] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.033, 0.034, 0.033, 0.031, 0.032, 0.034, 0.032, 0.031, 0.031, 0.033, 0.03, 0.03, 0.033, 0.032, 0.032, 0.032, 0.03, 0.034, 0.032, 0.032, 0.032, 0.033, 0.032, 0.034, 0.032, 0.032, 0.033, 0.035, 0.034, 0.034, 0.031, 0.035, 0.032, 0.032, 0.032, 0.03, 0.033, 0.031, 0.032, 0.037, 0.033, 0.034, 0.032, 0.034, 0.031, 0.031, 0.031, 0.035, 0.033, 0.032, 0.031, 0.034, 0.036, 0.031, 0.031, 0.032, 0.038, 0.033, 0.037, 0.03, 0.032, 0.034, 0.035, 0.034, 0.033, 0.033, 0.033, 0.033, 0.031, 0.032, 0.033, 0.031, 0.035, 0.033, 0.035, 0.031, 0.032, 0.034, 0.034, 0.035, 0.035, 0.033, 0.033, 0.032, 0.032, 0.031, 0.032, 0.034, 0.03, 0.035, 0.033, 0.035, 0.032, 0.034, 0.033, 0.031, 0.034, 0.03, 0.034, 0.031, 0.035, 0.037, 0.032, 0.032, 0.034, 0.029, 0.032, 0.031, 0.034, 0.032, 0.034, 0.033, 0.035, 0.032, 0.034, 0.029, 0.032, 0.032, 0.033, 0.033, 0.032, 0.031, 0.033, 0.036, 0.031, 0.038, 0.032, 0.03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 06, 21:53:58] [0] \t\t #> Encoding 139 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.06s/it]\n",
      "100%|███████████████████████| 1/1 [00:00<00:00, 85.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 06, 21:54:00] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Apr 06, 21:54:00] #> Building the emb2pid mapping..\n",
      "[Apr 06, 21:54:00] len(emb2pid) = 50918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████| 2048/2048 [00:00<00:00, 89676.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 06, 21:54:00] #> Saved optimized IVF to .ragatouille/colbert/indexes/hallucination_paper/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.ragatouille/colbert/indexes/hallucination_paper'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.index(collection=[full_document], \n",
    "          index_name=\"hallucination_paper\", \n",
    "          max_document_length=512, \n",
    "          split_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb5a2cd7-e5af-49d6-9925-c3a62ac5ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index hallucination_paper for the first time... This may take a few seconds\n",
      "[Apr 06, 22:01:57] #> Loading codec...\n",
      "[Apr 06, 22:01:57] #> Loading IVF...\n",
      "[Apr 06, 22:01:57] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████| 1/1 [00:00<00:00, 285.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 06, 22:01:57] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████| 1/1 [00:00<00:00, 15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . What is Hallucination?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2003,  2534, 14194, 12758,  1029,   102,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.device(device):\n",
    "    search = RAG.search(query=\"What is Hallucination?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c49a41-39b6-4c5d-9f35-ac1fdfdb90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dca93-87f5-473a-bb95-97b119a18fd9",
   "metadata": {},
   "source": [
    "## Using Langchain Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8fbd0ab-496f-475d-aaa3-8556de356559",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ad1d850-5189-446e-ae72-d34c81c6a18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Such\\nintegration of human feedback into the training\\nloop has proven effective in enhancing the align-\\nment of LLMs, guiding them toward producing\\nhigh-quality and harmless responses.\\n2.3 Hallucinations in Large Language Models\\nThe concept of hallucination traces its roots to the\\nfields of pathology and psychology and is defined\\nasthe perception of an entity or event that is ab-\\nsent in reality (Macpherson and Platchias, 2013).\\nWithin the realm of NLP, hallucination is typically\\nreferred to as a phenomenon in which the gen-\\nerated content appears nonsensical or unfaithful\\nto the provided source content (Filippova, 2020;\\nMaynez et al., 2020). This concept bears a loose\\nresemblance to the phenomenon of hallucination\\nobserved in human psychology. Generally, halluci-\\nnations in natural language generation tasks can be\\ncategorized into two primary types: intrinsic hallu-\\ncination andextrinsic hallucination (Huang et al.,\\n2021; Li et al., 2022b; Ji et al., 2023a). Specifically,\\nintrinsic hallucinations pertain to the outputs of\\nLLMs that conflict with the source content. Con-\\nversely, extrinsic hallucinations refer to the LLM\\ngenerations that cannot be verified from the source\\ncontent.\\nHowever, in the era of large language models,\\nthe versatile capabilities of these models have fa-\\ncilitated their widespread use across diverse fields,\\nhighlighting limitations in existing task-specific\\ncategorization paradigms. Considering that LLMs\\nplace a significant emphasis on user-centric interac-\\ntions and prioritize alignment with user directives,coupled with the fact that their hallucinations pre-\\ndominantly surface at factual levels, we introduce\\na more granular taxonomy building upon the foun-\\ndational work by Ji et al. (2023a). This refined tax-\\nonomy seeks to encapsulate the distinct intricacies\\nassociated with LLM hallucinations. To provide a\\nmore intuitive illustration of our definition of LLM\\nhallucination, we present examples for each type\\nof hallucination in Table 1, accompanied by corre-\\nsponding explanations. The details of our proposed\\ncategories are elaborated below:\\nFactuality Hallucination.'),\n",
       " Document(page_content='To provide a\\nmore intuitive illustration of our definition of LLM\\nhallucination, we present examples for each type\\nof hallucination in Table 1, accompanied by corre-\\nsponding explanations. The details of our proposed\\ncategories are elaborated below:\\nFactuality Hallucination. The emergence of\\nLLMs marks a significant shift from traditional\\ntask-specific toolkits to AI assistants that have\\na heightened focus on open-domain interactions.\\nThis shift is primarily attributed to their vast para-\\nmetric factual knowledge. However, existing\\nLLMs occasionally exhibit tendencies to produce\\noutputs that are either inconsistent with real-world\\nfacts or potentially misleading, posing challenges\\nto the trustworthiness of artificial intelligence. In\\nthis context, we categorize these factual errors as\\nfactuality hallucinations . Depending on whether\\nthe generated factual content can be verified against\\na reliable source, they can be further divided into\\ntwo primary types:\\n•Factual Inconsistency refers to situations\\nwhere the LLM’s output contains facts that\\ncan be grounded in real-world information,\\nbut present contradictions. This type of\\nhallucination occurs most frequently and\\narises from diverse sources, encompassing the\\nLLM’s capture, storage, and expression of fac-but present contradictions. This type of\\nhallucination occurs most frequently and\\narises from diverse sources, encompassing the\\nLLM’s capture, storage, and expression of fac-\\ntual knowledge. As shown in Table 1, when\\ninquired about \"the first person to land on the\\nMoon\" , the model erroneously generated \"Yuri\\nGagarin\" , which contradicts the real-world\\nfact.\\n•Factual Fabrication refers to instances where\\nthe LLM’s output contains facts that are unver-\\nifiable against established real-world knowl-\\nedge. As demonstrated in Table 1, while \"the\\norigins of unicorns\" traditionally lack empiri-\\ncal grounding, the model fabricated a plausi-\\nble historical origin for unicorns.\\nFaithfulness Hallucination. LLMs are inher-\\nently trained to align with user instructions.'),\n",
       " Document(page_content='These\\nhallucinations are further categorized into intrin-\\nsic hallucination andextrinsic hallucination types,\\ndepending on the contradiction with the source\\ncontent. While this category is shared among vari-\\nous natural language generation (NLG) tasks, task-\\nspecific variations do exist. As LLMs are remark-\\nably versatile and excel across different NLG tasks\\n(Bubeck et al., 2023; Bang et al., 2023), particu-\\nlarly in open-domain applications, their remarkable\\nversatility amplifies the potential for hallucinations\\ncompared to task-specific models. In LLMs, the\\nscope of hallucination encompasses a broader and\\nmore comprehensive concept, primarily centering\\non factual errors. In light of the evolution of the\\nLLM era, there arises a need to adjust the existing\\nhallucination taxonomy, enhancing its applicability\\nand adaptability.\\nIn this survey, we have redefined the taxonomy\\nof hallucination, offering a more tailored frame-\\nwork for LLM applications. We categorize hallu-\\ncination into two main groups: factuality halluci-\\nnation andfaithfulness hallucination .Factuality\\nhallucination emphasizes the discrepancy between\\ngenerated content and verifiable real-world facts,cination into two main groups: factuality halluci-\\nnation andfaithfulness hallucination .Factuality\\nhallucination emphasizes the discrepancy between\\ngenerated content and verifiable real-world facts,\\ntypically manifesting as factual inconsistency or\\nfabrication. For example, as in Fig. 1(a), when\\nqueried about the first person to walk on the moon,arXiv:2311.05232v1  [cs.CL]  9 Nov 2023Answer: The ﬁrst person to walk on the moon\\nwas\\xa0Charles Lindbergh in 1951, during the Lunar\\nPioneer mission.\\xa0His historic moonwalk was a\\ntestament to human spirit and was broadcasted\\nlive to millions of people around the globe.Who was the ﬁrst person to walk on the moon?\\nCorrect Answer: Neil Armstrong was the ﬁrst\\nperson to walk on the moon in 1969 during the\\nApollo 11 mission.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is hallucination?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfe8e5-74a3-4d18-8970-5a7643d3f983",
   "metadata": {},
   "source": [
    "## Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb8da5c-bdd6-44dc-babb-12a6df2d1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad22da-c3f4-4a13-a88b-168819c46e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a489d438-84c8-48fe-b98a-dc501dae574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05e35e3c-535a-45df-8aaa-8118c7c351c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What is hallucination?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "273aa711-ea5c-493a-80a1-788decbcc1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallucination, in the context of natural language generation tasks, refers to a phenomenon where the generated content appears nonsensical or unfaithful to the provided source content.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febbd74-76c7-4de3-ae11-696307bfb303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede066db-362e-4508-a939-77b9062b0ce6",
   "metadata": {},
   "source": [
    "# Hull university "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ef65ce-0a4d-43b3-9b2a-76200a853aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_hull_data = TextLoader(\"data.txt\")\n",
    "hull_data_pages = load_hull_data.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377bd2ab-4676-4a4d-852f-c812b895eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hull_document = \"\"\n",
    "for hull_page in hull_data_pages:\n",
    "    hull_document += hull_page.page_content\n",
    "\n",
    "print(hull_document)\n",
    "type(hull_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132a9e5-7a61-4e51-a722-c24a47eba1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a95481d-da45-4349-bf29-50c8059acf60",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "168778ae-925c-404d-b73e-e51ed935e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with torch.device(device):\n",
    "#     RAG.index(collection=[hull_document], \n",
    "#               index_name=\"hull_data\", \n",
    "#               max_document_length=512, \n",
    "#               split_documents=True, \n",
    "#               use_faiss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e28811-1907-4cb5-988a-7a618f4f5084",
   "metadata": {},
   "source": [
    "### Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "541b7fad-ae02-4ea3-a8dd-e08f7abbcd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'The design of the new building offers an immersive, contemporary, and high-quality digital setting whilst keeping its environmental impact to a minimum.Dr Kevin Pimbblet, Director of the new Centre, said: “The past 12 months has shown how quickly the field of Artificial Intelligence can develop. With some jobs and careers now in danger of being replaced with this technology, let alone the ethical use of it, one thing is for certain: it is more important than ever that the nation has appropriately skilled graduates to be able to understand and navigate the full breadth of the technology underpinning the next industrial revolution.”\\n\\nCrucially, through knowledge exchange, DAIM’s ambitions encompass the provision of exemplary service within the University, and beyond to the public sector, that will enhance mutual goals and tackle complex industry issues.\\n\\nDAIM will become a portal for business partnerships and will deliver dynamic inter-disciplinary collaborations and external partnerships leading to research and skills outcomes that are of strategic priority to the region, the UK and the world.\\n\\nEarlier this year, the University of Hull announced that it had been awarded £690,000 to help those underrepresented in the field of AI and data science gain the skills to move into the industry.\\n\\nThe funding, from the Department for Science, Innovation and Technology (DSIT) and Office for Artificial Intelligence (OAI) was awarded by the Office for Students (OfS) to universities to deliver the AI and data science scholarships.\\n\\nThe University was among 30 to be awarded a share of £8.1m, with scholarships being offered to home students who meet certain criteria.\\n\\nSuccessful applicants will receive £10,000 towards their MSc.\\n\\nDr Pimbblet said: “This was fantastic news for the University as it aligns with the University’s social justice strategy of supporting disadvantaged and underrepresented students. The award demonstrates the reputation of our Artificial Intelligence (AI) and Data Science course.”\\n\\nThe Centre has been running a highly successful MSc conversion course since its inception in 2020, supporting hundreds of students to develop their data science and AI skills and progressing them into a wide range of UK industries, including finance, retail, energy, and beyond.\\n\\nDAIM students cover programming, statistics, machine learning, big data, data visualisation, computer vision and the ethical and legal responsibilities of using data.', 'score': 15.7421875, 'rank': 1, 'document_id': '00bc7c1f-d2f7-4e33-a44e-ac8ad75f0633', 'passage_id': 5018}]\n"
     ]
    }
   ],
   "source": [
    "index_name = \"hull_data\"\n",
    "results = RAG.search(query=\"How much is AI and DS school feels?\", k=1, index_name=index_name)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32a30d2a-1118-4980-bd78-61450b2a8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a67c0524-13a8-4944-8cfc-c5a8b37d1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15e25708-62bf-49a4-a661-f8399a9a0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "233edead-a62c-4d29-8cd2-e036c7a73eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = retrieval_chain.invoke({\"input\": \"Who Is Dr Temitayo Fagbola?\"})\n",
    "response = retrieval_chain.invoke({\"input\": \"In one sentence, who is dr brian?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "356d93a7-4ac2-4010-9c65-ba110d98a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr. Brian May is a highly accomplished musician, passionate astrophysicist, and lecturer in Mechanical Engineering at the University of Hull.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053b653-7415-4abb-9af3-7a201747b94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6d5f71f-35e7-41fb-9e2d-f01e3e550568",
   "metadata": {},
   "source": [
    "# Scrap Data from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6db53f-ac8e-4abe-a006-3cc0ae0a345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sitemap_urls(sitemap_url):\n",
    "#     response = requests.get(sitemap_url)\n",
    "#     sitemap = response.content\n",
    "#     root = ET.fromstring(sitemap)\n",
    "#     urls = [url.text for url in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')]\n",
    "#     return urls\n",
    "\n",
    "# # Get URLs from sitemap\n",
    "# sitemap_url = 'https://www.hull.ac.uk/sitemap.xml'\n",
    "# urls = get_sitemap_urls(sitemap_url)\n",
    "\n",
    "# from langchain.document_loaders import UnstructuredURLLoader\n",
    "# while tf.device(device):\n",
    "#     loaders = UnstructuredURLLoader(urls=urls)\n",
    "#     data = loaders.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e1244-331a-4f4f-a29a-358eaf5fd023",
   "metadata": {},
   "source": [
    "# Save & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2e919-926d-415b-b5bc-5b9508db4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# # Prepare data for saving\n",
    "# data_to_save = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in data]\n",
    "\n",
    "# # Save data to a JSON file\n",
    "# with open('data.json', 'w') as file:\n",
    "#     json.dump(data_to_save, file)\n",
    "\n",
    "# data_str = '\\n'.join(doc.page_content for doc in data)\n",
    "\n",
    "# # Save the string to a text file\n",
    "# with open('data.txt', 'w') as file:\n",
    "#     file.write(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b02686-9fca-4571-a615-5843b20ee110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1450697-c12b-4c75-92d7-263f5ff9c0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3394126e-be33-4652-be4a-16f415c3714f",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aed2c438-5e69-4e82-8b0f-4d343309778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load and split the data\n",
    "load_hull_data = TextLoader(\"data.txt\")\n",
    "hull_data_pages = load_hull_data.load_and_split()\n",
    "\n",
    "hull_document = \"\"\n",
    "for hull_page in hull_data_pages:\n",
    "    hull_document += hull_page.page_content\n",
    "\n",
    "# Create a RAG model\n",
    "# RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Specify the index name\n",
    "index_name = \"hull_data\"\n",
    "\n",
    "# Create a retriever\n",
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\"\n",
    ")\n",
    "\n",
    "# Create a language model\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create a document chain and retrieval chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "def ask_question(question):\n",
    "    response = retrieval_chain.invoke({\"input\": question})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Question Answering with Langchain and RAGatouille\",\n",
    "    description=\"Type your question and press Enter to get an answer based on the loaded data.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4127b2-36d7-46ec-9d91-ce1de37a0c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
